from langchain_core.tools import tool
import re
import sys
import asyncio
import json
import concurrent.futures
import os
from typing import Dict, List, Any, Optional, AsyncGenerator, Tuple

# Fallback ì‹œìŠ¤í…œ import (Docker ë³¼ë¥¨ ë§ˆìš´íŠ¸ëœ utils í´ë”)
sys.path.append('/app')
from utils.model_fallback import ModelFallbackManager
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

# arxiv_search í•¨ìˆ˜ì˜ ì‹¤ì œ ë¡œì§ì„ ì§ì ‘ êµ¬í˜„
import urllib.parse
import urllib.request
import xml.etree.ElementTree as ET

from langchain.prompts import PromptTemplate
from langchain import hub
from langchain.agents import AgentExecutor, create_react_agent
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI

from ..models.models import SearchResult
from ...services.search.search_tools import (
    debug_web_search,
    rdb_search,
    vector_db_search,
    graph_db_search,
    scrape_and_extract_content,
    arxiv_search,
)
from ...utils.session_logger import get_session_logger, set_current_session, session_print

# ì „ì—­ ThreadPoolExecutor ìƒì„± (ì¬ì‚¬ìš©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ)
_global_executor = ThreadPoolExecutor(max_workers=16, thread_name_prefix="search_worker")

# ì¶”ê°€: í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
PERSONA_PROMPTS = {}
try:
    import os
    current_dir = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(current_dir, "prompts", "persona_prompts.json")
    with open(file_path, "r", encoding="utf-8") as f:
        PERSONA_PROMPTS = json.load(f)
    print(f"ProcessorAgent: í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì„±ê³µ ({len(PERSONA_PROMPTS)}ê°œ).")
except Exception as e:
    print(f"ProcessorAgent: í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹¤íŒ¨ - {e}")


class DataGathererAgent:
    """ë°ì´í„° ìˆ˜ì§‘ ë° ì¿¼ë¦¬ ìµœì í™” ì „ë‹´ Agent"""

    def __init__(self, model: str = "gemini-2.5-flash-lite", temperature: float = 0):
        # Gemini ëª¨ë¸ (ê¸°ë³¸)
        self.llm = ChatGoogleGenerativeAI(model=model, temperature=temperature)

        # Gemini ë°±ì—… í‚¤ì™€ OpenAI fallback ëª¨ë¸ë“¤

        # Gemini ë°±ì—… ëª¨ë¸ (ë‘ ë²ˆì§¸ í‚¤)
        try:
            self.llm_gemini_backup = ChatGoogleGenerativeAI(
                model=model,
                temperature=temperature,
                google_api_key=ModelFallbackManager.GEMINI_KEY_2
            )
            print("DataGathererAgent: Gemini ë°±ì—… ëª¨ë¸ (í‚¤ 2) ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"DataGathererAgent: Gemini ë°±ì—… ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.llm_gemini_backup = None

        # OpenAI fallback ëª¨ë¸ë“¤
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        if self.openai_api_key:
            self.llm_openai_mini = ChatOpenAI(
                model="gpt-4o-mini",
                temperature=temperature,
                api_key=self.openai_api_key
            )
            self.llm_openai_4o = ChatOpenAI(
                model="gpt-4o",
                temperature=temperature,
                api_key=self.openai_api_key
            )
            print("DataGathererAgent: OpenAI fallback ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            self.llm_openai_mini = None
            self.llm_openai_4o = None
            print("DataGathererAgent: ê²½ê³ : OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")

        # ë„êµ¬ ë§¤í•‘ ì„¤ì • - ì´ë¦„ í†µì¼
        self.tool_mapping = {
            "web_search": self._web_search,
            "vector_db_search": self._vector_db_search,
            "graph_db_search": self._graph_db_search,
            "arxiv_search": self._arxiv_search,
            "pubmed_search": self._pubmed_search,
            "rdb_search": self._rdb_search,
            "scrape_content": self._scrape_content,
        }

    # === [NEW] ê·¸ë˜í”„ ë¦¬í¬íŠ¸ì—ì„œ 'ì›ì‚°ì§€ ê´€ê³„ ì •ë³´' íŒŒì‹± ===
    def _parse_isfrom_from_graph_report(self, text: str):
        try:
            import re
            if "ì›ì‚°ì§€ ê´€ê³„ ì •ë³´" not in text:
                return []
            section = text.split("ì›ì‚°ì§€ ê´€ê³„ ì •ë³´", 1)[1]
            pat = re.compile(
                r"^\s*\d+\.\s+(?P<item>.+?)\s*â†’\s*(?P<origin>.+?)"
                r"(?:\s*\(ë†ì¥ìˆ˜\s*(?P<count>\d+)\s*ê°œ\))?\s*$",
                re.MULTILINE
            )
            out = []
            for m in pat.finditer(section):
                d = {k: (v.strip() if isinstance(v, str) and v else v) for k, v in m.groupdict().items()}
                if d.get("count"):
                    try: d["count"] = int(d["count"])
                    except: pass
                out.append(d)
            return out
        except Exception:
            return []

    # === [NEW] ê·¸ë˜í”„ ë¦¬í¬íŠ¸ì—ì„œ 'ì˜ì–‘ì„±ë¶„ ê´€ê³„ ì •ë³´' íŒŒì‹± ===
    def _parse_nutrients_from_graph_report(self, text: str):
        try:
            import re
            if "ì˜ì–‘ì„±ë¶„ ê´€ê³„ ì •ë³´" not in text:
                return []
            section = text.split("ì˜ì–‘ì„±ë¶„ ê´€ê³„ ì •ë³´", 1)[1]
            pat = re.compile(
                r"^\s*\d+\.\s+(?P<item>.+?)\s*-\s*(?P<nutrient>.+?)"
                r"(?:\s*\(ì–‘:\s*(?P<value>[0-9.]+)\s*(?P<unit>[^)]+)?\))?\s*$",
                re.MULTILINE
            )
            out = []
            for m in pat.finditer(section):
                d = {k: (v.strip() if isinstance(v, str) and v else v) for k, v in m.groupdict().items()}
                if d.get("value"):
                    try: d["value"] = float(d["value"])
                    except: pass
                out.append(d)
            return out
        except Exception:
            return []

    # === [NEW] ê·¸ë˜í”„ ë¦¬í¬íŠ¸ì—ì„œ 'ë¬¸ì„œ ê´€ê³„ ì •ë³´' íŒŒì‹± ===
    def _parse_docrels_from_graph_report(self, text: str):
        """
        neo4j_rag_toolì˜ ë¬¸ìì—´ ë¦¬í¬íŠ¸ì—ì„œ 'ë¬¸ì„œ ê´€ê³„ ì •ë³´' ë¸”ë¡ë§Œ ì¶”ì¶œí•´
        [{source, target, rel_type?, doc?}, ...] í˜•íƒœë¡œ ë°˜í™˜.
        """
        try:
            import re
            if "ë¬¸ì„œ ê´€ê³„ ì •ë³´" not in text:
                return []
            # í•´ë‹¹ ì„¹ì…˜ë§Œ ì˜ë¼ë‚´ê¸°
            section = text.split("ë¬¸ì„œ ê´€ê³„ ì •ë³´", 1)[1]
            # ë²ˆí˜¸ëª©ë¡ ë¼ì¸ íŒŒì‹±: "  1. ì†ŒìŠ¤ - íƒ€ê²Ÿ (type: íƒ€ì…)"
            pat = re.compile(
                r"^\s*\d+\.\s+(?P<source>.+?)\s*-\s*(?P<target>.+?)"
                r"(?:\s*\(type:\s*(?P<rel_type>[^)]+)\))?\s*$",
                re.MULTILINE
            )
            out = []
            for m in pat.finditer(section):
                d = {k: (v.strip() if v else v) for k, v in m.groupdict().items()}
                # doc í•„ë“œëŠ” Neo4j ì¶œë ¥ì— ì—†ìœ¼ë¯€ë¡œ Noneìœ¼ë¡œ ì„¤ì •
                d["doc"] = None
                out.append(d)
            return out
        except Exception:
            return []

    # === [NEW] ê·¸ë˜í”„ ì „ì²´ ì¦ê±°(docrels + isfrom + nutrients)ë¡œ ë²¡í„° ì¿¼ë¦¬ í•œ ì¤„ ìƒì„± ===
    async def _refine_query_from_graph_all(
        self,
        original_query: str,
        docrels: List[Dict[str, str]],
        isfrom: List[Dict[str, str]],
        nutrients: List[Dict[str, str]],
    ) -> str:

        preview = {
            "docrels": docrels,
            "isfrom": isfrom,
            "nutrients": nutrients,
        }
        preview_json = json.dumps(preview, ensure_ascii=False)

        prompt = f"""
ë‹¹ì‹ ì€ Elasticsearch ê¸°ë°˜ Vector DBì—ì„œ ê³ ì •ë°€ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ **ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥**ì„ ì‘ì„±í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
'ì‚¬ìš©ì ì§ˆë¬¸'ê³¼ 'ê·¸ë˜í”„ì—ì„œ ë°œê²¬ëœ ê´€ê³„ ì •ë³´'(docrels, isfrom, nutrients)ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìì˜ **ì›ë˜ ê²€ìƒ‰ ì˜ë„ëŠ” ë³´ì¡´**í•˜ë©´ì„œ
ë©€í‹°í™‰ì—ì„œ í™•ì¸ëœ **ì—°ê²° ê³ ë¦¬(bridge)** ë¥¼ ë¬¸ì¥ ì†ì— ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•˜ì—¬, ë²¡í„° ê²€ìƒ‰ì„ **ì‹¤ì§ˆì ìœ¼ë¡œ ì •ë°€í™”**í•˜ëŠ” ì¿¼ë¦¬ë¡œ ê°œì„ í•˜ì„¸ìš”.

í•µì‹¬ ì›ì¹™:
1) **ì˜ë„ ì™„ì „ ë³´ì¡´**: ì§ˆë¬¸ì˜ ëª©ì /ë²”ìœ„/ë§¥ë½ì„ ë°”ê¾¸ê±°ë‚˜ ì¶•ì†ŒÂ·í™•ì¥í•˜ì§€ ë§ˆì„¸ìš”.
2) **ë¸Œë¦¬ì§€ í™œìš© ë³´ê°•**: docrels(ì—°ê´€ ì•„ì´í…œ/ë¬¸ì„œ ê´€ê³„), isfrom(ì›ì‚°ì§€/ì¶œì²˜), nutrients(ì˜ì–‘ ì„±ë¶„Â·ìˆ˜ì¹˜/ë‹¨ìœ„) ì¤‘
   ì› ì§ˆë¬¸ì˜ ëª¨í˜¸ì„±ì„ ì¤„ì´ê±°ë‚˜ ëŒ€ìƒì„ íŠ¹ì •í•´ì¤„ ë‹¨ì„œë¥¼ ì„ ë³„í•´ ìì—°ìŠ¤ëŸ½ê²Œ ì‚½ì…í•˜ì„¸ìš”.
3) **Vector DB ìµœì í™”**: ë¬¸ì„œì—ì„œ ì‹¤ì œë¡œ ì“°ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ í‘œí˜„Â·ë™ì˜ì–´ë¥¼ ì‚¬ìš©í•˜ë˜, **í‚¤ì›Œë“œ ë‚˜ì—´ì´ ì•„ë‹Œ ì™„ì „í•œ ë¬¸ì¥**ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
4) **ì¦ê±° ê¸°ë°˜**: ê·¸ë˜í”„ì— ì—†ëŠ” ì •ë³´ëŠ” ë§Œë“¤ì§€ ë§ê³ , ìˆ˜ì¹˜/ë‹¨ìœ„/ì¶œì²˜ëŠ” **ê·¸ë˜í”„ ê°’ë§Œ** ì‚¬ìš©í•˜ì„¸ìš”.
5) **ë¬´íš¨ ë³´ê°• ë°©ì§€**: ê·¸ë˜í”„ ë‹¨ì„œê°€ ê²€ìƒ‰ í’ˆì§ˆì„ ì‹¤ì§ˆì ìœ¼ë¡œ ê°œì„ í•˜ì§€ ëª»í•˜ë©´ **ì› ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥**í•˜ì„¸ìš”.

ê·¸ë˜í”„ ì¦ê±° í™œìš© ê°€ì´ë“œ:
- ë¬¸ì„œê´€ê³„(docrels): ëŒ€ìƒ ì‹í’ˆ/ì„±ë¶„ ê°„ì˜ ê´€ê³„ë‚˜ ì°¸ê³  ë¬¸ì„œ ìœ í˜•ì„ **ëŒ€ìƒ íŠ¹ì •í™”**ì— ë„ì›€ì´ ë  ë•Œë§Œ ê°„ê²°íˆ ë°˜ì˜
- ì›ì‚°ì§€(isfrom): ì§€ì—­/êµ­ê°€ê°€ ì§ˆë¬¸ ë§¥ë½ì— ì˜ë¯¸ê°€ ìˆì„ ë•Œë§Œ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
- ì˜ì–‘ì†Œ(nutrients): ì˜ì–‘ ì„±ë¶„ ê´€ë ¨ ì§ˆë¬¸ì´ë©´ êµ¬ì²´ì ì¸ ì˜ì–‘ì†Œëª…ì´ë‚˜ ì„±ë¶„ ì •ë³´ í¬í•¨

ì‘ì„± ê·œì¹™(ë©€í‹°í™‰ ëŒ€ì‘):
- ì› ì§ˆë¬¸ì˜ í•µì‹¬ í‘œí˜„ì„ ìœ ì§€í•˜ê³ , ë¸Œë¦¬ì§€ ë‹¨ì„œë¥¼ **ê´„í˜¸Â·ì¢…ì†ì ˆ** ë“±ìœ¼ë¡œ ë§¤ë„ëŸ½ê²Œ ê²°í•©í•˜ì—¬ êµ¬ì²´í™”
- ë™ì˜ì–´/ì•½ì–´ëŠ” ë¬¸ë§¥ìƒ ìì—°ìŠ¤ëŸ½ê²Œ 1íšŒë§Œ ì‚¬ìš©

ì£¼ì˜ì‚¬í•­:
- **ê¸¸ì´ í•˜í•œ**: ìµœì¢… ë¬¸ì¥ì€ **ì› ì§ˆë¬¸ ê¸¸ì´ ì´ìƒ**ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
- ê·¸ë˜í”„ ë‹¨ì„œê°€ ê²€ìƒ‰ í’ˆì§ˆì„ ê°œì„ í•˜ì§€ ëª»í•œë‹¤ê³  íŒë‹¨ë˜ë©´ **ì› ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥**í•˜ì„¸ìš”.
- ì‚¬ìš©ìê°€ ë¬»ì§€ ì•Šì€ ìƒˆ ì£¼ì œë¡œ í™•ì¥ ê¸ˆì§€
- ì§ˆë¬¸ ë²”ìœ„ë¥¼ ì„ì˜ë¡œ ì¢íˆê±°ë‚˜ ì¼ë°˜í™”í•˜ì§€ ë§ˆì„¸ìš”
- ê·¸ë˜í”„ì— ì—†ëŠ” ê°’ ì¶”ì • ê¸ˆì§€

ì‚¬ìš©ì ì§ˆë¬¸: "{original_query}"

ê·¸ë˜í”„ ì¦ê±°:
{preview_json}

ê°œì„ ëœ Vector ê²€ìƒ‰ ì¿¼ë¦¬:


### ì˜ˆì‹œ A (ì˜ë„ ë³´ì¡´ + ë¸Œë¦¬ì§€ ì£¼ì…)
ì› ì§ˆë¬¸: "ì„±ì¸ ê³„ë€ ì¼ì¼ ê¶Œì¥ ì„­ì·¨ëŸ‰ê³¼ ì£¼ì˜ì‚¬í•­, ì˜ì–‘ì†Œ í†µê³„ ìˆ˜ì¹˜ ë°ì´í„°"
ê·¸ë˜í”„(ë°œì·Œ):
{{
  "docrels":[
    {{"source":"ê³„ë€","target":"ì‹ì•½ì²˜_2021_ê¶Œê³ ","rel_type":"guideline","doc":"mfds_2021.pdf"}},
    {{"source":"ê³„ë€","target":"ì½œë ˆìŠ¤í…Œë¡¤_ì£¼ì˜","rel_type":"caution","doc":"who_lipid_guide.pdf"}}
  ],
  "nutrients":[
    {{"item":"ê³„ë€","nutrient":"ë‹¨ë°±ì§ˆ","value":12.6,"unit":"g/100g"}},
    {{"item":"ê³„ë€","nutrient":"ì½œë ˆìŠ¤í…Œë¡¤","value":373.0,"unit":"mg/100g"}}
  ],
  "isfrom":[{{"item":"ê³„ë€","origin":"êµ­ë‚´ì‚°","count":18}}]
}}
ì¶œë ¥(ì˜ˆ):
"ì„±ì¸ ëŒ€ìƒì˜ ê³„ë€ ì„­ì·¨ì— ëŒ€í•´ ì¼ì¼ ê¶Œì¥ëŸ‰ê³¼ ì£¼ì˜ì‚¬í•­ì„ í™•ì¸í•  ìˆ˜ ìˆëŠ” ìë£Œ(ì‹ì•½ì²˜ 2021 ê¶Œê³ , WHO ì§€ì§ˆ ê°€ì´ë“œ)ì™€ í•¨ê»˜ ë‹¨ë°±ì§ˆ(12.6 g/100g), ì½œë ˆìŠ¤í…Œë¡¤(373 mg/100g) ë“± ì˜ì–‘ì†Œ ìˆ˜ì¹˜ í†µê³„ë¥¼ ë‹¤ë£¬ ë¬¸ì„œë¥¼ ì°¾ì•„ì¤˜."

### ì˜ˆì‹œ B (ì¦ê±°ê°€ ë„ì›€ ì•ˆ ë˜ë©´ ì›ë¬¸ ìœ ì§€)
ì› ì§ˆë¬¸: "ì•„ë³´ì¹´ë„ ì›ì‚°ì§€ë³„ ìˆ˜ì… ë¹„ì¤‘ í†µê³„"
ê·¸ë˜í”„(ë°œì·Œ): {{"nutrients":[{{"item":"ì•„ë³´ì¹´ë„","nutrient":"ë¹„íƒ€ë¯¼E","value":2.1,"unit":"mg/100g"}}]}}
ì¶œë ¥(ì˜ˆ):
"ì•„ë³´ì¹´ë„ ì›ì‚°ì§€ë³„ ìˆ˜ì… ë¹„ì¤‘ í†µê³„"

### ì˜ˆì‹œ C (isfromë¡œ ëª¨í˜¸ì„± ì¶•ì†Œ)
ì› ì§ˆë¬¸: "ì‚¬ê³¼ í’ˆì¢…ë³„ ë‹¹ë„ ë¹„êµ í†µê³„"
ê·¸ë˜í”„(ë°œì·Œ): {{"isfrom":[{{"item":"í›„ì§€","origin":"ì¼ë³¸ì‚°","count":12}},{{"item":"í™ì˜¥","origin":"êµ­ë‚´ì‚°","count":7}}]}}
ì¶œë ¥(ì˜ˆ):
"ì‚¬ê³¼ í’ˆì¢…ë³„ ë‹¹ë„ ë¹„êµ í†µê³„ë¥¼ ì°¾ë˜ í›„ì§€(ì¼ë³¸ì‚°)ì™€ í™ì˜¥(êµ­ë‚´ì‚°)ì— ëŒ€í•œ ìˆ˜ì¹˜ ë¹„êµê°€ í¬í•¨ëœ ë¬¸ì„œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ê²€ìƒ‰í•´ì¤˜."

### ì˜ˆì‹œ D (ê²½ê³  ë¬¸ì„œ + ìˆ˜ì¹˜ ê²°í•©)
ì› ì§ˆë¬¸: "ì°¸ì¹˜ìº” ë‚˜íŠ¸ë¥¨ 1íšŒ ì œê³µëŸ‰ ê¸°ì¤€ê³¼ ê±´ê°•ìƒ ì£¼ì˜ì‚¬í•­"
ê·¸ë˜í”„(ë°œì·Œ):
{{
  "docrels":[{{"source":"ì°¸ì¹˜ìº”","target":"ë‚˜íŠ¸ë¥¨_ì£¼ì˜","rel_type":"caution","doc":"sodium_alert_v2.pdf"}}],
  "nutrients":[{{"item":"ì°¸ì¹˜ìº”","nutrient":"ë‚˜íŠ¸ë¥¨","value":320.0,"unit":"mg/serving"}}]
}}
ì¶œë ¥(ì˜ˆ):
"ì°¸ì¹˜ìº” ì„­ì·¨ì™€ ê´€ë ¨í•´ 1íšŒ ì œê³µëŸ‰ ê¸°ì¤€ì˜ ë‚˜íŠ¸ë¥¨ ìˆ˜ì¹˜(320 mg/serving)ì™€ ê±´ê°•ìƒ ì£¼ì˜ì‚¬í•­ì„ ì œì‹œí•œ ê²½ê³ Â·ê°€ì´ë“œ ë¬¸ì„œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì°¾ì•„ì¤˜."
""".strip()

        try:
            refined = await self._invoke_with_fallback(prompt)
            refined = refined.strip().splitlines()[0].strip()
            return refined or original_query
        except Exception:
            return original_query
        
    # === [NEW] ê·¸ë˜í”„ ì¦ê±°ë¥¼ ì§ˆë¬¸ê³¼ì˜ ì§ì ‘ ê´€ë ¨ì„± ê¸°ì¤€ìœ¼ë¡œ ì„ ë³„(LLM) ===
    async def _select_relevant_graph_evidence(
        self,
        original_query: str,
        docrels: List[dict],
        isfrom: List[dict],
        nutrients: List[dict],
        k_per_type: int = 8,
    ) -> Dict[str, List[dict]]:
        """
        ì…ë ¥ëœ ê·¸ë˜í”„ ì¦ê±°ë“¤ì„ í•œ ë²ˆì— LLMì— ë„˜ê²¨ 'ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨' í•­ëª©ë§Œ ì„ ë³„.
        ì‹¤íŒ¨ ì‹œì—ëŠ” ìƒìœ„ kê°œ ìŠ¬ë¼ì´ìŠ¤ë¡œ í´ë°±.
        ë°˜í™˜ í˜•ì‹: {"docrels":[...], "isfrom":[...], "nutrients":[...]}
        """
        import json

        def _head(xs, n=8):
            return xs[:n] if xs else []

        # LLMì´ ì²˜ë¦¬í•˜ê¸° ì‰½ê²Œ ë¯¸ë¦¬ ìë¥¸ preview(ê³¼ëŒ€ ì…ë ¥ ë°©ì§€)
        preview = {
            "docrels": _head(docrels, 50),
            "isfrom": _head(isfrom, 50),
            "nutrients": _head(nutrients, 50),
        }
        preview_json = json.dumps(preview, ensure_ascii=False)

        prompt = f"""
ë‹¹ì‹ ì€ ë­í‚¹/í•„í„°ë§ ëª¨ë¸ì…ë‹ˆë‹¤. 'ì‚¬ìš©ì ì§ˆë¬¸'ê³¼ 'ê·¸ë˜í”„ ì¦ê±° í›„ë³´'ë¥¼ ë³´ê³ ,
ì§ˆë¬¸ê³¼ **ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨**ë˜ê±°ë‚˜, ë©€í‹°í™‰ íƒìƒ‰ì—ì„œ **ì—°ê²° ê³ ë¦¬(bridge)** ë¡œ ì‘ë™í•´ ë²¡í„° ì¬ê²€ìƒ‰ ì •í™•ë„ë¥¼ ì˜¬ë¦´ í•­ëª©ë§Œ íƒ€ì…ë³„ ìµœëŒ€ {k_per_type}ê°œì”© ê³¨ë¼ì„œ JSONìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.

ë°˜ë“œì‹œ ì•„ë˜ ìŠ¤í‚¤ë§ˆë¥¼ ì§€í‚¤ì„¸ìš”(ì¡´ì¬í•˜ëŠ” í‚¤ë§Œ ìœ ì§€; ê°’ì€ ì›ë³¸ì—ì„œ ê·¸ëŒ€ë¡œ ë³µì‚¬):
{{
  "docrels":   [{{"source": str, "target": str, "rel_type": str, "doc": str}}] ,
  "isfrom":    [{{"item": str, "origin": str, "count": int|null}}] ,
  "nutrients": [{{"item": str, "nutrient": str, "value": float|null, "unit": str|null}}]
}}

ì„ ì • ëª©í‘œ(ë©€í‹°í™‰):
- ì§ˆë¬¸ì˜ í•µì‹¬ ì—”í‹°í‹°/ì¡°ê±´ê³¼ **ì§ì ‘ ì—°ê²°(1-hop)** ë˜ê±°ë‚˜,
- ì§ˆë¬¸ì˜ ì—”í‹°í‹° â†” ëª©í‘œ ì •ë³´ ì‚¬ì´ì˜ ê²½ë¡œë¥¼ **ìµœëŒ€ 2-hop ì´ë‚´ë¡œ ë‹«ì•„ì£¼ëŠ” ë¸Œë¦¬ì§€**ë§Œ ì„ íƒí•˜ì„¸ìš”.
  (ì˜ˆ: docrelsë¡œ ì—°ê²°ëœ ì•„ì´í…œ â†’ í•´ë‹¹ ì•„ì´í…œì˜ isfrom/ì˜ì–‘ì†Œë¡œ ì´ì–´ì§€ëŠ” ê²½ë¡œ)

ì„ ì • ê¸°ì¤€(ìš°ì„ ìˆœìœ„):
1) ì§ì ‘ ì—°ê²°(1-hop) > ë¸Œë¦¬ì§€(2-hop)
2) ë‹¤ìˆ˜ í›„ë³´ì— **ê³µí†µìœ¼ë¡œ ì—°ê²°**ë˜ëŠ” í•­ëª©, isfrom.countê°€ í° í•­ëª© ìš°ì„ 
3) êµ¬ì²´ ëª…ì¹­/ì›ì‚°ì§€/ê´€ê³„íƒ€ì…ì´ **ëª¨í˜¸ì–´/ì¼ë°˜ì–´**ë³´ë‹¤ ìš°ì„ 

ì œì™¸ ê·œì¹™:
- ì§ˆë¬¸ì˜ ì˜ë„ì™€ **ê²½ë¡œê°€ ë‹«íˆì§€ ì•ŠëŠ”** í•­ëª©
- ì¤‘ë³µ/ë™ì˜ì–´/ê²½ë¯¸í•œ í‘œê¸° ì°¨ì´ â†’ í•˜ë‚˜ë§Œ ë‚¨ê¸°ê¸°
- ì„œë¡œ **ëª¨ìˆœ**ë˜ëŠ” ê°’ì´ ë™ì‹œì— ì¡´ì¬í•  ê²½ìš° ì‹ ë¢°ë„ê°€ ë‚®ì€ ê²ƒ ì œê±°
- ë‹¨ì§€ ì—°ìƒë§Œ ê°€ëŠ¥í•œ í•­ëª©(ì£¼ì œ í™•ì¥) ê¸ˆì§€

ìµœì†Œ-ì¶©ë¶„ ì§‘í•©:
- ì§ˆë¬¸ì˜ ì£¼ìš” ì œì•½(ëŒ€ìƒÂ·ì†ì„±Â·ì¡°ê±´)ì„ **ëª¨ë‘ ì»¤ë²„**í•˜ëŠ” **ìµœì†Œ ìˆ˜ì˜ í•­ëª©**ë§Œ ë‚¨ê¸°ì„¸ìš”.
- ê¸°ì¤€ì„ ì¶©ì¡±í•˜ëŠ” ê²ƒì´ ì—†ìœ¼ë©´ í•´ë‹¹ íƒ€ì… ë°°ì—´ì€ **ë¹ˆ ë°°ì—´**ë¡œ ë‘ì„¸ìš”.

í˜•ì‹ ì œì•½:
- JSONë§Œ ì¶œë ¥(ì—¬ëŠ”/ë‹«ëŠ” ì¤‘ê´„í˜¸ í¬í•¨). ì•ë’¤ ì„¤ëª…Â·ì½”ë“œë¸”ë¡ ê¸ˆì§€.
- ë¯¸ì¡´ì¬ íƒ€ì… í‚¤ëŠ” ìƒëµ ê°€ëŠ¥. ê°’ì€ **ì›ë³¸ì—ì„œ ê·¸ëŒ€ë¡œ ë³µì‚¬**.

ì‚¬ìš©ì ì§ˆë¬¸: "{original_query}"

ê·¸ë˜í”„ ì¦ê±° í›„ë³´(JSON):
{preview_json}
""".strip()

        try:
            txt = await self._invoke_with_fallback(prompt)  # ê¸°ì¡´ LLM í˜¸ì¶œ ìœ í‹¸ì„ ì¬ì‚¬ìš©
            # JSONë§Œ ì¶”ì¶œ
            m = re.search(r"\{[\s\S]*\}\s*$", txt.strip())
            if m:
                txt = m.group(0)
            data = json.loads(txt)
            # í˜•ì‹ ë³´ì • ë° ìŠ¬ë¼ì´ìŠ¤
            sel_docrels   = _head(list(data.get("docrels", []) or []),   k_per_type)
            sel_isfrom    = _head(list(data.get("isfrom", []) or []),    k_per_type)
            sel_nutrients = _head(list(data.get("nutrients", []) or []), k_per_type)
            return {"docrels": sel_docrels, "isfrom": sel_isfrom, "nutrients": sel_nutrients}
        except Exception as e:
            print(f"  - [WARN] LLM evidence selection failed: {e} (fallback to head)")
            return {"docrels": _head(docrels, k_per_type),
                    "isfrom": _head(isfrom, k_per_type),
                    "nutrients": _head(nutrients, k_per_type)}


    async def _invoke_with_fallback(self, prompt: str, use_4o: bool = False) -> str:
        """Gemini API ì‹¤íŒ¨ ì‹œ OpenAIë¡œ fallbackí•˜ëŠ” ë©”ì„œë“œ"""
        try:
            # 1ì°¨ ì‹œë„: Gemini (í‚¤ 1)
            print("  - Gemini í‚¤ 1 API ì‹œë„ ì¤‘...")
            response = await self.llm.ainvoke(prompt)
            return response.content.strip()
        except Exception as e:
            error_msg = str(e)
            print(f"  - Gemini í‚¤ 1 API ì‹¤íŒ¨: {error_msg}")

            # Rate limit ë˜ëŠ” quota ì˜¤ë¥˜ ì²´í¬
            if any(keyword in error_msg.lower() for keyword in ['429', 'quota', 'rate limit', 'exceeded']):
                print("  - Rate limit ê°ì§€, fallback ìˆœì°¨ ì‹œë„...")

                # 2ì°¨ ì‹œë„: Gemini ë°±ì—… (í‚¤ 2)
                if self.llm_gemini_backup:
                    try:
                        print("  - Gemini í‚¤ 2 API ì‹œë„ ì¤‘...")
                        response = await self.llm_gemini_backup.ainvoke(prompt)
                        print("  - Gemini í‚¤ 2 API ì„±ê³µ!")
                        return response.content.strip()
                    except Exception as backup_error:
                        print(f"  - Gemini í‚¤ 2 APIë„ ì‹¤íŒ¨: {backup_error}")

                # 3ì°¨ ì‹œë„: OpenAI
                if self.llm_openai_mini:
                    try:
                        fallback_model = self.llm_openai_4o if use_4o else self.llm_openai_mini
                        model_name = "gpt-4o" if use_4o else "gpt-4o-mini"
                        print(f"  - {model_name} API ì‹œë„ ì¤‘...")
                        response = await fallback_model.ainvoke(prompt)
                        print(f"  - {model_name} API ì„±ê³µ!")
                        return response.content.strip()
                    except Exception as openai_error:
                        print(f"  - OpenAI APIë„ ì‹¤íŒ¨: {openai_error}")
                        raise openai_error
                else:
                    print("  - OpenAI API í‚¤ê°€ ì—†ì–´ ìµœì¢… fallback ë¶ˆê°€")
                    raise e
            else:
                # Rate limitì´ ì•„ë‹Œ ë‹¤ë¥¸ ì˜¤ë¥˜ëŠ” ê·¸ëŒ€ë¡œ ë°œìƒ
                raise e

    async def _optimize_query_for_tool(self, query: str, tool: str) -> str:
        """ê° ë„êµ¬ì˜ íŠ¹ì„±ì— ë§ê²Œ ìì—°ì–´ ì¿¼ë¦¬ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤."""

        # RDBì™€ GraphDBëŠ” í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì— ë” íš¨ê³¼ì ì…ë‹ˆë‹¤.
        if tool == "rdb_search":
            print(f"  - {tool} ì¿¼ë¦¬ ìµœì í™” ì‹œì‘: '{query}'")
            prompt = f"""
        ë„ˆëŠ” PostgreSQL RDBì— ì§ˆì˜í•  ê²€ìƒ‰ë¬¸ì„ ë§Œë“œëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤.
        ì‚¬ìš©ì ì§ˆë¬¸ì„ RDBì—ì„œ ì‹¤ì œë¡œ ê²€ìƒ‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì¬ì‘ì„±í•´ë¼.

        [ì¤‘ìš”: RDB í…Œì´ë¸” êµ¬ì¡° ì´í•´]
        - ê°€ê²©/ì‹œì„¸ í…Œì´ë¸”: kamis_product_price_latest (í’ˆëª©ëª…: ì‚¬ê³¼, ë°°, ê°ê·¤, ìŒ€, ê³ ë“±ì–´, ì „ë³µ ë“± êµ¬ì²´ì  í’ˆëª©)
        - ì˜ì–‘ í…Œì´ë¸”: foods + proximates/minerals/vitamins (ì‹í’ˆëª…: ì‚¬ê³¼, ìŒ€, ê³ ë“±ì–´ ë“± êµ¬ì²´ì  ì‹í’ˆ)

        [í’ˆëª© ë³€í™˜ ê·œì¹™ - ë§¤ìš° ì¤‘ìš”]
        1) ì¶”ìƒì  ì¹´í…Œê³ ë¦¬ â†’ êµ¬ì²´ì  í’ˆëª©ë“¤ë¡œ ë³€í™˜:
        - "ë†ì¶•ìˆ˜ì‚°ë¬¼" â†’ "ì‚¬ê³¼, ë°°, ìŒ€, ê°ì"
        - "íŠ¹ì‚°í’ˆ" â†’ "ê°ê·¤, ì‚¬ê³¼, ë°°"
        - "ë†ì‚°ë¬¼" â†’ "ì‚¬ê³¼, ë°°, ìŒ€, ê°ì"
        - "ìˆ˜ì‚°ë¬¼" â†’ "ê³ ë“±ì–´, ëª…íƒœ" (KAMISì— ì œí•œì )
        - "ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ" â†’ "í™ì‚¼, í”„ë¡œí´ë¦¬ìŠ¤"

        2) êµ¬ì²´ì  í’ˆëª©ì€ ê·¸ëŒ€ë¡œ ìœ ì§€:
        - "ì‚¬ê³¼" â†’ "ì‚¬ê³¼"
        - "ê°ê·¤" â†’ "ê°ê·¤"
        - "ìŒ€" â†’ "ìŒ€"

        [ì˜ë„ ë‹¨ìˆœí™”]
        - ë³µì¡í•œ í‘œí˜„ â†’ ë‹¨ìˆœ í‚¤ì›Œë“œ:
        - "ì‹œì„¸ ë³€ë™ ì¶”ì´" â†’ "ê°€ê²©"
        - "ì˜ì–‘ì„±ë¶„(ë¹„íƒ€ë¯¼Â·ë¯¸ë„¤ë„Â·ë‹¨ë°±ì§ˆ)" â†’ "ì˜ì–‘ì„±ë¶„"
        - "ì‹œì¥ í˜„í™© ë¶„ì„" â†’ "ì‹œì¥í˜„í™©"

        [ê¸°ê°„ ì •ê·œí™”]
        - "2025ë…„ 7-8ì›”" â†’ "recent"
        - "ìµœê·¼" â†’ "recent"
        - "í˜„ì¬" â†’ "today"

        [ì¶œë ¥ í˜•ì‹]
        êµ¬ì²´ì  í’ˆëª©ëª…ì„ í¬í•¨í•œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±:

        [ë³€í™˜ ì˜ˆì‹œ]
        - ì…ë ¥: "ì§€ì—­=ëŒ€í•œë¯¼êµ­, í’ˆëª©=ë†ì¶•ìˆ˜ì‚°ë¬¼, ì˜ë„=ì‹œì„¸ ë³€ë™ ì¶”ì´, ê¸°ê°„=2025ë…„ 7-8ì›”"
        - ì¶œë ¥: "ì‚¬ê³¼ ë°° ìŒ€ ê°ì ìµœê·¼ ê°€ê²© ì •ë³´"

        - ì…ë ¥: "ì§€ì—­=ëŒ€í•œë¯¼êµ­, í’ˆëª©=íŠ¹ì‚°í’ˆ, ì˜ë„=ì‹œì„¸ ë³€ë™ ì¶”ì´, ê¸°ê°„=7-8ì›”"
        - ì¶œë ¥: "ê°ê·¤ ì‚¬ê³¼ ë°° ê°€ê²© ê±°ë˜ëŸ‰ ì •ë³´"

        - ì…ë ¥: "ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì‹œì¥ ë™í–¥ ë¶„ì„"
        - ì¶œë ¥: "í™ì‚¼ í”„ë¡œí´ë¦¬ìŠ¤ ì‹œì¥í˜„í™©"

        ì‚¬ìš©ì ì§ˆë¬¸: "{query}"
        ìµœì í™”ëœ ê²€ìƒ‰ë¬¸:
        """
            try:
                response_content = await self._invoke_with_fallback(prompt)
                optimized_query = response_content.strip()
                print(f"  - ìµœì í™”ëœ í‚¤ì›Œë“œ: '{optimized_query}'")
                return optimized_query
            except Exception as e:
                print(f"  - ì¿¼ë¦¬ ìµœì í™” ì‹¤íŒ¨, ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©: {e}")
                return query

        elif tool == "graph_db_search":
            print(f"  - {tool} ì¿¼ë¦¬ ìµœì í™” ì‹œì‘: '{query}'")

            # ê·¸ë˜í”„ ìŠ¤í‚¤ë§ˆ ê¸°ì¤€: (í’ˆëª©)-[:isFrom]->(Origin), (í’ˆëª©)-[:hasNutrient]->(Nutrient)
            # ìš°ë¦¬ê°€ ì›í•˜ëŠ” ì •ê·œ ë¬¸êµ¬:
            #   - "<í’ˆëª©>ì˜ ì›ì‚°ì§€"            -> isFrom
            #   - "<í’ˆëª©>ì˜ ì˜ì–‘ì†Œ"            -> hasNutrient
            #   - "<ì§€ì—­>ì˜ <í’ˆëª©> ì›ì‚°ì§€"     -> isFrom + region filter
            #   - "(í™œì–´|ì„ ì–´|ëƒ‰ë™|ê±´ì–´) <ìˆ˜ì‚°ë¬¼> ì›ì‚°ì§€" -> isFrom + fishState filter
            prompt = f"""
        ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„, Neo4j ê·¸ë˜í”„ ê²€ìƒ‰ì— ë°”ë¡œ ë„£ì„ ìˆ˜ ìˆëŠ” **ì •ê·œ ì§ˆì˜ ë¬¸êµ¬**ë¡œ ë³€í™˜í•˜ì„¸ìš”.
        ê·¸ë˜í”„ ìŠ¤í‚¤ë§ˆ:
        - í’ˆëª© ë…¸ë“œ ë¼ë²¨: Ingredient, Food  (ê³µí†µ ì†ì„±: product)
        - ì›ì‚°ì§€ ë…¸ë“œ ë¼ë²¨: Origin (ì†ì„±: city, region)
        - ì˜ì–‘ì†Œ ë…¸ë“œ ë¼ë²¨: Nutrient (ì†ì„±: name)
        - ë¬¸ì„œ ë‚´ í‚¤ì›Œë“œ ë…¸ë“œ ë¼ë²¨: Entity (ì†ì„±: name)
        - ê´€ê³„: (Ingredient)-[:isFrom]->(Origin), (Food)-[:hasNutrient]->(Nutrient), (Entity)-[:relation]->(Entity)

        ê·œì¹™(ë°˜ë“œì‹œ ì¤€ìˆ˜):
        1) ê²°ê³¼ëŠ” **í•œ ì¤„ë‹¹ í•˜ë‚˜ì˜ ì§ˆì˜**ë¡œ ì¶œë ¥í•˜ê³ , ì•„ë˜ 4ê°€ì§€ íŒ¨í„´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
        - "<í’ˆëª©>ì˜ ì›ì‚°ì§€"
        - "<í’ˆëª©>ì˜ ì˜ì–‘ì†Œ"
        - "<ì§€ì—­>ì˜ íŠ¹ì‚°í’ˆ"
        2) ì§ˆë¬¸ì— í•´ë‹¹ë˜ì§€ ì•ŠëŠ” íŒ¨í„´ì€ ë§Œë“¤ì§€ ë§ˆì„¸ìš”. ì¶”ì¸¡ ê¸ˆì§€.
        3) ë¶ˆí•„ìš”í•œ ì ‘ë‘ì‚¬/ì„¤ëª…/ë”°ì˜´í‘œ/ë²ˆí˜¸/Bullet ê¸ˆì§€. í…ìŠ¤íŠ¸ë§Œ.
        4) ê²°ê³¼ê°€ ì—†ìœ¼ë©´ **ë¹ˆ ë¬¸ìì—´**ë§Œ ë°˜í™˜.

        ì˜ˆì‹œ:
        - ì§ˆë¬¸: "ì‚¬ê³¼ ì–´ë””ì„œ ë‚˜ì™€?"  ->  ì‚¬ê³¼ì˜ ì›ì‚°ì§€
        - ì§ˆë¬¸: "ì˜¤ë Œì§€ ì˜ì–‘ ì„±ë¶„ ì•Œë ¤ì¤˜" -> ì˜¤ë Œì§€ì˜ ì˜ì–‘ì†Œ
        - ì§ˆë¬¸: "ê²½ìƒë¶ë„ì—ì„œëŠ” ë­ê°€ ìƒì‚°ë¼?" -> ê²½ìƒë¶ë„ì˜ íŠ¹ì‚°í’ˆ
        - ì§ˆë¬¸: "ì´ë²ˆ í˜¸ìš°ë¡œ ì¶©ì²­ë„ ë†ì‚°ë¬¼ í”¼í•´ê°€ ì‹¬ê°í•´." -> ì¶©ì²­ë„ì˜ íŠ¹ì‚°í’ˆ

        ì§ˆë¬¸: "{query}"
        ì¶œë ¥:
        """.strip()

            try:
                response_content = await self._invoke_with_fallback(prompt)
                # íŒŒì‹±: ì¤„ ë‹¨ìœ„ë¡œ ì •ë¦¬, í—ˆìš© íŒ¨í„´ë§Œ í†µê³¼
                allowed_prefixes = ("í™œì–´ ", "ì„ ì–´ ", "ëƒ‰ë™ ", "ê±´ì–´ ")
                def _is_allowed(line: str) -> bool:
                    if not line: return False
                    # íŒ¨í„´ 1/2: "<í’ˆëª©>ì˜ (ì›ì‚°ì§€|ì˜ì–‘ì†Œ)"
                    if "ì˜ ì›ì‚°ì§€" in line or "ì˜ ì˜ì–‘ì†Œ" in line:
                        return True
                    # íŒ¨í„´ 3: "<ì§€ì—­>ì˜ <í’ˆëª©> ì›ì‚°ì§€"
                    if line.endswith(" ì›ì‚°ì§€") and "ì˜ " in line and not any(line.startswith(p) for p in allowed_prefixes):
                        return True
                    # íŒ¨í„´ 4: "<ìƒíƒœ> <ìˆ˜ì‚°ë¬¼> ì›ì‚°ì§€"
                    if line.endswith(" ì›ì‚°ì§€") and any(line.startswith(p) for p in allowed_prefixes):
                        return True
                    return False

                lines = [l.strip().lstrip("-â€¢").strip().strip('"').strip("'") for l in response_content.splitlines()]
                lines = [l for l in lines if _is_allowed(l)]
                optimized_query = "\n".join(dict.fromkeys(lines))  # ì¤‘ë³µ ì œê±°, ìˆœì„œ ìœ ì§€

                print(f"  - ìµœì í™”ëœ í‚¤ì›Œë“œ(ì •ê·œ ì§ˆì˜):\n{optimized_query or '(empty)'}")
                return optimized_query if optimized_query else query  # ë¹„ë©´ ì›ë³¸ ì§ˆë¬¸ ì „ë‹¬

            except Exception as e:
                print(f"  - ì¿¼ë¦¬ ìµœì í™” ì‹¤íŒ¨, ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©: {e}")
                return query

        # Vector DBëŠ” êµ¬ì²´ì ì¸ ì •ë³´ ê²€ìƒ‰ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜
        elif tool == "vector_db_search":
            print(f"  - {tool} ì¿¼ë¦¬ ìµœì í™” ì‹œì‘ (ìˆ˜ì¹˜/í‘œ/í†µê³„ í‚¤ì›Œë“œ íŠ¹í™”): '{query}'")
            prompt = f"""
ë‹¤ìŒ ìš”ì²­ì„ Vector DBì—ì„œ **ìˆ˜ì¹˜ ë°ì´í„°, í‘œ, í†µê³„ ìë£Œ**ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” **í‚¤ì›Œë“œ ì¡°í•©**ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.

**ì¤‘ìš” ê·œì¹™ (ë°˜ë“œì‹œ ì¤€ìˆ˜)**:
1. **ì§ˆë¬¸ í˜•ì‹ ê¸ˆì§€** - "~ì¸ê°€ìš”?", "~ì…ë‹ˆê¹Œ?" ë“± ì§ˆë¬¸ í‘œí˜„ ì‚¬ìš© ê¸ˆì§€
2. **í‚¤ì›Œë“œ ì¤‘ì‹¬ì˜ ê²€ìƒ‰ì–´ë§Œ ìƒì„±** - ëª…ì‚¬ì™€ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¡°í•©
3. **ë²ˆí˜¸ ë§¤ê¸°ê¸° ì ˆëŒ€ ê¸ˆì§€** (1., 2., 3. ë“± ì‚¬ìš© ê¸ˆì§€)
4. **ë¶€ê°€ ì„¤ëª… ê¸ˆì§€** - "ì›ë³¸ ìš”ì²­:", "ê²€ìƒ‰ì–´:" ë“± ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
5. **í•œ ì¤„ì˜ í‚¤ì›Œë“œ ì¡°í•©ë§Œ ì¶œë ¥**

**ìˆ˜ì¹˜/í‘œ/í†µê³„ ë°ì´í„° íŠ¹í™” í‚¤ì›Œë“œ ê·œì¹™**:
- **ìˆ˜ì¹˜ ë°ì´í„° í‚¤ì›Œë“œ**: ë§¤ì¶œì•¡, ì‹œì¥ê·œëª¨, ì ìœ ìœ¨, ìƒì‚°ëŸ‰, ì†Œë¹„ëŸ‰, ìˆ˜ì¶œì…ëŸ‰, ê°€ê²©, ìˆœìœ„, ë¹„ìœ¨, í†µê³„
- **í‘œ/ì°¨íŠ¸ í‚¤ì›Œë“œ**: í‘œ, í†µê³„í‘œ, í˜„í™©í‘œ, ë°ì´í„°, ì°¨íŠ¸, ê·¸ë˜í”„, ë„í‘œ
- **ë¹„êµ ë¶„ì„ í‚¤ì›Œë“œ**: ì§€ì—­ë³„, ì—…ì²´ë³„, ë¸Œëœë“œë³„, ì—°ë„ë³„, ì›”ë³„, ìƒìœ„, ìˆœìœ„, ë¹„êµ
- **ì‹œê³„ì—´ í‚¤ì›Œë“œ**: 2024ë…„, 2025ë…„, 7ì›”, 8ì›”, ë¶„ê¸°ë³„, ì›”ë³„, ë…„ë„ë³„, ë³€í™”ìœ¨, ì¦ê°
- **ì •ëŸ‰ ì§€í‘œ**: ì–µì›, ì¡°ì›, í¼ì„¼íŠ¸, í†¤, ê°œ, ëª…, ê±´ìˆ˜, ì¦ê°€ìœ¨, ê°ì†Œìœ¨

ë³€í™˜ ì˜ˆì‹œ (í‚¤ì›Œë“œ ì¤‘ì‹¬):
ì…ë ¥: "êµ­ë‚´ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì‹œì¥ì˜ ìµœê·¼ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤"
ì¶œë ¥: 2024ë…„ ëŒ€í•œë¯¼êµ­ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì‹œì¥ê·œëª¨ ë§¤ì¶œì•¡ ê¸°ì—…ë³„ ì ìœ ìœ¨ ìˆœìœ„ í†µê³„í‘œ

ì…ë ¥: "ìš°ë¦¬ë‚˜ë¼ ì‹ìì¬ ì‹œì¥ì˜ ì¸ì‚¬ì´íŠ¸ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
ì¶œë ¥: 2024ë…„ ëŒ€í•œë¯¼êµ­ ì‹ìì¬ ì‹œì¥ê·œëª¨ í†µê³„ ì§€ì—­ë³„ ìƒì‚°ëŸ‰ ìˆ˜ì¹˜ ë°ì´í„°

ì…ë ¥: "ì§‘ì¤‘í˜¸ìš° í”¼í•´ì§€ì—­ ë†ì‚°ë¬¼ í˜„í™©ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤"
ì¶œë ¥: 2025ë…„ 7ì›” 8ì›” ì§‘ì¤‘í˜¸ìš° í”¼í•´ì§€ì—­ ë†ì‚°ë¬¼ ìƒì‚°ëŸ‰ ê°ì†Œìœ¨ í”¼í•´ì•¡ í†µê³„í‘œ

ì…ë ¥: "ë§Œë‘ ì‹œì¥ ë¶„ì„ì„ í•©ë‹ˆë‹¤"
ì¶œë ¥: 2024ë…„ ëŒ€í•œë¯¼êµ­ ë§Œë‘ ì‹œì¥ê·œëª¨ ë¸Œëœë“œë³„ ì ìœ ìœ¨ ìˆœìœ„ í†µê³„ ë§¤ì¶œì•¡

ì…ë ¥: "ì‹í’ˆ ê°€ê²© ë™í–¥ì„ íŒŒì•…í•©ë‹ˆë‹¤"
ì¶œë ¥: 2024ë…„ 2025ë…„ ì‹í’ˆ í’ˆëª©ë³„ ê°€ê²© ë³€ë™ë¥  ì›”ë³„ ë¬¼ê°€ì§€ìˆ˜ í†µê³„í‘œ

**ì›ë³¸ ìš”ì²­**: "{query}"

**ë³€í™˜ëœ í‚¤ì›Œë“œ ì¡°í•©** (í‚¤ì›Œë“œë§Œ ì¶œë ¥):
"""
            try:
                raw_query = await self._invoke_with_fallback(prompt)

                # [ìˆ˜ì •ë¨] í‚¤ì›Œë“œ ì¡°í•© ì¶”ì¶œ
                optimized_query = self._extract_keywords(raw_query)

                print(f"  - ìµœì í™”ëœ í‚¤ì›Œë“œ: '{optimized_query}'")
                return optimized_query
            except Exception as e:
                print(f"  - ì¿¼ë¦¬ ìµœì í™” ì‹¤íŒ¨, ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©: {e}")
                return query

        # Web SearchëŠ” ë§¥ë½ ì •ë³´ë¥¼ í¬í•¨í•œ ê²€ìƒ‰ í‚¤ì›Œë“œë¡œ ìµœì í™”
        elif tool == "web_search":
            print(f"  - {tool} ì¿¼ë¦¬ ìµœì í™” ì‹œì‘ (ë§¥ë½ ê°•í™”): '{query}'")
            prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì„ ì›¹ ê²€ìƒ‰ì— ìµœì í™”ëœ í‚¤ì›Œë“œë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.
ê²€ìƒ‰ íš¨ê³¼ë¥¼ ë†’ì´ê¸° ìœ„í•´ ì¤‘ìš”í•œ ë§¥ë½ ì •ë³´(êµ­ê°€, ì—°ë„, ëŒ€ìƒ ë“±)ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

ìµœì í™” ê·œì¹™:
1. ì§€ì—­ ì •ë³´ ëª…í™•í™” ë° ê¸°ë³¸ê°’ ì„¤ì •:
   - "êµ­ë‚´" â†’ "ëŒ€í•œë¯¼êµ­"
   - "ìš°ë¦¬ë‚˜ë¼" â†’ "ëŒ€í•œë¯¼êµ­"
   - "í•œêµ­" â†’ "ëŒ€í•œë¯¼êµ­"
   - ì§€ì—­ ì–¸ê¸‰ì´ ì—†ìœ¼ë©´ â†’ "ëŒ€í•œë¯¼êµ­" ìë™ ì¶”ê°€
2. êµ¬ì²´ì ì¸ ì—°ë„ ëª…ì‹œ:
   - "ìµœê·¼" â†’ "2024ë…„ 2025ë…„"
   - "ìš”ì¦˜" â†’ "2024ë…„ 2025ë…„"
   - "í˜„ì¬" â†’ "2025ë…„"
   - "ì‘ë…„" â†’ "2024ë…„"
3. êµ¬ì²´ì ì¸ ë¶„ì•¼ë‚˜ ëŒ€ìƒ ëª…ì‹œ
4. ê²€ìƒ‰ ì˜ë„ì— ë§ëŠ” í‚¤ì›Œë“œ ì¡°í•©

ì˜ˆì‹œ:
- ì›ë³¸: "êµ­ë‚´ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì‹œì¥ í˜„í™©ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤"
- ìµœì í™”: "2024ë…„ 2025ë…„ ëŒ€í•œë¯¼êµ­ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì‹œì¥ í˜„í™© íŠ¸ë Œë“œ"

- ì›ë³¸: "ìš°ë¦¬ë‚˜ë¼ MZì„¸ëŒ€ ì†Œë¹„ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤"
- ìµœì í™”: "2024ë…„ 2025ë…„ ëŒ€í•œë¯¼êµ­ MZì„¸ëŒ€ ì†Œë¹„ íŠ¸ë Œë“œ íŒ¨í„´"

- ì›ë³¸: "ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì‹œì¥ í˜„í™©ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤" (ì§€ì—­ ì–¸ê¸‰ ì—†ìŒ)
- ìµœì í™”: 2024ë…„ 2025ë…„ ëŒ€í•œë¯¼êµ­ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì‹œì¥ í˜„í™© íŠ¸ë Œë“œ

- ì›ë³¸: "í•œêµ­ì˜ ìœ ë§í•œ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ë¶„ì•¼ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤"
- ìµœì í™”: 2024ë…„ 2025ë…„ ëŒ€í•œë¯¼êµ­ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ìœ ë§ ë¶„ì•¼ ì‹œì¥ ì „ë§

- ì›ë³¸: "ìµœê·¼ ì‹ìì¬ ì‹œì¥ íŠ¸ë Œë“œë¥¼ ì¡°ì‚¬í•©ë‹ˆë‹¤"
- ìµœì í™”: 2024ë…„ 2025ë…„ ëŒ€í•œë¯¼êµ­ ì‹ìì¬ ì‹œì¥ íŠ¸ë Œë“œ ë™í–¥ ë¶„ì„

ì¤‘ìš”: í°ë”°ì˜´í‘œë‚˜ ì‘ì€ë”°ì˜´í‘œë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ìì—°ìŠ¤ëŸ¬ìš´ ê²€ìƒ‰ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: "{query}"
ìµœì í™”ëœ ê²€ìƒ‰ í‚¤ì›Œë“œ (ë”°ì˜´í‘œ ì—†ì´):
"""
            try:
                optimized_query = await self._invoke_with_fallback(prompt)
                # ë”°ì˜´í‘œ ì œê±° (í˜¹ì‹œ LLMì´ ì¶”ê°€í–ˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„)
                optimized_query = optimized_query.strip().strip('"').strip("'")
                print(f"  - ìµœì í™”ëœ ê²€ìƒ‰ í‚¤ì›Œë“œ: '{optimized_query}'")
                return optimized_query
            except Exception as e:
                print(f"  - ì¿¼ë¦¬ ìµœì í™” ì‹¤íŒ¨, ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©: {e}")
                return query

        elif tool == "pubmed_search":
            print(f"  - {tool} ì¿¼ë¦¬ ìµœì í™” ì‹œì‘ (ë§¥ë½ ê°•í™”): '{query}'")
            prompt = f"""
ë‹¤ìŒ ì§ˆì˜ì—ì„œ PubMedì— ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ 5ë‹¨ì–´ ì´í•˜ë¡œ ì¶”ì¶œí•˜ê³  **ì˜ì–´**ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”.
ì˜ì–´ë¡œ ë²ˆì—­ëœ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì œì™¸í•œ ë‹¤ë¥¸ ë‚´ìš©ì€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.

ì…ë ¥: {query}
ì¶œë ¥(ì˜ì–´ í‚¤ì›Œë“œ, ë”°ì˜´í‘œ ì—†ì´):
"""
            try:
                optimized_query = await self._invoke_with_fallback(prompt)
                # ë”°ì˜´í‘œ ì œê±° (í˜¹ì‹œ LLMì´ ì¶”ê°€í–ˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„)
                optimized_query = optimized_query.strip().strip('"').strip("'")
                print(f"  - ìµœì í™”ëœ ê²€ìƒ‰ í‚¤ì›Œë“œ: '{optimized_query}'")
                return optimized_query
            except Exception as e:
                print(f"  - ì¿¼ë¦¬ ìµœì í™” ì‹¤íŒ¨, ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©: {e}")
                return query

        # ê¸°íƒ€ ë„êµ¬ëŠ” ì›ë³¸ ì¿¼ë¦¬ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        return query

    def _extract_keywords(self, raw_response: str) -> str:
        """LLM ì‘ë‹µì—ì„œ í‚¤ì›Œë“œ ì¡°í•©ë§Œ ì¶”ì¶œí•˜ëŠ” í—¬í¼ ë©”ì„œë“œ"""
        lines = raw_response.strip().split('\n')

        # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if not line:
                continue
            # ë²ˆí˜¸ ë§¤ê¸°ê¸°, ì„¤ëª…, ì§ˆë¬¸ í˜•íƒœ ì œê±°
            if line.startswith(('1.', '2.', '3.', '4.', '5.', '6.', '-', '*')):
                continue
            if any(word in line for word in ['ì›ë³¸', 'ë³€í™˜', 'í‚¤ì›Œë“œ:', 'ì¶œë ¥:', 'ê²€ìƒ‰ì–´:']):
                continue
            if '**' in line:  # ë§ˆí¬ë‹¤ìš´ í—¤ë” ì œê±°
                continue
            if line.endswith(('?', 'ì¸ê°€ìš”', 'ì…ë‹ˆê¹Œ', 'ë‚˜ìš”')):  # ì§ˆë¬¸ í˜•íƒœ ì œê±°
                continue
            cleaned_lines.append(line)

        # ì²« ë²ˆì§¸ í‚¤ì›Œë“œ ì¡°í•©ë§Œ ë°˜í™˜
        for line in cleaned_lines:
            # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¼ì¸ ì°¾ê¸°
            if any(keyword in line for keyword in ['ì‹œì¥', 'í†µê³„', 'ë°ì´í„°', 'ë§¤ì¶œ', 'ìƒì‚°', 'ê°€ê²©', 'ì ìœ ìœ¨']):
                return line

        # fallback: ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” ë¼ì¸
        if cleaned_lines:
            return cleaned_lines[0]

        return raw_response.strip()

    def _extract_single_question(self, raw_response: str) -> str:
        """LLM ì‘ë‹µì—ì„œ ë‹¨ì¼ ì§ˆë¬¸ë§Œ ì¶”ì¶œí•˜ëŠ” í—¬í¼ ë©”ì„œë“œ (ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€)"""
        lines = raw_response.strip().split('\n')

        # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if not line:
                continue
            # ë²ˆí˜¸ ë§¤ê¸°ê¸°ë‚˜ ë¶€ê°€ ì„¤ëª… ì œê±°
            if line.startswith(('1.', '2.', '3.', '4.', '5.', '6.', '-', '*')):
                continue
            if 'ì›ë³¸' in line or 'ë³€í™˜' in line or 'ì§ˆë¬¸:' in line or 'ì¶œë ¥:' in line:
                continue
            if '**' in line:  # ë§ˆí¬ë‹¤ìš´ í—¤ë” ì œê±°
                continue
            cleaned_lines.append(line)

        # ì²« ë²ˆì§¸ ì§ˆë¬¸ ë¬¸ì¥ë§Œ ë°˜í™˜
        for line in cleaned_lines:
            if line.endswith('?') or line.endswith('ìš”') or line.endswith('ê¹Œ'):
                return line

        # ì ì ˆí•œ ì§ˆë¬¸ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ìœ íš¨í•œ ë¼ì¸ ë°˜í™˜
        if cleaned_lines:
            return cleaned_lines[0]

        # ëª¨ë“  ì²˜ë¦¬ê°€ ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ ì‘ë‹µ ë°˜í™˜
        return raw_response.strip()


    async def execute(self, tool: str, inputs: Dict[str, Any], state: Dict[str, Any] = None) -> Tuple[List[SearchResult], str]:
        """ë‹¨ì¼ ë„êµ¬ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰í•˜ë©°, ì‹¤í–‰ ì „ ì¿¼ë¦¬ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤."""
        if tool not in self.tool_mapping:
            print(f"- ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {tool}")
            return [], ""

        original_query = inputs.get("query", "")
        
        # Vector DB ê²€ìƒ‰ì—ì„œ graph_probe ì •ë³´ í™œìš©
        if tool == "vector_db_search" and state:
            import os
            
            # ì „ì ë°©ì‹ (Graph-to-Vector) ì œì–´ í™˜ê²½ë³€ìˆ˜ ì²´í¬
            disable_graph_to_vector = os.environ.get("DISABLE_GRAPH_TO_VECTOR", "false").lower() == "true"
            
            graph_probe = state.get("metadata", {}).get("graph_probe", {})
            docrels = graph_probe.get("docrels", [])
            isfrom = graph_probe.get("isfrom", [])
            nutrients = graph_probe.get("nutrients", [])
            precheck_disabled = graph_probe.get("precheck_disabled", False)
            
            print(f"  - vector_db_search ì„¤ì •: precheck_disabled={precheck_disabled}, "
            f"graph_to_vector_disabled={disable_graph_to_vector}, "
            f"docrels={len(docrels)}, isfrom={len(isfrom)}, nutrients={len(nutrients)}")

            evidence_exists = bool(docrels or isfrom or nutrients)

            if disable_graph_to_vector:
                print(f"  ğŸ”´ Graph-to-Vector DISABLED: DISABLE_GRAPH_TO_VECTOR={os.environ.get('DISABLE_GRAPH_TO_VECTOR')}")
                print(f"  - Graph-to-Vector ë¹„í™œì„±í™”: ê¸°ë³¸ ì¿¼ë¦¬ ìµœì í™” ìˆ˜í–‰")
                optimized_query = await self._optimize_query_for_tool(original_query, tool)
            else:
                print(f"  ğŸŸ¢ Graph-to-Vector ENABLED: DISABLE_GRAPH_TO_VECTOR={os.environ.get('DISABLE_GRAPH_TO_VECTOR')}")
                print(f"  - Vector DB ê²€ìƒ‰: ì‹¤ì‹œê°„ Graph ì¡°íšŒ ìˆ˜í–‰")
                # Vector DB ê²€ìƒ‰ ì‹œì—ëŠ” ì‹¤ì‹œê°„ìœ¼ë¡œ Graphë¥¼ ì¡°íšŒ
                try:
                    from ...services.database.neo4j_rag_tool import neo4j_graph_search
                    report = await neo4j_graph_search(original_query)
                    text = str(report) if report else ""
                    
                    # ì‹¤ì‹œê°„ìœ¼ë¡œ Graph ì¦ê±° íŒŒì‹±
                    realtime_docrels = self._parse_docrels_from_graph_report(text)
                    realtime_isfrom = self._parse_isfrom_from_graph_report(text)  
                    realtime_nutrients = self._parse_nutrients_from_graph_report(text)

                    print(f"docrels {realtime_docrels}")
                    print(f"isfrom {realtime_isfrom}")
                    print(f"nutrients {realtime_nutrients}")
                    
                    realtime_evidence_exists = bool(realtime_docrels or realtime_isfrom or realtime_nutrients)
                    print(f"  - ì‹¤ì‹œê°„ Graph ì¡°íšŒ ê²°ê³¼: docrels={len(realtime_docrels)}, isfrom={len(realtime_isfrom)}, nutrients={len(realtime_nutrients)}")
                    
                    if realtime_evidence_exists:
                        # â¶ LLMìœ¼ë¡œ 'ì„ ë³„'
                        selected = await self._select_relevant_graph_evidence(
                        original_query, realtime_docrels, realtime_isfrom, realtime_nutrients, k_per_type=50
                        )
                        print(f"  - ì„ ë³„ ê²°ê³¼: docrels={len(selected['docrels'])}, isfrom={len(selected['isfrom'])}, nutrients={len(selected['nutrients'])}")
                        print(f" selected docrels: {selected['docrels']}")
                        print(f" selected isfrom: {selected['isfrom']}")
                        print(f" selected nutrients: {selected['nutrients']}")

                        # â· ì„ ë³„ ê²°ê³¼ë¡œ 'ì¬ì‘ì„±'
                        optimized_query = await self._refine_query_from_graph_all(
                            original_query, selected["docrels"], selected["isfrom"], selected["nutrients"]
                        )
                        print(f"  - ìµœì í™”ëœ ì¿¼ë¦¬: '{optimized_query}'")
                        # state ì—…ë°ì´íŠ¸ (ë©”íƒ€ë°ì´í„° íƒœê¹…ì„ ìœ„í•´)
                        docrels = selected["docrels"]
                        isfrom = selected["isfrom"]
                        nutrients = selected["nutrients"]
                    else:
                        print("  - ì‹¤ì‹œê°„ Graph ì¡°íšŒ: ì¦ê±° ì—†ìŒ, ê¸°ë³¸ ì¿¼ë¦¬ ìµœì í™” ìˆ˜í–‰")
                        optimized_query = await self._optimize_query_for_tool(original_query, tool)
                except Exception as e:
                    print(f"  - ì‹¤ì‹œê°„ Graph ì¡°íšŒ ì‹¤íŒ¨: {e}, ê¸°ë³¸ ì¿¼ë¦¬ ìµœì í™” ìˆ˜í–‰")
                    optimized_query = await self._optimize_query_for_tool(original_query, tool)
            #else:
                #print(f"  - ê·¸ë˜í”„ ì¦ê±° ì—†ìŒ: ê¸°ë³¸ ì¿¼ë¦¬ ìµœì í™” ìˆ˜í–‰")
                #optimized_query = await self._optimize_query_for_tool(original_query, tool)
        else:
            # [ê¸°ì¡´] ì‹¤ì œ ë„êµ¬ ì‹¤í–‰ ì „, ì¿¼ë¦¬ ìµœì í™” ë‹¨ê³„ ì¶”ê°€
            optimized_query = await self._optimize_query_for_tool(original_query, tool)

        # ìµœì í™”ëœ ì¿¼ë¦¬ë¡œ ìƒˆë¡œìš´ inputs ë”•ì…”ë„ˆë¦¬ ìƒì„±
        optimized_inputs = inputs.copy()
        optimized_inputs["query"] = optimized_query

        try:
            print(f"\n>> DataGatherer: '{tool}' ë„êµ¬ ì‹¤í–‰ (ì¿¼ë¦¬: '{optimized_query}')")
            result = await self.tool_mapping[tool](**optimized_inputs)
            
            # Vector DBì—ì„œ Graph-to-Vector ì‚¬ìš© ì‹œ ë©”íƒ€ë°ì´í„° íƒœê¹…
            if tool == "vector_db_search" and state:
                import os
                disable_graph_to_vector = os.environ.get("DISABLE_GRAPH_TO_VECTOR", "false").lower() == "true"
                graph_probe = state.get("graph_probe", {})
                docrels = graph_probe.get("docrels", [])
                isfrom = graph_probe.get("isfrom", [])
                nutrients = graph_probe.get("nutrients", [])
                precheck_disabled = graph_probe.get("precheck_disabled", False)
                
                print(f"  - [DEBUG] Vector DB ë©”íƒ€ë°ì´í„° íƒœê¹… ì¡°ê±´ í™•ì¸:")
                print(f"    - disable_graph_to_vector: {disable_graph_to_vector}")
                print(f"    - precheck_disabled: {precheck_disabled}")
                print(f"    - docrels count: {len(docrels)}")
                print(f"    - isfrom count: {len(isfrom)}")
                print(f"    - nutrients count: {len(nutrients)}")
                print(f"    - result count: {len(result) if result else 0}")
                print(f"    - graph_probe keys: {list(graph_probe.keys())}")
                
                evidence_exists = bool(docrels or isfrom or nutrients)
                print(f"    - evidence_exists: {evidence_exists}")
                print(f"    - ìµœì¢… ì¡°ê±´: not {disable_graph_to_vector} and {evidence_exists}")
                
                if not disable_graph_to_vector and evidence_exists:
                    print(f"  - [DEBUG] Graph-to-Vector ë©”íƒ€ë°ì´í„° íƒœê¹… ì‹œì‘")
                    # Graph-to-Vectorê°€ ì‚¬ìš©ëœ ê²½ìš° ë©”íƒ€ë°ì´í„° íƒœê¹…
                    tagged_count = 0
                    for i, search_result in enumerate(result):
                        print(f"    - ê²°ê³¼ {i+1}: type={type(search_result)}, hasattr metadata={hasattr(search_result, 'metadata')}")
                        if hasattr(search_result, 'metadata'):
                            md = getattr(search_result, "metadata", {}) or {}
                            print(f"      - ê¸°ì¡´ metadata keys: {list(md.keys())}")
                            # ì¿¼ë¦¬ ìµœì í™”ì— ì‚¬ìš©ëœ Graph ì¦ê±° JSON ìƒì„± (preview_jsonê³¼ ë™ì¼í•œ í˜•íƒœ)
                            graph_evidence_json = {
                                "docrels": docrels,
                                "isfrom": isfrom,
                                "nutrients": nutrients
                            }
                            
                            md.update({
                                "derived_from": "graph_evidence",
                                "refined_query": optimized_query,
                                "original_query": original_query,
                                "evidence_counts": {
                                    "docrels": len(docrels),
                                    "isfrom": len(isfrom),
                                    "nutrients": len(nutrients)
                                },
                                "evidence_details": {
                                    "docrels": docrels,  # ëª¨ë“  ë¬¸ì„œê´€ê³„ ì •ë³´ ì €ì¥
                                    "isfrom": isfrom,    # ëª¨ë“  ì›ì‚°ì§€ ì •ë³´ ì €ì¥
                                    "nutrients": nutrients  # ëª¨ë“  ì˜ì–‘ì†Œ ì •ë³´ ì €ì¥
                                },
                                "graph_evidence_json": graph_evidence_json  # ì¿¼ë¦¬ ìµœì í™”ì— ì‚¬ìš©ëœ ì‹¤ì œ Graph ì¦ê±°
                            })
                            search_result.metadata = md
                            print(f"      - ìƒˆ metadata keys: {list(md.keys())}")
                            print(f"      - [DEBUG] docrels íƒ€ì…: {type(docrels)}, ê°œìˆ˜: {len(docrels) if isinstance(docrels, list) else 'N/A'}")
                            print(f"      - [DEBUG] docrels ìƒ˜í”Œ: {docrels[:2] if isinstance(docrels, list) and len(docrels) > 0 else docrels}")
                            print(f"      - [DEBUG] evidence_details ì„¤ì •ë¨: {type(md.get('evidence_details'))}")
                            tagged_count += 1
                    print(f"  - Graph-to-Vector ë©”íƒ€ë°ì´í„° íƒœê¹… ì™„ë£Œ: {tagged_count}ê°œ ê²°ê³¼")
                else:
                    print(f"  - [DEBUG] Graph-to-Vector ë©”íƒ€ë°ì´í„° íƒœê¹… ì¡°ê±´ ë¶ˆë§Œì¡±")
            
            return result, optimized_query
        except Exception as e:
            print(f"- {tool} ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return [], optimized_query


    async def execute_parallel(self, tasks: List[Dict[str, Any]], state: Dict[str, Any] = None) -> Dict[str, List[SearchResult]]:
        """ì—¬ëŸ¬ ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        
        # Graph-to-Vector ì„¤ì • í™•ì¸
        import os
        disable_graph_to_vector = os.environ.get("DISABLE_GRAPH_TO_VECTOR", "false").lower() == "true"
        
        # ì‘ì—… ë¶„ë¥˜
        vector_tasks = [task for task in tasks if task.get("tool") == "vector_db_search"]
        graph_tasks = [task for task in tasks if task.get("tool") == "graph_db_search"]
        other_tasks = [task for task in tasks if task.get("tool") not in ["vector_db_search", "graph_db_search"]]
        
        # Graph-to-Vectorê°€ í™œì„±í™”ë˜ê³  Vector ì‘ì—…ì´ ìˆëŠ” ê²½ìš° ìˆœì°¨ ì‹¤í–‰
        if not disable_graph_to_vector and vector_tasks:
            print(f"\n>> DataGatherer: Graph-to-Vector í™œì„±í™” - ìˆœì°¨ ì‹¤í–‰ ëª¨ë“œ")
            print(f"   - Graph ìš°ì„  ì‹¤í–‰: {len(graph_tasks)}ê°œ")
            print(f"   - Vector í›„ì† ì‹¤í–‰: {len(vector_tasks)}ê°œ") 
            print(f"   - ê¸°íƒ€ ë³‘ë ¬ ì‹¤í–‰: {len(other_tasks)}ê°œ")
            
            organized_results = {}
            
            # 1ë‹¨ê³„: Graph ê²€ìƒ‰ ë¨¼ì € ì‹¤í–‰ (Graph ì‘ì—…ì´ ì—†ìœ¼ë©´ Vector ì¿¼ë¦¬ë¡œ Graph ê²€ìƒ‰ ìƒì„±)
            if not graph_tasks:
                # Vector ì‘ì—…ì´ ìˆì§€ë§Œ Graph ì‘ì—…ì´ ì—†ëŠ” ê²½ìš°, ì²« ë²ˆì§¸ Vector ì¿¼ë¦¬ë¡œ Graph ê²€ìƒ‰ ìë™ ìƒì„±
                first_vector_task = vector_tasks[0]
                vector_query = first_vector_task.get("inputs", {}).get("query", "")
                print(f"\nğŸ” 1ë‹¨ê³„: Graph ê²€ìƒ‰ ìë™ ìƒì„± (Vector ì¿¼ë¦¬ ê¸°ë°˜: '{vector_query}')")
                
                # Graph ê²€ìƒ‰ ì‹¤í–‰
                try:
                    graph_results, _ = await self.execute("graph_db_search", {"query": vector_query}, state)
                    print(f"  - graph_db_search ì™„ë£Œ: {len(graph_results)}ê°œ ê²°ê³¼")
                    organized_results["graph_db_search_0"] = graph_results
                except Exception as e:
                    print(f"  - graph_db_search ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                    organized_results["graph_db_search_0"] = []
            else:
                print("\nğŸ” 1ë‹¨ê³„: Graph ê²€ìƒ‰ ì‹¤í–‰")
                graph_coroutines = [self.execute(task.get("tool"), task.get("inputs", {}), state) for task in graph_tasks]
                graph_results = await asyncio.gather(*graph_coroutines, return_exceptions=True)
                
                for i, task in enumerate(graph_tasks):
                    tool_name = task.get("tool", f"unknown_tool_{i}")
                    result = graph_results[i]
                    
                    if isinstance(result, Exception):
                        print(f"  - {tool_name} ì‹¤í–‰ ì˜¤ë¥˜: {result}")
                        organized_results[f"{tool_name}_{i}"] = []
                    else:
                        search_results, optimized_query = result
                        print(f"  - {tool_name} ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
                        organized_results[f"{tool_name}_{len(organized_results)}"] = search_results
            
            # 2ë‹¨ê³„: Vector ê²€ìƒ‰ ì‹¤í–‰ (Graph ì •ë³´ê°€ stateì— ë°˜ì˜ë¨)
            if vector_tasks:
                print("\nğŸ” 2ë‹¨ê³„: Vector ê²€ìƒ‰ ì‹¤í–‰ (Graph ì •ë³´ ë°˜ì˜)")
                vector_coroutines = [self.execute(task.get("tool"), task.get("inputs", {}), state) for task in vector_tasks]
                vector_results = await asyncio.gather(*vector_coroutines, return_exceptions=True)
                
                for i, task in enumerate(vector_tasks):
                    tool_name = task.get("tool", f"unknown_tool_{i}")
                    result = vector_results[i]
                    
                    if isinstance(result, Exception):
                        print(f"  - {tool_name} ì‹¤í–‰ ì˜¤ë¥˜: {result}")
                        organized_results[f"{tool_name}_{len(organized_results)}"] = []
                    else:
                        search_results, optimized_query = result
                        print(f"  - {tool_name} ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
                        organized_results[f"{tool_name}_{len(organized_results)}"] = search_results
            
            # 3ë‹¨ê³„: ê¸°íƒ€ ê²€ìƒ‰ë“¤ ë³‘ë ¬ ì‹¤í–‰
            if other_tasks:
                print(f"\nğŸ” 3ë‹¨ê³„: ê¸°íƒ€ ê²€ìƒ‰ {len(other_tasks)}ê°œ ë³‘ë ¬ ì‹¤í–‰")
                other_coroutines = [self.execute(task.get("tool"), task.get("inputs", {}), state) for task in other_tasks]
                other_results = await asyncio.gather(*other_coroutines, return_exceptions=True)
                
                for i, task in enumerate(other_tasks):
                    tool_name = task.get("tool", f"unknown_tool_{i}")
                    result = other_results[i]
                    
                    if isinstance(result, Exception):
                        print(f"  - {tool_name} ì‹¤í–‰ ì˜¤ë¥˜: {result}")
                        organized_results[f"{tool_name}_{len(organized_results)}"] = []
                    else:
                        search_results, optimized_query = result
                        print(f"  - {tool_name} ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
                        organized_results[f"{tool_name}_{len(organized_results)}"] = search_results
                        
            return organized_results
            
        else:
            # ê¸°ì¡´ ë³‘ë ¬ ì‹¤í–‰ ëª¨ë“œ
            print(f"\n>> DataGatherer: {len(tasks)}ê°œ ì‘ì—… ë³‘ë ¬ ì‹¤í–‰ ì‹œì‘ (Graph-to-Vector ë¹„í™œì„±í™”)")
            
            # ê° ì‘ì—…ì— ëŒ€í•´ execute ì½”ë£¨í‹´ì„ ìƒì„±í•©ë‹ˆë‹¤. execute ë‚´ë¶€ì—ì„œ ì¿¼ë¦¬ ìµœì í™”ê°€ ìë™ìœ¼ë¡œ ì¼ì–´ë‚©ë‹ˆë‹¤.
            coroutines = [self.execute(task.get("tool"), task.get("inputs", {}), state) for task in tasks]

            # asyncio.gatherë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ì‘ì—…ì„ ë™ì‹œì— ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°›ìŠµë‹ˆë‹¤.
            results = await asyncio.gather(*coroutines, return_exceptions=True)



        organized_results = {}
        for i, task in enumerate(tasks):
            tool_name = task.get("tool", f"unknown_tool_{i}")
            result = results[i]

            # ì‘ì—… ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ê°€ ë°œìƒí–ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
            if isinstance(result, Exception):
                print(f"  - {tool_name} ë³‘ë ¬ ì‹¤í–‰ ì˜¤ë¥˜: {result}")
                organized_results[f"{tool_name}_{i}"] = []
            else:
                print(f"  - {tool_name} ë³‘ë ¬ ì‹¤í–‰ ì™„ë£Œ: {len(result)}ê°œ ê²°ê³¼")
                print(f"  - {tool_name} ë³‘ë ¬ ì‹¤í–‰ ê²°ê³¼: {result[:3]}...")  # ì²˜ìŒ 3ê°œ ê²°ê³¼ë§Œ ì¶œë ¥
                search_results, optimized_query = result  # íŠœí”Œ ì–¸íŒ¨í‚¹
                organized_results[f"{tool_name}_{i}"] = search_results

        return organized_results

    async def execute_parallel_streaming(self, tasks: List[Dict[str, Any]], state: Dict[str, Any] = None):
        """ì—¬ëŸ¬ ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ë˜, ê° ì‘ì—…ì´ ì™„ë£Œë  ë•Œë§ˆë‹¤ ì‹¤ì‹œê°„ìœ¼ë¡œ yieldí•©ë‹ˆë‹¤."""
        print(f"\n>> DataGatherer: {len(tasks)}ê°œ ì‘ì—… ìŠ¤íŠ¸ë¦¬ë° ë³‘ë ¬ ì‹¤í–‰ ì‹œì‘")

        # ë””ë²„ê¹…
        import pprint
        print("\n-- Tasks to be executed --")
        pprint.pprint(tasks, width=100, depth=2)
        print("\n-- Tasks to be executed --")

        # ê° íƒœìŠ¤í¬ì— ì¸ë±ìŠ¤ë¥¼ í• ë‹¹í•˜ì—¬ ìˆœì„œë¥¼ ì¶”ì 
        async def execute_with_callback(task_index: int, task: Dict[str, Any]):
            tool_name = task.get("tool", f"unknown_tool_{task_index}")
            inputs = task.get("inputs", {})
            query = inputs.get("query", "")

            try:
                print(f"  - {tool_name} ì‹œì‘: {query}")
                result, optimized_query = await self.execute(tool_name, inputs, state)  # state ì „ë‹¬
                print(f"  - {tool_name} ì™„ë£Œ: {len(result)}ê°œ ê²°ê³¼")

                # í”„ë¡ íŠ¸ì—”ë“œê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                formatted_results = []
                for i, search_result in enumerate(result):
                    result_dict = search_result.model_dump()
                    
                    print(f"  - [DEBUG] ê²°ê³¼ {i+1} í¬ë§· ë³€í™˜:")
                    print(f"    - ì›ë³¸ SearchResult metadata: {getattr(search_result, 'metadata', {})}")
                    print(f"    - model_dump metadata: {result_dict.get('metadata', {})}")
                    
                    formatted_result = {
                        "title": result_dict.get("title", "ì œëª© ì—†ìŒ"),
                        "content": result_dict.get("content", "content ì—†ìŒ"),
                        "url": result_dict.get("url", "url ì—†ìŒ"),
                        "source": result_dict.get("source", tool_name),
                        "score": result_dict.get("score", 0.0),
                        "metadata": result_dict.get("metadata", {}),  # ë©”íƒ€ë°ì´í„° ì¶”ê°€ - Graph-to-Vector ì •ë³´ ë³´ì¡´
                    }
                    formatted_results.append(formatted_result)

                    metadata = formatted_result.get("metadata", {})
                    if metadata.get("derived_from") == "graph_evidence":
                        print(f"    - [SUCCESS] Graph-to-Vector ë©”íƒ€ë°ì´í„° ë³´ì¡´ë¨: {metadata}")
                    else:
                        print(f"    - [WARNING] Graph-to-Vector ë©”íƒ€ë°ì´í„° ì—†ìŒ: {metadata}")
                    
                    print(f"  - {tool_name} ê²°ê³¼ í¬ë§· ì™„ë£Œ (metadata keys: {list(formatted_result.get('metadata', {}).keys())})")

                return {
                    "step": task_index + 1,
                    "tool_name": tool_name,
                    "query": optimized_query,
                    "results": formatted_results,
                    "original_results": result  # ì›ë³¸ SearchResult ê°ì²´ë“¤ë„ ë³´ì¡´
                }

            except Exception as e:
                print(f"  - {tool_name} ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                return {
                    "step": task_index + 1,
                    "tool_name": tool_name,
                    "query": optimized_query,
                    "results": [],
                    "error": str(e),
                    "original_results": []
                }

        # ëª¨ë“  ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹œì‘í•˜ê³ , ì™„ë£Œë˜ëŠ” ëŒ€ë¡œ yield
        tasks_coroutines = [execute_with_callback(i, task) for i, task in enumerate(tasks)]

        # asyncio.as_completedë¥¼ ì‚¬ìš©í•˜ì—¬ ì™„ë£Œë˜ëŠ” ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ì²˜ë¦¬
        collected_data = []

        for coro in asyncio.as_completed(tasks_coroutines):
            result = await coro
            collected_data.extend(result.get("original_results", []))

            # ê°œë³„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¦‰ì‹œ yield
            yield {
                "type": "search_results",
                "data": {
                    "step": result["step"],
                    "tool_name": result["tool_name"],
                    "query": result["query"],
                    "results": result["results"],
                    "message_id": state.get("message_id") if state else None
                }
            }

        # ëª¨ë“  ê²€ìƒ‰ì´ ì™„ë£Œëœ í›„ ì „ì²´ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë§ˆì§€ë§‰ì— yield
        yield {
            "type": "collection_complete",
            "data": {
                "total_results": len(collected_data),
                # SearchResult ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
                # SearchResultê°€ Pydantic ëª¨ë¸ì´ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.
                "collected_data": [res.model_dump() for res in collected_data]
            }
        }


    async def _web_search(self, query: str) -> List[SearchResult]:
        """ì›¹ ê²€ìƒ‰ ì‹¤í–‰ - ì•ˆì •ì„± ê°•í™”"""
        try:
            # ìµœì í™”ëœ ì¿¼ë¦¬ ì‚¬ìš© (ì´ë¯¸ _optimize_query_for_toolì—ì„œ ì²˜ë¦¬ë¨)
            print(f"  - ì›¹ ê²€ìƒ‰ ì‹¤í–‰ ì¿¼ë¦¬: {query}")

            # ì „ì—­ ThreadPoolExecutor ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬
            loop = asyncio.get_event_loop()
            result_text = await loop.run_in_executor(
                _global_executor,  # ì „ì—­ executor ì‚¬ìš©
                debug_web_search,
                query
            )

            # ê²°ê³¼ê°€ ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹±
            search_results = []
            if result_text and isinstance(result_text, str):
                # ê°„ë‹¨í•œ íŒŒì‹±ìœ¼ë¡œ SearchResult ê°ì²´ ìƒì„±
                lines = result_text.split('\n')
                current_result = {}

                for line in lines:
                    line = line.strip()
                    if line.startswith(('1.', '2.', '3.', '4.', '5.')):
                        # ì´ì „ ê²°ê³¼ ì €ì¥
                        if current_result:
                            search_result = SearchResult(
                                source="web_search",
                                content=current_result.get("snippet", ""),
                                search_query=query,
                                title=current_result.get("title", "ì›¹ ê²€ìƒ‰ ê²°ê³¼"),
                                url=current_result.get("link"),
                                score=0.9,  # ì›¹ê²€ìƒ‰ ê²°ê³¼ëŠ” ë†’ì€ ì ìˆ˜
                                timestamp=datetime.now().isoformat(),
                                document_type="web",
                                metadata={"optimized_query": query, **current_result},
                                source_url=current_result.get("link", "ì›¹ ê²€ìƒ‰ ê²°ê³¼")
                            )
                            search_results.append(search_result)

                        # ìƒˆ ê²°ê³¼ ì‹œì‘
                        current_result = {"title": line[3:].strip()}  # ë²ˆí˜¸ ì œê±°
                    elif line.startswith("ì¶œì²˜ ë§í¬:"):
                        current_result["link"] = line[7:].strip()  # "ì¶œì²˜ ë§í¬:" ì œê±°
                    elif line.startswith("ìš”ì•½:"):
                        current_result["snippet"] = line[3:].strip()

                # ë§ˆì§€ë§‰ ê²°ê³¼ ì €ì¥
                if current_result:
                    search_result = SearchResult(
                        source="web_search",
                        content=current_result.get("snippet", ""),
                        search_query=query,
                        title=current_result.get("title", "ì›¹ ê²€ìƒ‰ ê²°ê³¼"),
                        url=current_result.get("link"),
                        score=0.9,  # ì›¹ê²€ìƒ‰ ê²°ê³¼ëŠ” ë†’ì€ ì ìˆ˜
                        timestamp=datetime.now().isoformat(),
                        document_type="web",
                        metadata={
                            "optimized_query": query,
                            "link": current_result.get("link"),  # ì¶œì²˜ ë§í¬ í¬í•¨
                            **current_result
                        },
                        source_url=current_result.get("link", "ì›¹ ê²€ìƒ‰ ê²°ê³¼")
                    )
                    search_results.append(search_result)

            print(f"  - ì›¹ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            return search_results[:5]  # ìƒìœ„ 5ê°œ ê²°ê³¼ë§Œ

        except concurrent.futures.TimeoutError:
            print(f"ì›¹ ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ: {query}")
            return []
        except Exception as e:
            print(f"ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    async def _vector_db_search(self, query: str) -> List[SearchResult]:
        """Vector DB ê²€ìƒ‰ ì‹¤í–‰ - ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”"""
        try:
            print(f">> Vector DB ê²€ìƒ‰ ì‹œì‘: {query}")

            # ì „ì—­ ThreadPoolExecutor ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬
            loop = asyncio.get_event_loop()
            results = await loop.run_in_executor(
                _global_executor,  # ì „ì—­ executor ì‚¬ìš©
                vector_db_search,
                query
            )

            search_results = []
            for result in results:
                if isinstance(result, dict):
                    # ìƒˆë¡œìš´ doc_linkì™€ page_number í•„ë“œ ì‚¬ìš©
                    doc_link = result.get("source_url", "")
                    page_number = result.get("page_number", [])
                    # ë¬¸ì„œ ì œëª© ì¶”ì¶œ
                    doc_title = result.get("title", "")
                    meta_data = result.get("meta_data", {})
                    # ì œëª©ì— í˜ì´ì§€ ë²ˆí˜¸ ì¶”ê°€
                    full_title = f"{doc_title}, ({', '.join([f'p.{num}' for num in page_number])})".strip()
                    score = result.get("score", 5.2)
                    chunk_id = result.get("chunk_id", "")

                    search_results.append(SearchResult(
                        source="vector_db",
                        content=result.get("content", ""),
                        search_query=query,
                        title=full_title,
                        document_type="database",
                        score=score,
                        metadata = meta_data,
                        url=doc_link,  # ìƒˆ í•„ë“œ ì¶”ê°€
                        chunk_id = chunk_id,
                    ))


            print(f"  - Vector DB ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            return search_results[:5]

        # except concurrent.futures.TimeoutError:
        #     print(f"Vector DB ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ: {query}")
        #     return []
        except Exception as e:
            print(f"Vector DB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

    async def _graph_db_search(self, query: str) -> List[SearchResult]:
        """Graph DB ê²€ìƒ‰ ì‹¤í–‰ - í¬ë§· ë¶ˆì¼ì¹˜ ë°©ì§€ ë° íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬"""
        print(f"  - Graph DB ê²€ìƒ‰ ì‹œì‘: {query}")
        import concurrent.futures

        try:
            # graph_db_searchê°€ ë™ê¸° í•¨ìˆ˜ë¼ë©´
            loop = asyncio.get_running_loop()
            raw_results = await loop.run_in_executor(None, graph_db_search, query)
            # ë§Œì•½ langchain Toolì¼ ê²½ìš°:
            # with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            #     raw_results = executor.submit(graph_db_search.invoke, {"query": query}).result(timeout=20)

            search_results: List[SearchResult] = []

            if isinstance(raw_results, list):
                # list of dict or list of str
                for res in raw_results:
                    if isinstance(res, dict):
                        # Graph DB ê²°ê³¼ì˜ ì˜ë¯¸ìˆëŠ” ì œëª© ìƒì„±
                        title_candidates = [
                            res.get("title"),
                            res.get("entity"),
                            res.get("product"),
                            res.get("name"),
                            res.get("í’ˆëª©ëª…"),
                            res.get("ì›ì‚°ì§€"),
                            res.get("ì˜ì–‘ì†Œëª…")
                        ]
                        title = next((t for t in title_candidates if t), "ê·¸ë˜í”„ ì •ë³´")

                        # ê´€ê³„í˜• ë°ì´í„°ì˜ ì˜ë¯¸ìˆëŠ” ë‚´ìš© ìƒì„±
                        content_parts = []
                        relationship_info = []

                        # ê´€ê³„ ì •ë³´ ì¶”ì¶œ
                        for key, value in res.items():
                            if "ê´€ê³„" in key or "ì—°ê²°" in key or key.endswith("_ê´€ê³„"):
                                relationship_info.append(f"ğŸ”— {key}: {value}")
                            elif key not in ['title', 'content', 'entity', 'product'] and value:
                                content_parts.append(f"â€¢ {key}: {value}")

                        if relationship_info:
                            content_parts = relationship_info + content_parts

                        content = res.get("content") or "\n".join(content_parts[:8]) or json.dumps(res, ensure_ascii=False, indent=2)
                        score = res.get("confidence") or res.get("score") or 0.8
                    else:
                        title = f"ê·¸ë˜í”„ ê´€ê³„ ì •ë³´"
                        content = str(res)
                        score = 0.8

                    search_results.append(
                        SearchResult(
                            source="graph_db",
                            content=content,
                            search_query=query,
                            title=title,
                            score=score,
                            document_type="graph",
                            url=""  # ë¹ˆ ë¬¸ìì—´ë¡œ í†µì¼ (None ì“°ë©´ í›„ë‹¨ì—ì„œ ê¹¨ì§€ëŠ” ê²½ìš° ìˆìŒ)
                        )
                    )
            elif isinstance(raw_results, dict):
                # ë‹¨ì¼ ê·¸ë˜í”„ ê²°ê³¼ì˜ ì˜ë¯¸ìˆëŠ” ì œëª© ìƒì„±
                title_candidates = [
                    raw_results.get("title"),
                    raw_results.get("entity"),
                    raw_results.get("product"),
                    raw_results.get("name"),
                    raw_results.get("í’ˆëª©ëª…")
                ]
                title = next((t for t in title_candidates if t), "ê·¸ë˜í”„ ê´€ê³„ ì •ë³´")

                # êµ¬ì¡°í™”ëœ ë‚´ìš© ìƒì„±
                content_parts = []
                for key, value in raw_results.items():
                    if key not in ['title', 'content', 'entity', 'product'] and value:
                        if "ê´€ê³„" in key or "ì—°ê²°" in key:
                            content_parts.append(f"ğŸ”— {key}: {value}")
                        else:
                            content_parts.append(f"â€¢ {key}: {value}")

                content = raw_results.get("content") or "\n".join(content_parts[:8]) or json.dumps(raw_results, ensure_ascii=False, indent=2)
                score = raw_results.get("confidence") or raw_results.get("score") or 0.8
                search_results.append(
                    SearchResult(
                        source="graph_db",
                        content=content,
                        search_query=query,
                        title=title,
                        score=score,
                        document_type="graph",
                        url=""
                    )
                )
            else:
                # ë¬¸ìì—´ summary ê°™ì€ ê²½ìš°
                content = str(raw_results)
                title = "ê·¸ë˜í”„ ê²€ìƒ‰ ìš”ì•½"
                search_results.append(
                    SearchResult(
                        source="graph_db",
                        content=content,
                        search_query=query,
                        title=title,
                        score=0.6,
                        document_type="graph",
                        url=""
                    )
                )

            # # === ë¬¸ì„œ ê´€ê³„(docrels) ê¸°ë°˜ ë²¡í„° DB í›„ì† ê²€ìƒ‰ ===
            # # Graph-to-Vector ì„¤ì • í™•ì¸
            # import os
            # disable_graph_to_vector = os.environ.get("DISABLE_GRAPH_TO_VECTOR", "false").lower() == "true"
            
            # if not disable_graph_to_vector:
            #     print("  - Graph-to-Vector í™œì„±í™”: Graph DB í›„ Vector DB í™•ì¥ ê²€ìƒ‰ ì‹œì‘")
            #     try:
            #         # 1) ê·¸ë˜í”„ ë¦¬í¬íŠ¸ì—ì„œ ì¦ê±° ì¶”ì¶œ
            #         docrels = []
            #         isfrom  = []
            #         nutrs   = []

            #         if isinstance(raw_results, str):
            #             docrels = self._parse_docrels_from_graph_report(raw_results)
            #             isfrom  = self._parse_isfrom_from_graph_report(raw_results)
            #             nutrs   = self._parse_nutrients_from_graph_report(raw_results)
            #         elif isinstance(raw_results, dict):
            #             # í–¥í›„ êµ¬ì¡°í™” ë°˜í™˜ì„ ëŒ€ë¹„í•œ í´ë°±
            #             docrels = raw_results.get("docrels", []) if isinstance(raw_results.get("docrels"), list) else []
            #             isfrom  = raw_results.get("isfrom",  []) if isinstance(raw_results.get("isfrom"),  list) else []
            #             nutrs   = raw_results.get("nutrients",[]) if isinstance(raw_results.get("nutrients"),list) else []

            #         # 2) ê·¸ë˜í”„ ì „ evidenceë¥¼ ë°˜ì˜í•´ ë²¡í„° ì¿¼ë¦¬ ì¬ì‘ì„±
            #         refined_query = await self._refine_query_from_graph_all(query, docrels, isfrom, nutrs)
            #         print(f"  - Graph evidence ê¸°ë°˜ ì¬ì‘ì„± ì¿¼ë¦¬: {refined_query}")

            #         # 3) ë²¡í„° DB ì¬ê²€ìƒ‰ ì‹¤í–‰
            #         vec_results = await self._vector_db_search(refined_query)

            #         # 4) ë©”íƒ€ë°ì´í„° íƒœê¹… ë° ì ìˆ˜ ë³´ì •
            #         for vr in vec_results or []:
            #             md = getattr(vr, "metadata", {}) or {}
            #             md.update({
            #                 "derived_from": "graph_evidence",
            #                 "refined_query": refined_query,
            #                 "evidence_counts": {
            #                     "docrels": len(docrels),
            #                     "isfrom": len(isfrom),
            #                     "nutrients": len(nutrs)
            #                 },
            #                 "evidence_details": {
            #                     "docrels": docrels,
            #                     "isfrom": isfrom,
            #                     "nutrients": nutrs
            #                 },
            #                 "graph_evidence_json": {
            #                     "docrels": docrels,
            #                     "isfrom": isfrom,
            #                     "nutrients": nutrs
            #                 }
            #             })
            #             vr.metadata = md
            #             try:
            #                 vr.score = max(vr.score or 0.0, 0.85)
            #             except Exception:
            #                 pass

            #         search_results.extend(vec_results or [])
            #         print(f"  - Graphâ†’Vector í™•ì¥ ê²€ìƒ‰ ì™„ë£Œ: {len(vec_results or [])}ê°œ ì¶”ê°€ ê²°ê³¼")

            #     except Exception as follow_err:
            #         print(f"  - Graphâ†’Vector í›„ì† ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {follow_err}")
            # else:
            #     print("  - Graph-to-Vector ë¹„í™œì„±í™”: Graph DB í›„ Vector DB í™•ì¥ ê²€ìƒ‰ ìƒëµ")

            print(f"  - Graph DB ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            return search_results

        except concurrent.futures.TimeoutError:
            print(f"Graph DB ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ: {query}")
            return []
        except Exception as e:
            print(f"Graph DB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    async def _arxiv_search(self, query: str) -> List[SearchResult]:
        """arXiv ë…¼ë¬¸ ê²€ìƒ‰ ì‹¤í–‰ - í•™ìˆ  ë…¼ë¬¸ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸"""
        try:
            print(f"  - arXiv ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘: {query}")

            # arXiv ê²€ìƒ‰ í•¨ìˆ˜ë¥¼ ì§ì ‘ í˜¸ì¶œ (langchain tool ìš°íšŒ)
            loop = asyncio.get_event_loop()

            def _direct_arxiv_search(query_text, max_results=5):
                try:
                    # ê²€ìƒ‰ ì¿¼ë¦¬ URL ì¸ì½”ë”©
                    base_url = "https://export.arxiv.org/api/query?"
                    ##search_query = urllib.parse.quote(query_text)

                    # arXiv API íŒŒë¼ë¯¸í„°
                    params = {
                        'search_query': f'all:{query}',
                        'start': 0,
                        'max_results': max_results,
                        'sortBy': 'lastUpdatedDate',
                        'sortOrder': 'descending'
                    }

                    # URL ìƒì„±
                    url = base_url + urllib.parse.urlencode(params)
                    print(f"  - API URL: {url}")

                    # API í˜¸ì¶œ
                    # response = urllib.request.urlopen(url, timeout=10)
                    # data = response.read().decode('utf-8')
                    req = urllib.request.Request(
                        url,
                        headers={"User-Agent": "MyArxivClient/1.0 (contact: youremail@example.com)"}
                    )

                    with urllib.request.urlopen(req, timeout=15) as resp:
                        data = resp.read().decode("utf-8")
                    # XML íŒŒì‹±
                    root = ET.fromstring(data)

                    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜
                    ns = {
                        'atom': 'http://www.w3.org/2005/Atom',
                        'arxiv': 'http://arxiv.org/schemas/atom'
                    }

                    # ê²°ê³¼ íŒŒì‹±
                    entries = root.findall('atom:entry', ns)

                    if not entries:
                        return f"'{query_text}'ì— ëŒ€í•œ arXiv ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

                    results = []
                    for i, entry in enumerate(entries[:max_results], 1):
                        # ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ
                        title = entry.find('atom:title', ns).text.strip().replace('\n', ' ')

                        # ì €ì ì •ë³´
                        authors = entry.findall('atom:author', ns)
                        author_names = [author.find('atom:name', ns).text for author in authors]
                        author_str = ', '.join(author_names[:3])
                        if len(author_names) > 3:
                            author_str += f' ì™¸ {len(author_names)-3}ëª…'

                        # ì´ˆë¡
                        summary = entry.find('atom:summary', ns).text.strip()
                        if len(summary) > 300:
                            summary = summary[:297] + "..."

                        # ë°œí–‰ì¼
                        published = entry.find('atom:published', ns).text
                        pub_date = datetime.strptime(published[:10], '%Y-%m-%d').strftime('%Yë…„ %mì›” %dì¼')

                        # ì¹´í…Œê³ ë¦¬
                        categories = entry.findall('atom:category', ns)
                        cat_list = [cat.get('term') for cat in categories]
                        categories_str = ', '.join(cat_list[:3])

                        # ë…¼ë¬¸ ë§í¬
                        pdf_link = None
                        for link in entry.findall('atom:link', ns):
                            if link.get('type') == 'application/pdf':
                                pdf_link = link.get('href')
                                break

                        if not pdf_link:
                            pdf_link = entry.find('atom:id', ns).text.replace('abs', 'pdf')

                        # ê²°ê³¼ í¬ë§·íŒ…
                        result_text = f"{i}. ğŸ“„ {title}\n"
                        result_text += f"   ì €ì: {author_str}\n"
                        result_text += f"   ë°œí–‰ì¼: {pub_date}\n"
                        result_text += f"   ë¶„ì•¼: {categories_str}\n"
                        result_text += f"   PDF: {pdf_link}\n"
                        result_text += f"   ì´ˆë¡: {summary}\n"

                        results.append(result_text)

                    # ìµœì¢… ê²°ê³¼ ë°˜í™˜
                    final_result = f"arXiv ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼ (ê²€ìƒ‰ì–´: {query_text}):\n"
                    final_result += f"ì´ {len(results)}ê°œ ë…¼ë¬¸ ë°œê²¬\n\n"
                    final_result += "\n".join(results)

                    return final_result

                except Exception as e:
                    return f"arXiv ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

            result_text = await loop.run_in_executor(
                _global_executor,
                _direct_arxiv_search,
                query,
                5
            )

            # ê²°ê³¼ íŒŒì‹±
            search_results = []
            if result_text and isinstance(result_text, str):
                lines = result_text.split('\n')
                current_paper = {}

                for line in lines:
                    line = line.strip()
                    if line.startswith(('1.', '2.', '3.', '4.', '5.')):
                        # ì´ì „ ë…¼ë¬¸ ì €ì¥
                        if current_paper:
                            search_result = SearchResult(
                                source="arxiv_search",
                                content=current_paper.get("ì´ˆë¡", ""),
                                search_query=query,
                                title=current_paper.get("title", "arXiv ë…¼ë¬¸"),
                                url=current_paper.get("pdf", ""),
                                score=0.95,  # í•™ìˆ  ë…¼ë¬¸ì€ ë†’ì€ ì‹ ë¢°ë„
                                timestamp=datetime.now().isoformat(),
                                document_type="research_paper",
                                metadata={
                                    "authors": current_paper.get("ì €ì", ""),
                                    "date": current_paper.get("ë°œí–‰ì¼", ""),
                                    "categories": current_paper.get("ë¶„ì•¼", ""),
                                    "optimized_query": query
                                },
                                source_url=current_paper.get("pdf", "")
                            )
                            search_results.append(search_result)
                        # ìƒˆ ë…¼ë¬¸ ì‹œì‘
                        current_paper = {}
                        title_part = line.split('ğŸ“„', 1)
                        if len(title_part) > 1:
                            current_paper["title"] = title_part[1].strip()
                    elif 'ì €ì:' in line:
                        current_paper["ì €ì"] = line.split('ì €ì:', 1)[1].strip()
                    elif 'ë°œí–‰ì¼:' in line:
                        current_paper["ë°œí–‰ì¼"] = line.split('ë°œí–‰ì¼:', 1)[1].strip()
                    elif 'ë¶„ì•¼:' in line:
                        current_paper["ë¶„ì•¼"] = line.split('ë¶„ì•¼:', 1)[1].strip()
                    elif 'PDF:' in line:
                        current_paper["pdf"] = line.split('PDF:', 1)[1].strip()
                    elif 'ì´ˆë¡:' in line:
                        current_paper["ì´ˆë¡"] = line.split('ì´ˆë¡:', 1)[1].strip()

                # ë§ˆì§€ë§‰ ë…¼ë¬¸ ì €ì¥
                if current_paper:
                    search_result = SearchResult(
                        source="arxiv_search",
                        content=current_paper.get("ì´ˆë¡", ""),
                        search_query=query,
                        title=current_paper.get("title", "arXiv ë…¼ë¬¸"),
                        url=current_paper.get("pdf", ""),
                        score=0.95,
                        timestamp=datetime.now().isoformat(),
                        document_type="research_paper",
                        metadata={
                            "authors": current_paper.get("ì €ì", ""),
                            "date": current_paper.get("ë°œí–‰ì¼", ""),
                            "categories": current_paper.get("ë¶„ì•¼", ""),
                            "optimized_query": query
                        },
                        source_url=current_paper.get("pdf", "")
                    )
                    search_results.append(search_result)

            print(f"  - arXiv ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ë…¼ë¬¸")
            return search_results[:5]  # ìµœëŒ€ 5ê°œ ë…¼ë¬¸ë§Œ ë°˜í™˜

        except Exception as e:
            print(f"  - arXiv ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []


    async def _pubmed_search(self, query: str) -> List[SearchResult]:
        """PubMed ë…¼ë¬¸ ê²€ìƒ‰ ì‹¤í–‰ - í•™ìˆ  ë…¼ë¬¸ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ (ì˜ì–´ ë²ˆì—­ í›„ ê²€ìƒ‰)"""
        try:
            print(f"  - PubMed ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘ (ì›ë¬¸ ì¿¼ë¦¬): {query}")
            loop = asyncio.get_event_loop()

            # === [NEW] 1) ì¿¼ë¦¬ ì˜ì–´ ë²ˆì—­ (LLM) ==========================

            try:
                english_query_raw = (query or "").strip()

                # ì½”ë“œíœìŠ¤ ì œê±° ë° ì²« ë¹„ì–´ìˆì§€ ì•Šì€ ë¼ì¸ë§Œ ì‚¬ìš©
                if english_query_raw.startswith("```"):
                    # ```...``` ë¸”ë¡ ì•ˆì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ
                    end_idx = english_query_raw.rfind("```")
                    if end_idx != -1:
                        english_query_raw = english_query_raw[3:end_idx].strip()
                    # í”í•œ 'json', 'text' ê°™ì€ ì–¸ì–´ íƒœê·¸ ì œê±°
                    english_query_raw = english_query_raw.replace("json", "").replace("text", "").strip()

                english_query = ""
                for line in english_query_raw.splitlines():
                    line = line.strip()
                    if line:
                        english_query = line
                        break

                # ì•ˆì „ í´ë°±
                if not english_query:
                    english_query = query
                    print("  - ë²ˆì—­ ê²°ê³¼ ë¹„ì–´ìˆìŒ: ì›ë¬¸ ì¿¼ë¦¬ë¡œ í´ë°±")
                else:
                    print(f"  - ë²ˆì—­ëœ ì˜ì–´ ê²€ìƒ‰ì–´: {english_query}")

            except Exception as te:
                # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ì‚¬ìš©
                english_query = query
                print(f"  - ë²ˆì—­ ë‹¨ê³„ ì˜¤ë¥˜, ì›ë¬¸ìœ¼ë¡œ ì§„í–‰: {te}")
            # ============================================================

            def _direct_pubmed_search(query_text, max_results=5, original_query=None):
                try:
                    api_key = os.getenv("PUBMED_API_KEY", "").strip()

                    # 1) ESearchë¡œ PMID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                    esearch_base = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
                    esearch_params = {
                        "db": "pubmed",
                        "term": query_text,        # ì˜ì–´ë¡œ ë²ˆì—­ëœ ê²€ìƒ‰ì–´ ì‚¬ìš©
                        "retstart": 0,
                        "retmax": max_results,
                        "retmode": "json",
                        "sort": "pub+date"        # ìµœì‹  ë°œí–‰ì¼ ìš°ì„ 
                    }
                    if api_key:
                        esearch_params["api_key"] = api_key

                    esearch_url = esearch_base + "?" + urllib.parse.urlencode(esearch_params)
                    req = urllib.request.Request(
                        esearch_url,
                        headers={"User-Agent": "MyPubMedClient/1.0 (contact: youremail@example.com)"}
                    )
                    with urllib.request.urlopen(req, timeout=20) as resp:
                        search_json = json.loads(resp.read().decode("utf-8", errors="replace"))

                    idlist = search_json.get("esearchresult", {}).get("idlist", [])
                    if not idlist:
                        print("  - PubMed: ê²€ìƒ‰ ê²°ê³¼ 0ê±´")
                        return []

                    # 2) EFetchë¡œ ìƒì„¸(ì œëª©/ì´ˆë¡/ì €ì/DOI ë“±) ë°›ê¸°
                    efetch_base = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
                    efetch_params = {
                        "db": "pubmed",
                        "id": ",".join(idlist),
                        "retmode": "xml"
                    }
                    if api_key:
                        efetch_params["api_key"] = api_key

                    efetch_url = efetch_base + "?" + urllib.parse.urlencode(efetch_params)
                    req2 = urllib.request.Request(
                        efetch_url,
                        headers={"User-Agent": "MyPubMedClient/1.0 (contact: youremail@example.com)"}
                    )
                    with urllib.request.urlopen(req2, timeout=30) as resp2:
                        xml_text = resp2.read().decode("utf-8", errors="replace")

                    if not xml_text.lstrip().startswith("<?xml"):
                        raise RuntimeError("Non-XML response from PubMed efetch")

                    root = ET.fromstring(xml_text)

                    def _parse_date(article):
                        y = article.findtext(".//ArticleDate/Year") or article.findtext(".//JournalIssue/PubDate/Year")
                        m = article.findtext(".//ArticleDate/Month") or article.findtext(".//JournalIssue/PubDate/Month")
                        d = article.findtext(".//ArticleDate/Day") or article.findtext(".//JournalIssue/PubDate/Day")
                        month_map = {"Jan":"01","Feb":"02","Mar":"03","Apr":"04","May":"05","Jun":"06",
                                    "Jul":"07","Aug":"08","Sep":"09","Oct":"10","Nov":"11","Dec":"12"}
                        if m and len(m) == 3 and m in month_map:
                            m = month_map[m]
                        if y:
                            m = m or "01"
                            d = d or "01"
                            try:
                                return datetime.strptime(f"{y}-{m}-{d}", "%Y-%m-%d").strftime("%Yë…„ %mì›” %dì¼")
                            except Exception:
                                return f"{y}ë…„"
                        return ""

                    results: List[SearchResult] = []
                    for art in root.findall(".//PubmedArticle"):
                        pmid = art.findtext(".//PMID") or ""
                        title = (art.findtext(".//ArticleTitle") or "").strip()

                        # ì´ˆë¡
                        abs_elems = art.findall(".//Abstract/AbstractText")
                        if abs_elems:
                            parts = []
                            for t in abs_elems:
                                label = t.attrib.get("Label")
                                txt = (t.text or "").strip()
                                parts.append(f"{label}: {txt}" if label else txt)
                            summary = " ".join([p for p in parts if p])
                        else:
                            summary = ""

                        # ì €ì
                        author_elems = art.findall(".//AuthorList/Author")
                        authors = []
                        for a in author_elems:
                            last = a.findtext("LastName") or ""
                            fore = a.findtext("ForeName") or a.findtext("Initials") or ""
                            if last or fore:
                                authors.append((fore + " " + last).strip())
                        author_str = ", ".join(authors[:3])
                        if len(authors) > 3:
                            author_str += f" ì™¸ {len(authors)-3}ëª…"

                        # ì €ë„/ë‚ ì§œ/MeSH
                        journal = (art.findtext(".//Journal/Title") or "").strip()
                        pub_date = _parse_date(art)
                        mesh_terms = [(mh.text or "").strip() for mh in art.findall(".//MeshHeading/DescriptorName")]
                        mesh_str = ", ".join([t for t in mesh_terms if t][:3])

                        # DOI ë˜ëŠ” PubMed ë§í¬
                        doi = None
                        for aid in art.findall(".//ArticleIdList/ArticleId"):
                            if (aid.attrib or {}).get("IdType", "").lower() == "doi":
                                doi = (aid.text or "").strip()
                                break
                        link = f"https://doi.org/{doi}" if doi else f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"

                        # SearchResult êµ¬ì„±
                        item = SearchResult(
                            source="pubmed_search",
                            content=(summary[:1000] + ("..." if len(summary) > 1000 else "")),
                            search_query=query_text,  # ì˜ì–´ ê²€ìƒ‰ì‹
                            title=title or f"PMID {pmid}",
                            url=link,
                            score=0.95,
                            timestamp=datetime.now().isoformat(),
                            document_type="research_paper",
                            metadata={
                                "authors": author_str,
                                "date": pub_date,
                                "journal": journal,
                                "pmid": pmid,
                                "mesh_terms": mesh_terms[:10],
                                "categories": mesh_str,
                                "optimized_query": query_text,      # ë²ˆì—­/ìµœì í™”ëœ ì¿¼ë¦¬
                                "original_query": original_query or query_text  # [NEW] ì›ë¬¸ ì¿¼ë¦¬ ë³´ì¡´
                            },
                            source_url=link
                        )
                        results.append(item)

                    return results[:max_results]

                except Exception as e:
                    print(f"  - PubMed ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                    return []

            # === [CHANGED] 2) ì˜ì–´ ì¿¼ë¦¬ë¡œ PubMed í˜¸ì¶œ ===================
            results = await loop.run_in_executor(
                _global_executor, _direct_pubmed_search, english_query, 5, query  # original_query ì „ë‹¬
            )
            # ============================================================

            print(f"  - PubMed ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ë…¼ë¬¸ (ê²€ìƒ‰ì‹: {english_query})")
            return results

        except Exception as e:
            print(f"  - PubMed ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []


    async def _rdb_search(self, query: str) -> List[SearchResult]:
        """RDB ê²€ìƒ‰ ì‹¤í–‰ - ë°˜í™˜ í‘œì¤€í™”"""
        try:
            loop = asyncio.get_running_loop()
            result_text = await loop.run_in_executor(None, rdb_search, query)

            # rdb_searchëŠ” ë¬¸ìì—´ì„ ë°˜í™˜í•˜ë¯€ë¡œ, ì´ë¥¼ ë‹¨ì¼ SearchResultë¡œ ë³€í™˜
            if isinstance(result_text, str) and result_text.strip():
                # "PostgreSQL ê²€ìƒ‰ ê²°ê³¼: "ë¡œ ì‹œì‘í•˜ëŠ”ì§€ ì²´í¬
                if "PostgreSQL ê²€ìƒ‰ ê²°ê³¼:" in result_text and "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in result_text:
                    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
                    print(f"  - RDBì—ì„œ '{query}' ê´€ë ¨ ë°ì´í„° ì—†ìŒ")
                    return []

                search_result = SearchResult(
                    source="rdb_search",
                    content=result_text,
                    search_query=query,
                    title="PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼",
                    url="",
                    score=0.9,
                    document_type="database",
                    metadata={"raw_result": result_text}
                )
                print(f"  - rdb_search ë˜í¼ ë°˜í™˜: 1ê°œ (í…ìŠ¤íŠ¸ ê²°ê³¼)")
                return [search_result]
            else:
                print(f"  - RDB ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                return []


        except Exception as e:
            print(f"RDB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    async def _scrape_content(self, url: str, query: str = "") -> List[SearchResult]:
        """ì›¹í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
        try:
            print(f">> ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")

            # scrape_and_extract_contentëŠ” JSON ë¬¸ìì—´ì„ ë°›ì•„ì•¼ í•¨
            action_input = json.dumps({"url": url, "query": query})

            content = await asyncio.get_event_loop().run_in_executor(
                _global_executor, scrape_and_extract_content, action_input
            )

            if content:
                # URLì—ì„œ ì œëª© ì¶”ì¶œ
                title = url.split("/")[-1] if "/" in url else url
                if title.endswith('.pdf'):
                    title = f"PDF: {title}"
                else:
                    title = f"ì›¹í˜ì´ì§€: {title}"

                search_result = SearchResult(
                    source="scraper",
                    content=content,
                    search_query=query,
                    title=title,
                    url=url,
                    score=0.9,  # ëª…ì‹œì  ìŠ¤í¬ë˜í•‘ì´ë¯€ë¡œ ë†’ì€ ì ìˆ˜
                    timestamp=datetime.now().isoformat(),
                    document_type="web_scraping",
                    metadata={
                        "scraped_url": url,
                        "content_length": len(content),
                        "scraping_query": query
                    },
                    chunk_id=f"scrape_{hash(url)}"
                )
                print(f"  - ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(content)}ì")
                return [search_result]

            return []
        except Exception as e:
            print(f"ì›¹ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
            return []



class ProcessorAgent:
    """ë°ì´í„° ê°€ê³µ ë° ìƒì„± ì „ë‹´ Agent (ReAct ì œê±°, ìˆœì°¨ ìƒì„± ì§€ì›)"""

    def __init__(self, model_pro: str = "gemini-2.5-pro", model_flash: str = "gemini-2.5-flash-lite", temperature: float = 0.3):
        # ë³´ê³ ì„œ ìµœì¢… ìƒì„±ì„ ìœ„í•œ ê³ í’ˆì§ˆ ëª¨ë¸ (Gemini Pro) - í’ˆì§ˆ í–¥ìƒì„ ìœ„í•´ Pro ì‚¬ìš©
        self.llm_pro = ChatGoogleGenerativeAI(model=model_pro, temperature=temperature)
        # êµ¬ì¡° ì„¤ê³„, ìš”ì•½ ë“± ë¹ ë¥¸ ì‘ì—…ì— ì‚¬ìš©í•  ê²½ëŸ‰ ëª¨ë¸ (Gemini)
        self.llm_flash = ChatGoogleGenerativeAI(model=model_flash, temperature=0.1)

        # Fallback ëª¨ë¸ë“¤ (3ê°œ í‚¤ + OpenAI)
        import sys
        import os
        sys.path.append('/app/utils')
        try:
            from api_fallback import api_manager
        except ImportError:
            print("âš ï¸ api_fallback ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©")
            api_manager = None

        if api_manager:
            try:
                self.llm_pro_backup = api_manager.create_langchain_model(model_pro, temperature=temperature)
                self.llm_flash_backup = api_manager.create_langchain_model(model_flash, temperature=0.1)
                print(f"ProcessorAgent: Fallback ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (ì‚¬ìš© API: {api_manager.last_successful_api})")
            except Exception as e:
                print(f"ProcessorAgent: Fallback ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.llm_pro_backup = None
                self.llm_flash_backup = None
        else:
            self.llm_pro_backup = None
            self.llm_flash_backup = None

        # OpenAI fallback ëª¨ë¸ë“¤
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        if self.openai_api_key:
            self.llm_openai_mini = ChatOpenAI(
                model="gpt-4o-mini",
                temperature=0.1,
                api_key=self.openai_api_key
            )
            self.llm_openai_4o = ChatOpenAI(
                model="gpt-4o",
                temperature=temperature,
                api_key=self.openai_api_key
            )
            print("ProcessorAgent: OpenAI fallback ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            self.llm_openai_mini = None
            self.llm_openai_4o = None
            print("ProcessorAgent: ê²½ê³ : OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")

        self.personas = PERSONA_PROMPTS

        # Orchestratorê°€ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” ì‘ì—… ëª©ë¡ ì •ì˜
        self.processor_mapping = {
            "design_report_structure": self._design_report_structure,
            "create_chart_data": self._create_charts,
        }

    async def _invoke_with_fallback(self, prompt, primary_model, backup_model, fallback_model):
        """
        Gemini key 1 â†’ Gemini key 2 â†’ OpenAI ìˆœìœ¼ë¡œ fallback ì²˜ë¦¬
        """
        # Gemini key 1 ì‹œë„
        try:
            result = await primary_model.ainvoke(prompt)
            return result
        except Exception as e:
            print(f"ProcessorAgent: Gemini key 1 ì‹¤íŒ¨: {e}")

        # Gemini key 2 ì‹œë„
        if backup_model:
            try:
                result = await backup_model.ainvoke(prompt)
                print("ProcessorAgent: Gemini key 2 ì„±ê³µ")
                return result
            except Exception as e:
                print(f"ProcessorAgent: Gemini key 2 ì‹¤íŒ¨: {e}")

        # OpenAI ì‹œë„
        if fallback_model:
            try:
                result = await fallback_model.ainvoke(prompt)
                print("ProcessorAgent: OpenAI fallback ì„±ê³µ")
                return result
            except Exception as fallback_error:
                print(f"ProcessorAgent: OpenAI fallbackë„ ì‹¤íŒ¨: {fallback_error}")
                raise fallback_error
        else:
            print("ProcessorAgent: ëª¨ë“  fallback ëª¨ë¸ì´ ì—†ìŒ")
            raise Exception("ëª¨ë“  API í‚¤ ì‹œë„ ì‹¤íŒ¨")

    async def _astream_with_fallback(self, prompt, primary_model, backup_model, fallback_model):
        """
        ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ Gemini key 1 â†’ Gemini key 2 â†’ OpenAI ìˆœ fallback ì²˜ë¦¬
        """
        # Gemini key 1 ì‹œë„
        primary_chunks_received = 0
        primary_content_length = 0

        try:
            print(f"- Primary ëª¨ë¸ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‹œë„ ({type(primary_model).__name__})")
            async for chunk in primary_model.astream(prompt):
                primary_chunks_received += 1
                if hasattr(chunk, 'content') and chunk.content:
                    primary_content_length += len(chunk.content)
                yield chunk

            print(f"- Primary ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {primary_chunks_received}ê°œ ì²­í¬, {primary_content_length} ë¬¸ì")

            # ì²­í¬ë¥¼ ë°›ì•˜ì§€ë§Œ ë‚´ìš©ì´ ë¹„ì–´ìˆëŠ” ê²½ìš°ë„ ì‹¤íŒ¨ë¡œ ê°„ì£¼
            if primary_chunks_received == 0 or primary_content_length == 0:
                print(f"- Primary ëª¨ë¸ì—ì„œ ìœ íš¨í•œ ë‚´ìš©ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ, backup ì‹œë„")
                raise Exception("No valid content generated")
            return

        except Exception as e:
            print(f"ProcessorAgent: Gemini key 1 ì‹¤íŒ¨: {e}")

        # Gemini key 2 ì‹œë„
        if backup_model:
            try:
                print("ProcessorAgent: Gemini key 2ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")
                backup_chunks_received = 0
                async for chunk in backup_model.astream(prompt):
                    backup_chunks_received += 1
                    yield chunk
                print(f"ProcessorAgent: Gemini key 2 ì™„ë£Œ: {backup_chunks_received}ê°œ ì²­í¬")
                return
            except Exception as e:
                print(f"ProcessorAgent: Gemini key 2ë„ ì‹¤íŒ¨: {e}")

        # OpenAI ì‹œë„
        if fallback_model:
            try:
                print("ProcessorAgent: OpenAI fallbackìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")
                fallback_chunks_received = 0
                async for chunk in fallback_model.astream(prompt):
                    fallback_chunks_received += 1
                    yield chunk
                print(f"ProcessorAgent: OpenAI fallback ì™„ë£Œ: {fallback_chunks_received}ê°œ ì²­í¬")
            except Exception as fallback_error:
                print(f"ProcessorAgent: OpenAI fallbackë„ ì‹¤íŒ¨: {fallback_error}")
                raise fallback_error
        else:
            print("ProcessorAgent: ëª¨ë“  fallback ëª¨ë¸ì´ ì—†ìŒ")
            raise Exception("ëª¨ë“  API í‚¤ ì‹œë„ ì‹¤íŒ¨")

    async def process(self, processor_type: str, data: Any, param2: Any, param3: str, param4: str = "", yield_callback=None, state: Optional[Dict[str, Any]] = None):
        """Orchestratorë¡œë¶€í„° ë™ê¸°ì‹ ì‘ì—…ì„ ë°›ì•„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        session_id = state.get("session_id", "unknown") if state else "unknown"
        logger = get_session_logger(session_id, "ProcessorAgent")
        logger.info(f"Processor ì‹¤í–‰: {processor_type}")
        # stateëŠ” ì´ë¯¸ ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬ë°›ìŒ

        if processor_type == "design_report_structure":
            selected_indexes = param2
            original_query = param3
            async for result in self._design_report_structure(data, selected_indexes, original_query, state):
                yield result
        elif processor_type == "create_chart_data":
            section_title = param2
            generated_content = param3
            # yield_callbackì€ ì´ë¯¸ ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬ë°›ìŒ
            async for result in self._create_charts(data, section_title, generated_content, yield_callback, state):
                yield result

        else:
            yield {"type": "error", "data": {"error": f"ì•Œ ìˆ˜ ì—†ëŠ” ì²˜ë¦¬ íƒ€ì…: {processor_type}"}}

    async def _design_report_structure(self, data: List[SearchResult], selected_indexes: List[int], query: str, state: Optional[Dict[str, Any]] = None):
        """ë³´ê³ ì„œ êµ¬ì¡° ì„¤ê³„ + ì„¹ì…˜ë³„ ì‚¬ìš©í•  ë°ì´í„° ì¸ë±ìŠ¤ ì„ íƒ"""

        print(f"\n>> ë³´ê³ ì„œ êµ¬ì¡° ì„¤ê³„ ì‹œì‘:")
        print(f"   ì „ì²´ ë°ì´í„°: {len(data)}ê°œ")
        print(f"   ì„ íƒëœ ì¸ë±ìŠ¤: {selected_indexes} ({len(selected_indexes)}ê°œ)")

        # í˜„ì¬ ë‚ ì§œ ì •ë³´ ì¶”ê°€
        import pytz
        kst = pytz.timezone('Asia/Seoul')
        current_date_str = datetime.now(kst).strftime("%Yë…„ %mì›” %dì¼")

        # í˜ë¥´ì†Œë‚˜ ì •ë³´ ì¶”ì¶œ
        persona_name = state.get("persona", "ê¸°ë³¸") if state else "ê¸°ë³¸"
        persona_description = self.personas.get(persona_name, {}).get("description", "ì¼ë°˜ì ì¸ ë¶„ì„ê°€")
        print(f"  - ë³´ê³ ì„œ êµ¬ì¡° ì„¤ê³„ì— '{persona_name}' í˜ë¥´ì†Œë‚˜ ê´€ì  ì ìš©")

        # ì„ íƒëœ ì¸ë±ìŠ¤ì˜ ë°ì´í„°ë¥¼ ì¸ë±ìŠ¤ì™€ í•¨ê»˜ ë§¤í•‘í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        indexed_context = ""
        for idx in selected_indexes:
            if 0 <= idx < len(data):
                res = data[idx]
                source = getattr(res, 'source', 'Unknown')
                title = getattr(res, 'title', 'No Title')
                content = getattr(res, 'content', '')  # ì „ì²´ ë‚´ìš© (ìš”ì•½ ì—†ì´)

                indexed_context += f"""
    --- ë°ì´í„° ì¸ë±ìŠ¤ [{idx}] ---
    ì¶œì²˜: {source}
    ì œëª©: {title}
    ë‚´ìš©: {content}

    """

        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ë‚´ê¸°)
        limited_indexed_context = indexed_context[:20000]  # ë” ë§ì€ ì •ë³´ í¬í•¨

        print(f"   ìƒì„±ëœ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(indexed_context)} ë¬¸ì")
        print(f"   ì œí•œëœ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(limited_indexed_context)} ë¬¸ì")

        # - ì‚¬ìš©ìì˜ 'ì›ë³¸ ì§ˆë¬¸(query)'ì„ ëª…í™•íˆ ì œì‹œí•˜ê³ , êµ¬ì¡° ì„¤ê³„ì˜ ëª¨ë“  ê³¼ì •ì´ ì´ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° ì§‘ì¤‘ë˜ë„ë¡ ì§€ì‹œ
        # - ê° ë°ì´í„°ê°€ ì›ë³¸ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ì§€ í‰ê°€í•˜ì—¬ ë¶€ì í•©í•œ ë°ì´í„°ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ëª…ì‹œ
        # - ì„¹ì…˜ ì œëª©ì´ ì›ë³¸ ì§ˆë¬¸ì˜ í•µì‹¬ ìš”ì†Œì™€ ì§ì ‘ì ìœ¼ë¡œ ì—°ê²°ë˜ë„ë¡ ê·œì¹™ ê°•í™”
        prompt = f"""
    ë‹¹ì‹ ì€ '{persona_name}'({persona_description})ì˜ ê´€ì ì„ ê°€ì§„ ë°ì´í„° ë¶„ì„ê°€ì´ì AI ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì„¤ê³„ìì…ë‹ˆë‹¤.
    ì£¼ì–´ì§„ **ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸**ê³¼ **ì„ ë³„ëœ ë°ì´í„°**ë¥¼ ë¶„ì„í•˜ì—¬, **ì˜¤ì§ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€**ìœ¼ë¡œë§Œ êµ¬ì„±ëœ **ë‚´ìš©ì´ ì ˆëŒ€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ”** ë…¼ë¦¬ì ì¸ ë³´ê³ ì„œ ëª©ì°¨ë¥¼ ì„¤ê³„í•˜ê³ , **ê° ì„¹ì…˜ì˜ êµ¬ì²´ì ì¸ ëª©í‘œ**ì™€ **ê° ì„¹ì…˜ë³„ë¡œ ì‚¬ìš©í•  ë°ì´í„° ì¸ë±ìŠ¤**ë¥¼ ëª…í™•íˆ ë¶„ë°°í•´ì£¼ì„¸ìš”.

    **### ê°€ì¥ ì¤‘ìš”í•œ ì›ì¹™ ###**
    **1. ì§ˆë¬¸ì—ë§Œ ì§‘ì¤‘:** ë‹¹ì‹ ì˜ ìœ ì¼í•œ ëª©í‘œëŠ” ì•„ë˜ **ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸**ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì§ˆë¬¸ì—ì„œ ë²—ì–´ë‚˜ëŠ” ì£¼ì œê°€ ë°ì´í„°ì— í¬í•¨ë˜ì–´ ìˆë”ë¼ë„, ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ìœ¼ë©´ **ê³¼ê°íˆ ë¬´ì‹œí•˜ê³  ë³´ê³ ì„œ êµ¬ì¡°ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**
    **2. ë°ì´í„° ì í•©ì„± ê²€ì¦:** ê° ë°ì´í„° ì¡°ê°ì´ **ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸**ì˜ ì–´ë–¤ ë¶€ë¶„ì— ë‹µí•  ìˆ˜ ìˆëŠ”ì§€ ë¨¼ì € í‰ê°€í•˜ì„¸ìš”. ê´€ë ¨ ì—†ëŠ” ë°ì´í„°ëŠ” ë³´ê³ ì„œ ìƒì„±ì— ì‚¬ìš©í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.
    **3. ë‚´ìš© ë°˜ë³µ ê¸ˆì§€:** ê° ì„¹ì…˜ì˜ ì£¼ì œëŠ” ëª…í™•íˆ ë¶„ë¦¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

    **ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸**: "{query}"
    **í˜„ì¬ ë‚ ì§œ**: {current_date_str}

    **ì„ ë³„ëœ ë°ì´í„° (ì¸ë±ìŠ¤ì™€ ì „ì²´ ë‚´ìš© í¬í•¨)**:
    {limited_indexed_context}

    **ì‘ì—… ì§€ì¹¨**:
    1. **ë³´ê³ ì„œ ëª©ì°¨ ë° ì„¹ì…˜ë³„ ëª©í‘œ ì„¤ê³„**
    - ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³ , **ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸ì„ ë¶„í•´**í•˜ì—¬, ê·¸ êµ¬ì„± ìš”ì†Œì— ì§ì ‘ ë‹µí•˜ëŠ” 3~5ê°œì˜ **ê³ ìœ í•˜ê³  êµ¬ì²´ì ì¸ ì„¹ì…˜**ìœ¼ë¡œ ëª©ì°¨ë¥¼ êµ¬ì„±í•˜ì„¸ìš”.
    - **í•µì‹¬ ê·œì¹™**: ê° ì„¹ì…˜ì˜ ì œëª©ì€ **ì„œë¡œ ë‹¤ë¥¸ ë¶„ì„ ê´€ì ì´ë‚˜ ì£¼ì œ**ë¥¼ ë‹¤ë£¨ì–´ì•¼ í•©ë‹ˆë‹¤. ëª¨í˜¸í•˜ê±°ë‚˜ ìœ ì‚¬í•œ ì œëª©ì€ ì ˆëŒ€ ê¸ˆì§€ë©ë‹ˆë‹¤.

        - **ì ˆëŒ€ í”¼í•´ì•¼ í•  ë‚˜ìœ ì˜ˆì‹œ (ì£¼ì œê°€ ìœ ì‚¬í•˜ì—¬ ë‚´ìš©ì´ ì¤‘ë³µë¨):**
            - "1. ë§Œë‘ ìˆ˜ì¶œ í˜„í™© ë¶„ì„", "2. ì£¼ìš” ìˆ˜ì¶œêµ­ ë° ì‹œì¥ ë™í–¥"  (X)
            - "1. ì‹œì¥ íŠ¸ë Œë“œ", "2. ìµœì‹  ë™í–¥ ë¶„ì„" (X)

        - **ë°˜ë“œì‹œ ë”°ë¼ì•¼ í•  ì¢‹ì€ ì˜ˆì‹œ (ì£¼ì œê°€ ëª…í™•íˆ ë¶„ë¦¬ë¨):**
            - "1. **êµ­ê°€ë³„ ìˆ˜ì¶œì•¡ ë°ì´í„°**ë¥¼ í†µí•œ í•µì‹¬ ì‹œì¥ ë¶„ì„" (ì •ëŸ‰ì , ìˆœìœ„)
            - "2. **ì†Œë¹„ì ì„ í˜¸ë„ ë° ìµœì‹  ì‹í’ˆ íŠ¸ë Œë“œ** ë¶„ì„" (ì •ì„±ì , íŠ¸ë Œë“œ)
            - "3. **ì£¼ìš” ê²½ìŸì‚¬ ì „ëµ ë° ì„±ê³µ/ì‹¤íŒ¨ ì‚¬ë¡€** ì—°êµ¬" (ê²½ìŸ, ì‚¬ë¡€ ë¶„ì„)

    - **ì„¹ì…˜ ì œëª© ê·œì¹™**: ì„¹ì…˜ ì œëª©ì€ **'ì‚¬ìš©ì ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œ'**ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

        - **ì¢‹ì€ ì˜ˆì‹œ (ì‚¬ìš©ì ì§ˆë¬¸: "ì›ë£Œë³„ ëŒ€ì²´ì‹í’ˆ ìœ í˜•ê³¼ ë¯¸ìƒë¬¼ ë°œíš¨ì‹í’ˆ R&D í˜„í™©"):**
          - "1. ì›ë£Œ ê¸°ë°˜ ëŒ€ì²´ì‹í’ˆì˜ ìœ í˜•ë³„ ë¶„ë¥˜"
          - "2. ë¯¸ìƒë¬¼ ë°œíš¨ ì‹í’ˆì˜ í•µì‹¬ ê¸°ìˆ  ë° ì›ë¦¬"
          - "3. ë¯¸ìƒë¬¼ ë°œíš¨ ì‹í’ˆì˜ ìµœì‹  ì—°êµ¬ê°œë°œ(R&D) ë™í–¥"

        - **ë‚˜ìœ ì˜ˆì‹œ (ì‚¬ìš©ìê°€ ë¬»ì§€ ì•Šì€ ë‚´ìš© í¬í•¨):**
          - "1. ê¸€ë¡œë²Œ ëŒ€ì²´ì‹í’ˆ ì‹œì¥ ê·œëª¨ ë¶„ì„" (X)
          - "2. ëŒ€ì²´ì‹í’ˆ ì†Œë¹„ì ì¸ì‹ ì¡°ì‚¬" (X)

    - **[ë§¤ìš° ì¤‘ìš”]** ê° ì„¹ì…˜ë§ˆë‹¤ `description` í•„ë“œë¥¼ ì‘ì„±í•  ë•Œ ë°˜ë“œì‹œ ì•„ë˜ íŠ¹ë³„ ê·œì¹™ì„ ì¤€ìˆ˜í•˜ì„¸ìš”. 'description' í•„ë“œëŠ” ë‚˜ì¤‘ì— ë‹¤ë¥¸ AIê°€ í•´ë‹¹ ì„¹ì…˜ì„ ì‘ì„±í•  ë•Œ ì§ì ‘ì ì¸ ê°€ì´ë“œë¼ì¸ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
        -**`description` í•„ë“œ ì‘ì„± íŠ¹ë³„ ê·œì¹™ (ë§¤ìš° ì¤‘ìš”!)**:
            - **`content_type: 'synthesis'`ì¸ ê²½ìš°**:
                - í•´ë‹¹ ì„¹ì…˜ì´ ë¶„ì„í•´ì•¼ í•  í•µì‹¬ ì§ˆë¬¸ì´ë‚˜ ë‹¬ì„± ëª©í‘œë¥¼ **í•œ ë¬¸ì¥ìœ¼ë¡œ ëª…í™•í•˜ê²Œ** ê¸°ìˆ í•©ë‹ˆë‹¤.
            - **`content_type: 'full_data_for_chart'`ì¸ ê²½ìš°**:
                - ì•„ë˜ 4ê°€ì§€ í•­ëª©ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì—¬ **êµ¬ì²´ì ì´ê³  ë…¼ë¦¬ì ìœ¼ë¡œ** ê¸°ìˆ í•©ë‹ˆë‹¤. ì´ ë‚´ìš©ì€ ì°¨íŠ¸ ìƒì„± AIì—ê²Œ ì „ë‹¬ë˜ëŠ” ëª…ì„¸ì„œ ì—­í• ì„ í•©ë‹ˆë‹¤.
                    - **(1) ë¶„ì„ ëª©í‘œ:** ì´ ì„¹ì…˜ê³¼ ì°¨íŠ¸ë¥¼ í†µí•´ ê¶ê·¹ì ìœ¼ë¡œ ë³´ì—¬ì£¼ê³ ì í•˜ëŠ” í•µì‹¬ ì¸ì‚¬ì´íŠ¸ê°€ ë¬´ì—‡ì¸ì§€ ì„œìˆ í•©ë‹ˆë‹¤.
                    - **(2) ì°¨íŠ¸ í•„ìš”ì„±:** ì™œ í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ì°¨íŠ¸ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ ë” íš¨ê³¼ì ì¸ì§€ ì´ìœ ë¥¼ ì„œìˆ í•©ë‹ˆë‹¤. (ì˜ˆ: "ì—°ë„ë³„ ì‹œì¥ ê·œëª¨ ë³€í™” ì¶”ì´ë¥¼ ì§ê´€ì ìœ¼ë¡œ íŒŒì•…í•˜ê¸° ìœ„í•´", "ì£¼ìš” êµ­ê°€ë³„ ìˆ˜ì¶œì•¡ì„ ëª…í™•í•˜ê²Œ ë¹„êµí•˜ê¸° ìœ„í•´")
                    - **(3) ì‹œê°í™”í•  ë‚´ìš©:** ì°¨íŠ¸ë¥¼ êµ¬ì„±í•˜ëŠ” í•µì‹¬ ë°ì´í„° ìš”ì†Œ(ë¼ë²¨, ê°’, í•­ëª© ë“±)ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œí•©ë‹ˆë‹¤.
Â  Â  Â  Â  Â  Â  	        - **(Bar/Line Chart ì˜ˆì‹œ:** "Xì¶•ì€ ì—°ë„, Yì¶•ì€ ì‹œì¥ ê·œëª¨(ì–µì›)")
Â  Â  Â  Â  Â  Â  	        - **(Pie/Doughnut Chart ì˜ˆì‹œ:** "ê° ì¡°ê°(slice)ì€ í’ˆëª©ëª…ì„, ê° ì¡°ê°ì˜ ê°’ì€ í•´ë‹¹ í’ˆëª©ì˜ ì ìœ ìœ¨(%)ì„ ì˜ë¯¸")
Â  Â  Â  Â  Â  Â  	        - **(Radar Chart ì˜ˆì‹œ:** "ê° ê¼­ì§“ì (axis)ì€ 'ì„±ì¥ì„±', 'ì•ˆì •ì„±' ë“± í‰ê°€ í•­ëª©ì„, ê° ì„ (line)ì€ ê²½ìŸì‚¬ë¥¼ ì˜ë¯¸")
Â  Â  Â  Â  Â  Â  	    - **(4) ì¶”ì²œ ì°¨íŠ¸ ìœ í˜•:** ë°ì´í„°ì˜ íŠ¹ì„±ì— ê°€ì¥ ì í•©í•œ ì°¨íŠ¸ ì¢…ë¥˜ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤. (ì˜ˆ: "ì‹œê³„ì—´ ë°ì´í„°ì´ë¯€ë¡œ Line chartê°€ ì í•©í•¨", "êµ¬ì„± ë¹„ìœ¨ì„ ë³´ì—¬ì£¼ë¯€ë¡œ Pie chartê°€ ì í•©í•¨")

    2. **ê° ì„¹ì…˜ë³„ ì‚¬ìš© ë°ì´í„° ì„ íƒ**
    - **1ë‹¨ê³„ì—ì„œ ì„¤ê³„í•œ ê³ ìœ í•œ ì„¹ì…˜ ì œëª©**ì„ ê¸°ì¤€ìœ¼ë¡œ, ê° ì„¹ì…˜ë§ˆë‹¤ `use_contents` í•„ë“œì— **ì¡°ê¸ˆ ì „ ë‹¹ì‹ ì´ ì§ì ‘ ì •ì˜í•œ `description`ì˜ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ëŠ” ë°** ê°€ì¥ ì í•©í•œ ë°ì´í„°ì˜ ì¸ë±ìŠ¤ ë²ˆí˜¸ë“¤ì„ ë°°ì—´ë¡œ í• ë‹¹í•˜ì„¸ìš”.
    - **ë°ì´í„° ì¤‘ë³µ í• ë‹¹ ì—„ê²© ê¸ˆì§€**: **ì„œë¡œ ë‹¤ë¥¸ ì„¹ì…˜ì€ ë°˜ë“œì‹œ ì„œë¡œ ë‹¤ë¥¸ `use_contents` ëª©ë¡ì„ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.** ë™ì¼í•œ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ì„¹ì…˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ìµœì†Œí™”í•´ì•¼ í•©ë‹ˆë‹¤.
    - **ìµœì  í• ë‹¹ ì›ì¹™**: ê° ë°ì´í„°ëŠ” ê·¸ê²ƒì´ ê°€ì¥ í•µì‹¬ì ìœ¼ë¡œ ë’·ë°›ì¹¨í•  ìˆ˜ ìˆëŠ” **ë‹¨ í•˜ë‚˜ì˜ ì„¹ì…˜ì—ë§Œ í• ë‹¹**í•˜ëŠ” ê²ƒì„ ì›ì¹™ìœ¼ë¡œ í•©ë‹ˆë‹¤.
    - **ì¸ë±ìŠ¤ ë²”ìœ„ ë° ê°œìˆ˜ ì œí•œ**:
        - ìœ„ì— ì œì‹œëœ ë°ì´í„° ì¸ë±ìŠ¤ ì¤‘ì—ì„œë§Œ ì„ íƒí•˜ì„¸ìš”: {selected_indexes}
        - í•œ ì„¹ì…˜ì— ë„ˆë¬´ ë§ì€ ë°ì´í„°(5ê°œ ì´ˆê³¼)ë¥¼ í• ë‹¹í•˜ì§€ ë§ˆì„¸ìš”.

    3. **'ê²°ë¡ ' ì„¹ì…˜ ì¶”ê°€ (í•„ìˆ˜)**: ë³´ê³ ì„œì˜ ê°€ì¥ ë§ˆì§€ë§‰ì—ëŠ” í•­ìƒ 'ê²°ë¡ ' ì„¹ì…˜ì„ í¬í•¨í•˜ì„¸ìš”.
    - `content_type`ì€ 'synthesis'ë¡œ ì„¤ì •
    - `use_contents`ì—ëŠ” ì£¼ìš” ì„¹ì…˜ë“¤ì˜ í•µì‹¬ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ í¬í•¨í•˜ì„¸ìš”. **(ì˜ˆì™¸: 'ê²°ë¡ ' ì„¹ì…˜ì€ ë‹¤ë¥¸ ì„¹ì…˜ì—ì„œ ì‚¬ìš©ëœ í•µì‹¬ ë°ì´í„°ë¥¼ ì¤‘ë³µ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)**

    4. **ì„¹ì…˜ë³„ ìµœì  ì½˜í…ì¸  ìœ í˜• ê²°ì • (ë§¤ìš° ì¤‘ìš”)**:
    - ê° ì„¹ì…˜ì˜ `content_type`ì„ ê²°ì •í•  ë•Œ, ì°¨íŠ¸ê°€ **ë°˜ë“œì‹œ í•„ìš”í•œì§€** ì‹ ì¤‘í•˜ê²Œ íŒë‹¨í•˜ì„¸ìš”. í…ìŠ¤íŠ¸ë‚˜ í‘œë¡œë„ ì¶©ë¶„íˆ ì „ë‹¬ ê°€ëŠ¥í•œ ì •ë³´ëŠ” `synthesis`ë¡œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.

    - **`full_data_for_chart`ëŠ” ë‹¤ìŒ ì¡°ê±´ ì¤‘ í•˜ë‚˜ ì´ìƒì„ ëª…í™•íˆ ë§Œì¡±í•  ë•Œë§Œ ì‚¬ìš©í•˜ì„¸ìš”:**
        - **(A) ëª…í™•í•œ ë¹„êµ:** ì—¬ëŸ¬ í•­ëª©(3ê°œ ì´ìƒ)ì˜ ìˆ˜ì¹˜ë¥¼ **ë¹„êµ**í•˜ì—¬ ìˆœìœ„ë‚˜ ì°¨ì´ë¥¼ ë³´ì—¬ì¤„ ë•Œ (ì˜ˆ: êµ­ê°€ë³„ ìˆ˜ì¶œì•¡, ì œí’ˆë³„ íŒë§¤ëŸ‰). Bar chartê°€ íš¨ê³¼ì ì…ë‹ˆë‹¤.
        - **(B) ì‹œê°„ì  ë³€í™”:** ì‹œê°„(ì—°ë„ë³„, ë¶„ê¸°ë³„, ì›”ë³„ ë“±)ì— ë”°ë¥¸ ë°ì´í„°ì˜ **ë³€í™” ì¶”ì„¸**ë¥¼ ë³´ì—¬ì¤„ ë•Œ (ì˜ˆ: ì—°ë„ë³„ ì‹œì¥ ê·œëª¨ ë³€í™”). Line chartê°€ íš¨ê³¼ì ì…ë‹ˆë‹¤.
        - **(C) êµ¬ì„± ë¹„ìœ¨:** ì „ì²´ì— ëŒ€í•œ ê° ë¶€ë¶„ì˜ **ë¹„ìœ¨**ì´ë‚˜ ì ìœ ìœ¨ì„ ë³´ì—¬ì¤„ ë•Œ (ì˜ˆ: ì‹œì¥ ì ìœ ìœ¨, ì„¤ë¬¸ì¡°ì‚¬ ì‘ë‹µ ë¹„ìœ¨). Pie/Doughnut chartê°€ íš¨ê³¼ì ì…ë‹ˆë‹¤.

    - **ë‹¤ìŒì˜ ê²½ìš°ì—ëŠ” `synthesis`ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:**
        - ë‹¨ìˆœí•œ ì‚¬ì‹¤, íŠ¹ì • ìˆ˜ì¹˜, ê°„ë‹¨í•œ ëª©ë¡ì„ ë‚˜ì—´í•˜ëŠ” ê²½ìš° (ì˜ˆ: 'A ì œí’ˆì˜ ê°€ê²©ì€ 5,000ì›').
        - ë°ì´í„°ê°€ ì •ì„±ì ì´ê±°ë‚˜ ì„œìˆ ì ì¸ ë¶„ì„, ê°œë… ì„¤ëª…, ìš”ì•½, ê²°ë¡ ì¸ ê²½ìš°.
        - ìˆ˜ì¹˜ ë°ì´í„°ê°€ ì¼ë¶€ í¬í•¨ë˜ì–´ ìˆì§€ë§Œ, ë¹„êµ/ì¶”ì„¸/ë¹„ìœ¨ì„ ë³´ì—¬ì£¼ëŠ” ëª…í™•í•œ ìŠ¤í† ë¦¬ê°€ ì—†ëŠ” ê²½ìš°.
        - **í‘œ(Table)ê°€ ì°¨íŠ¸ë³´ë‹¤ ì •ë³´ë¥¼ ë” ëª…í™•í•˜ê³  ì •í™•í•˜ê²Œ ì „ë‹¬í•  ìˆ˜ ìˆë‹¤ê³  íŒë‹¨ë˜ëŠ” ê²½ìš°.**

    **ì¤‘ìš”: ì°¨íŠ¸ ìƒì„± ì„¹ì…˜ ('full_data_for_chart')ì— ëŒ€í•œ íŠ¹ë³„ ê·œì¹™**:
    - ì°¨íŠ¸ ìƒì„±ì´ í•„ìš”í•œ ì„¹ì…˜ì€ **ê¸°ë³¸ ë°ì´í„° ì™¸ì— ì¶”ê°€ì ìœ¼ë¡œ í†µê³„/í‘œ/ìˆ˜ì¹˜ ë°ì´í„°ê°€ í’ë¶€í•œ ë°ì´í„°ë„ í¬í•¨**í•´ì•¼ í•©ë‹ˆë‹¤
    - ë‹¤ìŒê³¼ ê°™ì€ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë°ì´í„°ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì¶”ê°€ ì„ íƒí•˜ì„¸ìš”:
      * **ìˆ˜ì¹˜ í‚¤ì›Œë“œ**: "ë§¤ì¶œì•¡", "ì ìœ ìœ¨", "ì¦ê°€ìœ¨", "ê°ì†Œìœ¨", "í¼ì„¼íŠ¸", "%", "ì–µì›", "ì¡°ì›", "í†¤", "ê°œ"
      * **í‘œ/í†µê³„ í‚¤ì›Œë“œ**: "í‘œ", "í†µê³„", "í˜„í™©í‘œ", "ë°ì´í„°", "ìˆœìœ„", "ë¹„êµ", "ë¶„ì„"
      * **ì‹œê³„ì—´ í‚¤ì›Œë“œ**: "2024ë…„", "2025ë…„", "ì›”ë³„", "ë¶„ê¸°ë³„", "ì—°ë„ë³„", "ë³€í™”"
    - ì°¨íŠ¸ ìƒì„± ì„¹ì…˜ì˜ `use_contents`ëŠ” **ê¸°ë³¸ ë°ì´í„° 3-5ê°œ + í†µê³„/ìˆ˜ì¹˜ ë°ì´í„° 2-3ê°œ**ë¡œ êµ¬ì„±í•˜ì„¸ìš”
    - ì˜ˆì‹œ: [0, 2, 5] (ê¸°ë³¸ ë°ì´í„°) + [8, 12] (í†µê³„ ë°ì´í„°) = [0, 2, 5, 8, 12]

    5. **â­ ë§¤ìš° ì¤‘ìš”: ë°ì´í„° ì¶©ë¶„ì„± ê²€ì¦ (ì—„ê²©í•œ ê¸°ì¤€)**:
    - **`full_data_for_chart` ì„¹ì…˜ì˜ ì—„ê²©í•œ ê²€ì¦ ê¸°ì¤€**:
      * **ìˆ˜ì¹˜ ë°ì´í„° í•„ìˆ˜**: ìˆ«ì, í¼ì„¼íŠ¸, ê¸ˆì•¡, ë¹„ìœ¨ì´ **ëª…í™•íˆ ì œì‹œëœ ë°ì´í„°ê°€ 3ê°œ ì´ìƒ** ìˆì–´ì•¼ í•¨
      * **êµ¬ì¡°í™”ëœ ë°ì´í„° í•„ìˆ˜**: í‘œ, í†µê³„í‘œ, ìˆœìœ„, ë¹„êµ ë°ì´í„°ê°€ **êµ¬ì²´ì  ìˆ˜ì¹˜ì™€ í•¨ê»˜** ì œì‹œë˜ì–´ì•¼ í•¨
      * **ì°¨íŠ¸ ìƒì„± ê°€ëŠ¥ì„± í™•ì¸**:
        - ì§€ì—­ë³„/í’ˆëª©ë³„/ì‹œê¸°ë³„ **êµ¬ì²´ì  ìˆ˜ì¹˜ ë¹„êµ**ê°€ ê°€ëŠ¥í•œê°€?
        - "ì¬ë°°ë©´ì ê°ì†Œ", "ì •ì‹ì§€ì—°" ê°™ì€ **ì •ì„±ì  ì„¤ëª…ë§Œìœ¼ë¡œëŠ” ë¶ˆì¶©ë¶„**
        - ë°˜ë“œì‹œ "20% ê°ì†Œ", "3000í†¤ ìƒì‚°", "50ë§Œì›/í†¤" ê°™ì€ **êµ¬ì²´ì  ìˆ˜ì¹˜**ê°€ ìˆì–´ì•¼ í•¨
      * **ë¶€ì¡±í•œ ê²½ìš° ë°˜ë“œì‹œ `is_sufficient: false`** ì„¤ì •í•˜ê³  êµ¬ì²´ì ì¸ ì¶”ê°€ ê²€ìƒ‰ ì¿¼ë¦¬ ì œì•ˆ

    - **`synthesis` ì„¹ì…˜**: ì„¤ëª…, ë¶„ì„ì— í•„ìš”í•œ ê¸°ë³¸ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶©ë¶„ (ìˆ˜ì¹˜ ë°ì´í„° ë¶ˆí•„ìš”)

    - **`feedback_for_gatherer` ì‘ì„± ì‹œ**:
      * êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ë°ì´í„°ë¥¼ ìš”ì²­í•˜ëŠ” ê²€ìƒ‰ì–´ ì‘ì„±
      * ì˜ˆ: "2025ë…„ 7ì›” 8ì›” ê°•ì›ë„ í˜¸ë‚¨ì§€ì—­ ë†ì‚°ë¬¼ ìƒì‚°ëŸ‰ ì¬ë°°ë©´ì  í†µê³„ ìˆ˜ì¹˜ ë°ì´í„°"
      * ì˜ˆ: "ì§‘ì¤‘í˜¸ìš° í”¼í•´ ì§€ì—­ë³„ ë†ì‚°ë¬¼ ìƒì‚°ëŸ‰ ê°ì†Œìœ¨ í¼ì„¼íŠ¸ í†µê³„í‘œ"

    **ì„¹ì…˜ë³„ ë°ì´í„° ì„ íƒ ì˜ˆì‹œ**:
    - **ì°¨íŠ¸ ì„¹ì…˜**: "ì‹œì¥ ê·œëª¨ ë¶„ì„" (full_data_for_chart)
      â†’ ê¸°ë³¸: [0, 2, 5] (ì‹œì¥, ë§¤ì¶œ ê´€ë ¨) + í†µê³„: [8, 12] (ìˆ˜ì¹˜ í…Œì´ë¸” í¬í•¨) = [0, 2, 5, 8, 12]
    - **ì°¨íŠ¸ ì„¹ì…˜**: "ì§€ì—­ë³„ ìƒì‚° í˜„í™©" (full_data_for_chart)
      â†’ ê¸°ë³¸: [1, 3, 7] (ì§€ì—­, ìƒì‚° ê´€ë ¨) + í†µê³„: [9, 15] (ì§€ì—­ë³„ í†µê³„ í¬í•¨) = [1, 3, 7, 9, 15]
    - **í…ìŠ¤íŠ¸ ì„¹ì…˜**: "ì†Œë¹„ì íŠ¸ë Œë“œ" (synthesis) â†’ [4, 6, 10] (ì†Œë¹„ì, ì„ í˜¸ë„ ê´€ë ¨)
    - **ê²°ë¡  ì„¹ì…˜**: "ê²°ë¡  ë° ì œì–¸" (synthesis) â†’ [0, 1, 3, 5, 8] (ê° ì„¹ì…˜ í•µì‹¬ ë°ì´í„° ì¢…í•©)

    **ì¶œë ¥ í¬ë§· ì˜ˆì‹œ (ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ):**
   ```json
    {{
        "title": "êµ­ë‚´ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì‹œì¥ ë™í–¥ ë° ì†Œë¹„ì íŠ¸ë Œë“œ ë¶„ì„ ë³´ê³ ì„œ",
        "structure": [
            {{
                "section_title": "1. êµ­ë‚´ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì‹œì¥ ê·œëª¨ ë° ì„±ì¥ ì¶”ì´",
                "description": "(1) ë¶„ì„ ëª©í‘œ: ì—°ë„ë³„ ì‹œì¥ ê·œëª¨ ë³€í™”ë¥¼ í†µí•´ ì„±ì¥ ì¶”ì„¸ë¥¼ íŒŒì•…í•˜ê³  ë¯¸ë˜ ì‹œì¥ì„±ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\\n(2) ì°¨íŠ¸ í•„ìš”ì„±: í…ìŠ¤íŠ¸ë¡œ ë‚˜ì—´ëœ ìˆ˜ì¹˜ë³´ë‹¤ ì‹œê³„ì—´ ê·¸ë˜í”„ë¥¼ í†µí•´ ì‹œì¥ì˜ ì„±ì¥ íë¦„ì„ í•œëˆˆì— ì§ê´€ì ìœ¼ë¡œ ì „ë‹¬í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.\\n(3) ì‹œê°í™”í•  ë‚´ìš©: Xì¶•ì€ 'ì—°ë„', Yì¶•ì€ 'ì‹œì¥ ê·œëª¨(ì¡° ì›)'ë¥¼ ë‚˜íƒ€ë‚´ì–´ ì‹œê°„ì˜ íë¦„ì— ë”°ë¥¸ ë³€í™”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\\n(4) ì¶”ì²œ ì°¨íŠ¸ ìœ í˜•: ì‹œê°„ì— ë”°ë¥¸ ì—°ì†ì ì¸ ë°ì´í„° ë³€í™”ë¥¼ ë³´ì—¬ì£¼ëŠ” ë° ê°€ì¥ íš¨ê³¼ì ì¸ 'Line chart'ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.",
                "content_type": "full_data_for_chart",
                "use_contents": [0, 3, 7, 10],
                "is_sufficient": true,
                "feedback_for_gatherer": ""
             }},
            {{
                "section_title": "2. ì£¼ìš” í’ˆëª©ë³„ ì‹œì¥ ì ìœ ìœ¨",
                "description": "(1) ë¶„ì„ ëª©í‘œ: ì£¼ìš” ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ í’ˆëª©ì˜ ì‹œì¥ ì ìœ ìœ¨ì„ ë¹„êµí•˜ì—¬ í˜„ì¬ ì‹œì¥ì„ ì£¼ë„í•˜ëŠ” ì•„ì´í…œì„ íŒŒì•…í•©ë‹ˆë‹¤.\\n(2) ì°¨íŠ¸ í•„ìš”ì„±: ì „ì²´ ì‹œì¥ì—ì„œ ê° í’ˆëª©ì´ ì°¨ì§€í•˜ëŠ” ë¹„ì¤‘ì„ ì‹œê°ì ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ë¹„êµí•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.\\n(3) ì‹œê°í™”í•  ë‚´ìš©: ê° ì¡°ê°(slice)ì€ 'í’ˆëª©ëª…'ì„ ë‚˜íƒ€ë‚´ê³ , ê·¸ ê°’ì€ í•´ë‹¹ í’ˆëª©ì˜ 'ì‹œì¥ ì ìœ ìœ¨(%)'ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\\n(4) ì¶”ì²œ ì°¨íŠ¸ ìœ í˜•: ì „ì²´ì— ëŒ€í•œ ê° ë¶€ë¶„ì˜ ë¹„ìœ¨ì„ ë³´ì—¬ì£¼ëŠ” ë° ê°€ì¥ ì í•©í•œ 'Pie chart'ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.",
                "content_type": "full_data_for_chart",
                "use_contents": [1, 5, 9, 12],
                "is_sufficient": true,
                "feedback_for_gatherer": ""
            }},
            {{
                "section_title": "3. ìµœì‹  ì†Œë¹„ì íŠ¸ë Œë“œ ë° ì¸ì‹ ë³€í™”",
                "description": "ìµœì‹  ì†Œë¹„ì ì„¤ë¬¸ì¡°ì‚¬ ë° ê²€ìƒ‰ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì‹œì¥ì˜ í•µì‹¬ íŠ¸ë Œë“œì™€ ì†Œë¹„ì ì¸ì‹ ë³€í™”ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.",
                "content_type": "synthesis",
                "use_contents": [2, 4, 8],
                "is_sufficient": true,
                "feedback_for_gatherer": ""
             }},
            {{
                "section_title": "4. ì£¼ìš” ê²½ìŸì‚¬ë³„ ì‹œì¥ ì ìœ ìœ¨ ë¶„ì„ (ë°ì´í„° ë¶€ì¡±)",
                "description": "(1) ë¶„ì„ ëª©í‘œ: ì£¼ìš” ê²½ìŸì‚¬ë“¤ì˜ ì‹œì¥ ì ìœ ìœ¨ì„ ì‹œê°ì ìœ¼ë¡œ ë¹„êµí•˜ì—¬ ì‹œì¥ ë‚´ ê²½ìŸ êµ¬ë„ë¥¼ ëª…í™•íˆ íŒŒì•…í•©ë‹ˆë‹¤.\\n(2) ì°¨íŠ¸ í•„ìš”ì„±: ê° ê²½ìŸì‚¬ì˜ ìœ„ì¹˜ë¥¼ ì§ê´€ì ìœ¼ë¡œ ë¹„êµí•˜ê¸° ìœ„í•´ Bar chartë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\\n(3) ì‹œê°í™”í•  ë‚´ìš©: Xì¶•ì€ 'ê²½ìŸì‚¬ëª…', Yì¶•ì€ 'ì‹œì¥ ì ìœ ìœ¨(%)'ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\\n(4) ì¶”ì²œ ì°¨íŠ¸ ìœ í˜•: 'Bar chart'ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.",
                "content_type": "full_data_for_chart",
                "use_contents": [2, 8],
                "is_sufficient": false,
                "feedback_for_gatherer": {{
                    "tool": "vector_db_search",
                    "query": "2024ë…„ 2025ë…„ êµ­ë‚´ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ì‹œì¥ ê²½ìŸì‚¬ë³„ ì ìœ ìœ¨ í†µê³„ ë°ì´í„° í‘œ"
                }}
            }},
            {{
                "section_title": "5. ê²°ë¡  ë° ì‹œì¥ ì „ë§",
                "description": "ì•ì„œ ë¶„ì„í•œ ì‹œì¥ ê·œëª¨, í’ˆëª©ë³„ ì ìœ ìœ¨, ì†Œë¹„ì íŠ¸ë Œë“œë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ê²°ë¡ ì„ ë„ì¶œí•˜ê³  í–¥í›„ ì‹œì¥ì„ ì „ë§í•©ë‹ˆë‹¤.",
                "content_type": "synthesis",
                "use_contents": [0, 1, 2, 5, 7],
                "is_sufficient": true,
                "feedback_for_gatherer": ""
            }}
        ]
    }}
    ```

    **ì¤‘ìš”**: `use_contents` ë°°ì—´ì—ëŠ” ë°˜ë“œì‹œ ìœ„ì— ì œì‹œëœ ì¸ë±ìŠ¤ ë²ˆí˜¸ {selected_indexes} ì¤‘ì—ì„œë§Œ ì„ íƒí•˜ì„¸ìš”.
    """

        try:
            response = await self._invoke_with_fallback(
                prompt,
                self.llm_flash,
                self.llm_flash_backup,
                self.llm_openai_mini
            )

            print(f"  - ë³´ê³ ì„œ êµ¬ì¡° ì„¤ê³„ ì‘ë‹µ ê¸¸ì´: {len(response.content)} ë¬¸ì")

            # JSON íŒŒì‹±
            design_result = json.loads(re.search(r'\{.*\}', response.content, re.DOTALL).group())

            # ë³´ê³ ì„œ êµ¬ì¡° ì„¤ê³„ ê²°ê³¼ JSON dumpë¡œ ì¶œë ¥
            print(f"  - ë³´ê³ ì„œ êµ¬ì¡° ì„¤ê³„ ê²°ê³¼ JSON:")
            print(json.dumps(design_result, ensure_ascii=False, indent=2))

            # â­ í•µì‹¬ ì¶”ê°€: ì¸ë±ìŠ¤ ìœ íš¨ì„± ê²€ì¦ ë° ë°ì´í„° ì¶©ë¶„ì„± ì¬ê²€ì¦
            print(f"  - êµ¬ì¡° ì„¤ê³„ ê²°ê³¼ ê²€ì¦ ë° ë°ì´í„° ì¶©ë¶„ì„± ì¬í™•ì¸:")
            for i, section in enumerate(design_result.get("structure", [])):
                section_title = section.get("section_title", f"ì„¹ì…˜ {i+1}")
                content_type = section.get("content_type", "synthesis")
                use_contents = section.get("use_contents", [])
                is_sufficient = section.get("is_sufficient", True)

                # ìœ íš¨í•œ ì¸ë±ìŠ¤ë§Œ í•„í„°ë§
                valid_use_contents = []
                for idx in use_contents:
                    if isinstance(idx, int) and idx in selected_indexes:
                        valid_use_contents.append(idx)
                    else:
                        print(f"    ê²½ê³ : ì˜ëª»ëœ ì¸ë±ìŠ¤ {idx} ì œê±°ë¨ (í—ˆìš©ëœ ì¸ë±ìŠ¤: {selected_indexes})")

                section["use_contents"] = valid_use_contents

                print(f"    '{section_title}' ({content_type}): {len(valid_use_contents)}ê°œ ë°ì´í„°")
                print(f"      ì‚¬ìš© ì¸ë±ìŠ¤: {valid_use_contents}")
                print(f"      LLM íŒë‹¨ is_sufficient: {is_sufficient}")

                # â­ ì°¨íŠ¸ ì„¹ì…˜ì— ëŒ€í•œ ì‹¤ì œ ë°ì´í„° ë‚´ìš© ê¸°ë°˜ ì¬ê²€ì¦
                if content_type == "full_data_for_chart":
                    has_numeric_data = False
                    numeric_count = 0

                    print(f"      ğŸ“Š ì°¨íŠ¸ ì„¹ì…˜ ë°ì´í„° ë‚´ìš© ê²€ì¦:")
                    for idx in valid_use_contents:
                        if 0 <= idx < len(data):
                            data_item = data[idx]
                            content = getattr(data_item, 'content', '')
                            title = getattr(data_item, 'title', 'No Title')

                            # ìˆ˜ì¹˜ ë°ì´í„° íŒ¨í„´ ê²€ì‚¬ (ë” í¬ê´„ì )
                            numeric_patterns = [
                                r'\d+%',                    # í¼ì„¼íŠ¸
                                r'\d+\.\d+%',               # ì†Œìˆ˜ì  í¼ì„¼íŠ¸
                                r'\d+ì–µì›?',                 # ì–µì›
                                r'\d+ì¡°ì›?',                 # ì¡°ì›
                                r'\d+ë§Œì›?',                 # ë§Œì›
                                r'\d+ì²œí†¤',                  # ì²œí†¤
                                r'\d+í†¤',                   # í†¤
                                r'\d+,\d+',                 # ì²œì˜ ìë¦¬ ì½¤ë§ˆ
                                r'\d+\s*ì›/\s*\d+\s*ê°œ?',     # ë‹¨ê°€ (ì›/ê°œ, ì›/20ê°œ ë“±)
                                r'ë‹¨ìœ„:\s*ì›/\d+ê°œ?',          # "ë‹¨ìœ„: ì›/20ê°œ" í˜•íƒœ
                                r'\d+\s+\d+\s+\d+\s+\d+',     # ì—°ì†ëœ ìˆ«ìë“¤ (í…Œì´ë¸” ë°ì´í„°)
                                r'\d{4}ë…„\s+\d+',           # "2025ë…„ 38660" ê°™ì€ ì—°ë„+ìˆ«ì
                                r'\d+ì›”\s+\d+',             # "7ì›” 14324" ê°™ì€ ì›”+ìˆ«ì
                                r'ì¦ê°€ìœ¨.*?\d+',             # ì¦ê°€ìœ¨ + ìˆ«ì
                                r'ê°ì†Œìœ¨.*?\d+',             # ê°ì†Œìœ¨ + ìˆ«ì
                                r'ì ìœ ìœ¨.*?\d+',             # ì ìœ ìœ¨ + ìˆ«ì
                                r'\d+\s+\d+\s+\d+',         # í‘œ í˜•íƒœì˜ ì—°ì† ìˆ«ì
                                r'í‰ë…„\s+\d+',              # "í‰ë…„ 25686" ê°™ì€ ê¸°ì¤€ê°’
                            ]

                            numeric_matches = 0
                            for pattern in numeric_patterns:
                                matches = re.findall(pattern, content)
                                numeric_matches += len(matches)

                            if numeric_matches > 0:
                                has_numeric_data = True
                                numeric_count += numeric_matches
                                print(f"        [{idx}] âœ… ìˆ˜ì¹˜ ë°ì´í„° {numeric_matches}ê°œ ë°œê²¬: {title[:30]}...")
                            else:
                                print(f"        [{idx}] âŒ ìˆ˜ì¹˜ ë°ì´í„° ì—†ìŒ: {title[:30]}...")

                    # ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ì¶©ë¶„ì„± ì¬íŒë‹¨ (ë” ê´€ëŒ€í•œ ê¸°ì¤€)
                    # í…Œì´ë¸”ì´ë‚˜ ì‹œê³„ì—´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¶©ë¶„í•˜ë‹¤ê³  íŒë‹¨
                    has_table_data = any('ë‹¨ìœ„:' in getattr(data[idx], 'content', '') or
                                       'êµ¬ë¶„' in getattr(data[idx], 'content', '') or
                                       ('ë…„' in getattr(data[idx], 'content', '') and 'ì›”' in getattr(data[idx], 'content', ''))
                                       for idx in valid_use_contents if 0 <= idx < len(data))

                    # ê°€ê²©/ë‹¨ê°€ ì •ë³´ë„ ì°¨íŠ¸ ìƒì„± ê°€ëŠ¥í•˜ë‹¤ê³  íŒë‹¨
                    has_price_data = any(('ì›/' in getattr(data[idx], 'content', '') or
                                        'ê°€ê²©' in getattr(data[idx], 'title', ''))
                                       for idx in valid_use_contents if 0 <= idx < len(data))

                    actually_sufficient = (has_numeric_data and numeric_count >= 2) or has_table_data or has_price_data

                    print(f"        ğŸ’¡ í…Œì´ë¸” í˜•íƒœ ë°ì´í„° ë°œê²¬: {has_table_data}")
                    print(f"        ğŸ’° ê°€ê²©/ë‹¨ê°€ ë°ì´í„° ë°œê²¬: {has_price_data}")
                    print(f"        ğŸ“Š ìµœì¢… ì¶©ë¶„ì„± íŒë‹¨: ìˆ˜ì¹˜({numeric_count}ê°œ) + í…Œì´ë¸”({has_table_data}) + ê°€ê²©({has_price_data}) = {actually_sufficient}")

                    if is_sufficient and not actually_sufficient:
                        print(f"      ğŸ”§ LLM íŒë‹¨ ì˜¤ë¥˜ ê°ì§€: sufficient=trueì˜€ì§€ë§Œ ì‹¤ì œ ìˆ˜ì¹˜ ë°ì´í„° ë¶€ì¡± ({numeric_count}ê°œ)")
                        print(f"      ğŸ”„ is_sufficientë¥¼ falseë¡œ ìˆ˜ì •í•˜ê³  ì¶”ê°€ ê²€ìƒ‰ ìš”ì²­ ìƒì„±")
                        section["is_sufficient"] = False
                        section["feedback_for_gatherer"] = {
                            "tool": "vector_db_search",
                            "query": f"{section_title} ê´€ë ¨ êµ¬ì²´ì  ìˆ˜ì¹˜ í†µê³„ ë°ì´í„° í‘œ ê·¸ë˜í”„ í¼ì„¼íŠ¸ ê¸ˆì•¡ ìƒì‚°ëŸ‰"
                        }
                    elif actually_sufficient:
                        print(f"      âœ… ì°¨íŠ¸ ìƒì„± ê°€ëŠ¥: ìˆ˜ì¹˜ ë°ì´í„° {numeric_count}ê°œ ì¶©ë¶„")
                    else:
                        print(f"      âš ï¸  ì°¨íŠ¸ ìƒì„± ì–´ë ¤ì›€: ìˆ˜ì¹˜ ë°ì´í„° {numeric_count}ê°œ ë¶€ì¡±")

                # ì‚¬ìš©ë  ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                for idx in valid_use_contents[:2]:  # ì²˜ìŒ 2ê°œë§Œ
                    if 0 <= idx < len(data):
                        data_item = data[idx]
                        print(f"      [{idx:2d}] {getattr(data_item, 'source', 'Unknown'):10s} | {getattr(data_item, 'title', 'No Title')[:40]}")

            print(f"  - ë³´ê³ ì„œ êµ¬ì¡° ì„¤ê³„ ì™„ë£Œ: '{design_result.get('title', 'ì œëª©ì—†ìŒ')}'")
            yield {"type": "result", "data": design_result}

        except Exception as e:
            print(f"  - ë³´ê³ ì„œ êµ¬ì¡° ì„¤ê³„ ì‹¤íŒ¨: {e}")
            print(f"  - ì•ˆì „ ëª¨ë“œë¡œ ê¸°ë³¸ êµ¬ì¡° ìƒì„±")

            # ì•ˆì „ ëª¨ë“œ: ëª¨ë“  ì„ íƒëœ ì¸ë±ìŠ¤ë¥¼ í•˜ë‚˜ì˜ ì„¹ì…˜ì—ì„œ ì‚¬ìš©
            fallback_design = {
                "title": f"{query} - í†µí•© ë¶„ì„ ë³´ê³ ì„œ",
                "structure": [{
                    "section_title": "ì¢…í•© ë¶„ì„", "description": "ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ì¢…í•© ë¶„ì„í•©ë‹ˆë‹¤.",
                    "content_type": "synthesis", "use_contents": selected_indexes[:10],
                    "is_sufficient": True, "feedback_for_gatherer": ""
                }]
            }
            yield {"type": "result", "data": fallback_design}


    # worker_agents.py - ProcessorAgent í´ë˜ìŠ¤ì˜ ìˆ˜ì •ëœ í•¨ìˆ˜ë“¤

    async def _synthesize_data_for_section(self, section_title: str, section_data: List[SearchResult]) -> str:
        """â­ ìˆ˜ì •: ì„¹ì…˜ë³„ ì„ íƒëœ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬ ì¶œì²˜ ë²ˆí˜¸ ì •í™•íˆ ë§¤í•‘"""

        # â­ í•µì‹¬ ê°œì„ : ì„¹ì…˜ë³„ ì„ íƒëœ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬ ì¶œì²˜ ì •ë³´ ìƒì„±
        context_with_sources = ""
        for i, res in enumerate(section_data):  # section_dataë§Œ ì‚¬ìš© (all_data ëŒ€ì‹ )
            source_info = ""
            source_link = ""

            # Web search ê²°ê³¼ì¸ ê²½ìš°
            if hasattr(res, 'source') and 'web_search' in str(res.source).lower():
                if hasattr(res, 'url') and res.url:
                    source_link = res.url
                    source_info = f"ì›¹ ì¶œì²˜: {res.url}"
                elif hasattr(res, 'metadata') and res.metadata and 'link' in res.metadata:
                    source_link = res.metadata['link']
                    source_info = f"ì›¹ ì¶œì²˜: {res.metadata['link']}"
                else:
                    source_info = "ì›¹ ê²€ìƒ‰ ê²°ê³¼"
                    source_link = "ì›¹ ê²€ìƒ‰"

            # Vector DB ê²°ê³¼ì¸ ê²½ìš°
            elif hasattr(res, 'source_url'):
                source_info = f"ë¬¸ì„œ ì¶œì²˜: {res.source_url}"
                source_link = res.source_url
            elif hasattr(res, 'title'):
                source_info = f"ë¬¸ì„œ: {res.title}"
                source_link = res.title
            else:
                source_name = res.source if hasattr(res, 'source') else 'Vector DB'
                source_info = f"ì¶œì²˜: {source_name}"
                source_link = source_name

            # í•µì‹¬: ì„¹ì…˜ ë°ì´í„° ë‚´ì—ì„œì˜ ì¸ë±ìŠ¤ ì‚¬ìš© (0, 1, 2...)
            context_with_sources += f"--- ë¬¸ì„œ ID {i}: [{source_info}] ---\nì œëª©: {res.title}\në‚´ìš©: {res.content}\nì¶œì²˜_ë§í¬: {source_link}\n\n"

        prompt = f"""
    ë‹¹ì‹ ì€ ì—¬ëŸ¬ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì¢…í•©í•˜ì—¬ íŠ¹ì • ì£¼ì œì— ëŒ€í•œ ë¶„ì„ ë³´ê³ ì„œì˜ í•œ ì„¹ì…˜ì„ ì €ìˆ í•˜ëŠ” ì£¼ì œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

    **ì‘ì„±í•  ì„¹ì…˜ì˜ ì£¼ì œ**: "{section_title}"

    **ì‚¬ìš© ë°ì´í„° ì¸ë±ìŠ¤**

    **ì°¸ê³ í•  ì„ íƒëœ ë°ì´í„°** (ì„¹ì…˜ë³„ë¡œ ì—„ì„ ëœ ê´€ë ¨ ë°ì´í„°):
    {context_with_sources[:8000]}

    **ì‘ì„± ì§€ì¹¨**:
    1. **í•µì‹¬ ì •ë³´ ì¶”ì¶œ**: '{section_title}' ì£¼ì œì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ í•µì‹¬ ì‚¬ì‹¤, ìˆ˜ì¹˜, í†µê³„ ìœ„ì£¼ë¡œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
    2. **ê°„ê²°í•œ ìš”ì•½**: ì •ë³´ë¥¼ ë‹¨ìˆœíˆ ë‚˜ì—´í•˜ì§€ ë§ê³ , 1~2 ë¬¸ë‹¨ ì´ë‚´ì˜ ê°„ê²°í•˜ê³  ë…¼ë¦¬ì ì¸ í•µì‹¬ ìš”ì•½ë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±í•´ì£¼ì„¸ìš”.
    3. **ì¤‘ë³µ ì œê±°**: ì—¬ëŸ¬ ë¬¸ì„œì— ê±¸ì³ ë°˜ë³µë˜ëŠ” ë‚´ìš©ì€ í•˜ë‚˜ë¡œ í†µí•©í•˜ì—¬ ì œê±°í•˜ì„¸ìš”.
    4. **ê°ê´€ì„± ìœ ì§€**: ë°ì´í„°ì— ê¸°ë°˜í•˜ì—¬ ê°ê´€ì ì¸ ì‚¬ì‹¤ë§Œì„ ì „ë‹¬í•´ì£¼ì„¸ìš”.
    5. **â­ ì¶œì²˜ ì •ë³´ ë³´ì¡´**: ì¤‘ìš”í•œ ì •ë³´ë‚˜ ìˆ˜ì¹˜ë¥¼ ì–¸ê¸‰í•  ë•Œ í•´ë‹¹ ì •ë³´ì˜ ì¶œì²˜ë¥¼ [SOURCE:ìˆ«ì] í˜•ì‹ìœ¼ë¡œ í‘œê¸°í•˜ì„¸ìš”. ë°˜ë“œì‹œ ìˆ«ìë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
    - **ë¬¸ì„œ ID ë²ˆí˜¸ë¥¼ ì‚¬ìš©**
    - ì˜ˆì‹œ: "ì‹œì¥ ê·œëª¨ê°€ ì¦ê°€í–ˆìŠµë‹ˆë‹¤ [SOURCE:1]", "ë§¤ì¶œì´ ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤ [SOURCE:2]"
    - ì˜ëª»ëœ ì˜ˆì‹œ: [SOURCE:ë°ì´í„° 1], [SOURCE:ë¬¸ì„œ 1] (ì´ëŸ° í˜•ì‹ ì‚¬ìš© ê¸ˆì§€)
    6. **â­ ë…¸ì…˜ ìŠ¤íƒ€ì¼ ë§ˆí¬ë‹¤ìš´ ì ê·¹ í™œìš©**:
    - **ì¤‘ìš”í•œ í‚¤ì›Œë“œë‚˜ ìˆ˜ì¹˜**: **êµµì€ ê¸€ì”¨**ë¡œ ê°•ì¡°
    - *ì¼ë°˜ì ì¸ ê°•ì¡°ë‚˜ íŠ¸ë Œë“œ*: *ê¸°ìš¸ì„ì²´*ë¡œ í‘œí˜„
    - **í•µì‹¬ í¬ì¸íŠ¸ë‚˜ ê²°ë¡ **: > ì¸ìš©ë¬¸ í˜•íƒœë¡œ ê°•ì¡°
    - **í•­ëª©ì´ ì—¬ëŸ¬ ê°œ**: - ì²« ë²ˆì§¸ í•­ëª©, - ë‘ ë²ˆì§¸ í•­ëª© í˜•íƒœ
    - **í•˜ìœ„ ë¶„ë¥˜**:   - ì„¸ë¶€ í•­ëª© (ë“¤ì—¬ì“°ê¸°)
    - **ë‹¨ë½ êµ¬ë¶„**: ë‚´ìš© ë³€í™” ì‹œ ê³µë°± ë¼ì¸ìœ¼ë¡œ ëª…í™•íˆ êµ¬ë¶„

    **ê²°ê³¼ë¬¼ (í•µì‹¬ ìš”ì•½ë³¸)**:
    """
        response = await self._invoke_with_fallback(
            prompt,
            self.llm_flash,
            self.llm_flash_backup,
            self.llm_openai_mini
        )
        return response.content

    async def generate_section_streaming(
        self,
        section: Dict[str, Any],
        full_data_dict: Dict[int, Dict],
        original_query: str,
        use_indexes: List[int],
        awareness_context: str = "",
        state: Optional[Dict[str, Any]] = None
    ) -> AsyncGenerator[str, None]:
        """í˜ë¥´ì†Œë‚˜, ì „ì²´ êµ¬ì¡° ì¸ì§€(awareness_context), ì„¹ì…˜ ëª©í‘œ(description)ë¥¼ ë°˜ì˜í•˜ì—¬ ì „ì²´ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ì—ì„œ í•´ë‹¹ ì„¹ì…˜ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ìƒì„±"""
        section_title = section.get("section_title", "ì œëª© ì—†ìŒ")
        content_type = section.get("content_type", "synthesis")
        description = section.get("description", "ì´ ì„¹ì…˜ì˜ ë‚´ìš©ì„ ìš”ì•½í•©ë‹ˆë‹¤.")

        # í˜ë¥´ì†Œë‚˜ ì •ë³´ ì¶”ì¶œ
        persona_name = state.get("persona", "ê¸°ë³¸") if state else "ê¸°ë³¸"
        default_report_prompt = "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ AI ë¶„ì„ê°€ì…ë‹ˆë‹¤."
        persona_instruction = self.personas.get(persona_name, {}).get("report_prompt", default_report_prompt)

        print(f"  - ì„¹ì…˜ '{section_title}' ìƒì„±ì— '{persona_name}' í˜ë¥´ì†Œë‚˜ ìŠ¤íƒ€ì¼ ì ìš© (ì „ì²´ êµ¬ì¡° ì¸ì§€)")

        # ë³´ê³ ì„œ ì œëª©ê³¼ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ ì„¹ì…˜ ì œëª© í™•ì¸
        report_title = state.get("report_title", "") if state else ""

        # ì²« ë²ˆì§¸ ì„¹ì…˜ì´ê³  ì œëª©ì´ ë³´ê³ ì„œ ì œëª©ê³¼ ìœ ì‚¬í•œ ê²½ìš° í—¤ë” ìƒëµ
        is_duplicate_title = (
            report_title and
            (section_title.replace(" ", "").lower() in report_title.replace(" ", "").lower() or
             report_title.replace(" ", "").lower() in section_title.replace(" ", "").lower())
        )

        # ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ë§Œ ì„¹ì…˜ í—¤ë” ì¶œë ¥
        if not is_duplicate_title:
            section_header = f"\n\n## {section_title}\n\n"
            yield section_header
            print(f"  - ì„¹ì…˜ í—¤ë” ì¶œë ¥: {section_title}")
        else:
            print(f"  - ì„¹ì…˜ í—¤ë” ìƒëµ (ë³´ê³ ì„œ ì œëª©ê³¼ ì¤‘ë³µ): {section_title} â‰ˆ {report_title}")
            # ì¤‘ë³µì¸ ê²½ìš° ê°„ê²©ë§Œ ì¶”ê°€
            yield "\n\n"

        prompt = "" # ìµœì¢… í”„ë¡¬í”„íŠ¸ë¥¼ ë‹´ì„ ë³€ìˆ˜

        if content_type == "synthesis":
            print(f"\nğŸ” === SECTION STREAMING ë””ë²„ê¹… ({section_title}) ===")
            print(f"use_indexes: {use_indexes}")
            print(f"full_data_dict í‚¤ë“¤: {list(full_data_dict.keys()) if full_data_dict else 'None'}")

            # ì „ì²´ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ì—ì„œ í•´ë‹¹ ì¸ë±ìŠ¤ë§Œ ì„ ë³„í•˜ì—¬ í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ í¬ë§·íŒ…
            section_data_content = ""
            valid_indexes = []
            for actual_index in use_indexes:
                if actual_index in full_data_dict:
                    valid_indexes.append(actual_index)
                    data_info = full_data_dict[actual_index]
                    section_data_content += f"**ë°ì´í„° {actual_index}: {data_info['source']}**\n"
                    section_data_content += f"- **ì œëª©**: {data_info['title']}\n"
                    section_data_content += f"- **ë‚´ìš©**: {data_info['content']}\n\n"
                    print(f"  âœ… [{actual_index}] ë°ì´í„° ë§¤í•‘ ì„±ê³µ: '{data_info['title'][:30]}...'")
                else:
                    print(f"  âŒ [{actual_index}] full_data_dictì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ!")

            print(f"ìœ íš¨í•œ ì¸ë±ìŠ¤ë“¤: {valid_indexes}")
            print(f"í”„ë¡¬í”„íŠ¸ì—ì„œ ì‚¬ìš©í•  SOURCE ë²ˆí˜¸ë“¤: {valid_indexes}")


            prompt_template = """

    {persona_instruction}

    ë‹¹ì‹ ì€ ìœ„ì˜ í˜ë¥´ì†Œë‚˜ ì§€ì¹¨ì„ ë”°ë¥´ëŠ” ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤. ì „ì²´ ë³´ê³ ì„œì˜ ì¼ë¶€ì¸ í•œ ì„¹ì…˜ì„ ì‘ì„±í•˜ëŠ” ì„ë¬´ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤.

    **ì‚¬ìš©ìì˜ ì „ì²´ ì§ˆë¬¸**: "{original_query}"

    ---
    **[ë§¤ìš° ì¤‘ìš”] ì „ì²´ ë³´ê³ ì„œ êµ¬ì¡° ë° ë‹¹ì‹ ì˜ ì—­í• **:
    ë‹¹ì‹ ì€ ì•„ë˜ êµ¬ì¡°ë¡œ êµ¬ì„±ëœ ì „ì²´ ë³´ê³ ì„œì—ì„œ **ì˜¤ì§ '{section_title}' ì„¹ì…˜ë§Œ**ì„ ì±…ì„ì§€ê³  ìˆìŠµë‹ˆë‹¤.
    ë‹¤ë¥¸ ì „ë¬¸ê°€ë“¤ì´ ë‚˜ë¨¸ì§€ ì„¹ì…˜ë“¤ì„ ë™ì‹œì— ì‘ì„±í•˜ê³  ìˆìœ¼ë¯€ë¡œ, **ë‹¤ë¥¸ ì„¹ì…˜ì˜ ì£¼ì œë¥¼ ì ˆëŒ€ ì¹¨ë²”í•˜ì§€ ë§ê³  ë‹¹ì‹ ì˜ ì—­í• ì—ë§Œ ì§‘ì¤‘í•˜ì„¸ìš”.**

    {awareness_context}
    ---

    **í˜„ì¬ ì‘ì„±í•  ì„¹ì…˜ ì œëª©**: "{section_title}"
    **ì´ ì„¹ì…˜ì˜ í•µì‹¬ ëª©í‘œ**: "{description}"

    **ì°¸ê³  ë°ì´í„° (ì‹¤ì œ ì¸ë±ìŠ¤ ë²ˆí˜¸ í¬í•¨)**:
    {section_data_content}

    **ì‘ì„± ì§€ì¹¨ (ë§¤ìš° ì¤‘ìš”)**:
    1. **ì—­í•  ì¤€ìˆ˜**: ìœ„ 'ì „ì²´ ë³´ê³ ì„œ êµ¬ì¡°'ì™€ 'í•µì‹¬ ëª©í‘œ'ë¥¼ ë°˜ë“œì‹œ ì¸ì§€í•˜ê³ , '{section_title}'ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ë§Œ ê¹Šì´ ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”.
    2. **í˜ë¥´ì†Œë‚˜ ì—­í•  ìœ ì§€**: ë‹¹ì‹ ì˜ ì—­í• ê³¼ ë§íˆ¬, ë¶„ì„ ê´€ì ì„ ë°˜ë“œì‹œ ìœ ì§€í•˜ë©° ì‘ì„±í•˜ì„¸ìš”.
    3. **ê°„ê²°ì„± ìœ ì§€**: ë°˜ë“œì‹œ 1~2 ë¬¸ë‹¨ ì´ë‚´ë¡œ, ê°€ì¥ í•µì‹¬ì ì¸ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”.
    4. **ì œëª© ë°˜ë³µ ê¸ˆì§€**: ì£¼ì–´ì§„ ì„¹ì…˜ ì œëª©ì„ ì ˆëŒ€ ë°˜ë³µí•´ì„œ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”. ë°”ë¡œ ë³¸ë¬¸ ë‚´ìš©ìœ¼ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.
    5. **ë°ì´í„° ê¸°ë°˜**: ì°¸ê³  ë°ì´í„°ì— ìˆëŠ” êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ì‚¬ì‹¤, ì¸ìš©êµ¬ë¥¼ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ë‚´ìš©ì„ êµ¬ì„±í•˜ì„¸ìš”.
    6. **ì „ë¬¸ê°€ì  ë¬¸ì²´**: ëª…í™•í•˜ê³  ê°„ê²°í•˜ë©° ë…¼ë¦¬ì ì¸ ì „ë¬¸ê°€ì˜ í†¤ìœ¼ë¡œ ê¸€ì„ ì‘ì„±í•˜ì„¸ìš”.
    7. **â­ ë…¸ì…˜ ìŠ¤íƒ€ì¼ ë§ˆí¬ë‹¤ìš´ ì ê·¹ í™œìš© (ë§¤ìš° ì¤‘ìš”) - ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•¨**:

    **ê¸°ë³¸ í¬ë§·íŒ… (í•„ìˆ˜)**:
    - **í•µì‹¬ í‚¤ì›Œë“œë‚˜ ì¤‘ìš”í•œ ìˆ˜ì¹˜**: ë°˜ë“œì‹œ **êµµì€ ê¸€ì”¨**ë¡œ ê°•ì¡° (ì˜ˆ: **58,000ì›/10kg**, **ì „ë…„ ëŒ€ë¹„ 81.4% í•˜ë½**)
    - *ë³€í™”ë‚˜ ì¶”ì„¸*: ë°˜ë“œì‹œ *ê¸°ìš¸ì„ì²´*ë¡œ í‘œí˜„ (ì˜ˆ: *ì „ë…„ ëŒ€ë¹„ ê°ì†Œ*, *ì§‘ì¤‘í˜¸ìš°ë¡œ ì¸í•œ í”¼í•´*)
    - ë¬¸ë‹¨ë³„ë¡œ **ë°˜ë“œì‹œ 2-3ê°œ ì´ìƒì˜ ê°•ì¡°** í¬í•¨í•  ê²ƒ

    **êµ¬ì¡°í™” (í•„ìˆ˜)**:
    - **ì¤‘ìš”í•œ ì¸ì‚¬ì´íŠ¸ë‚˜ ê²°ë¡ **: ë°˜ë“œì‹œ `> **í•µì‹¬ ìš”ì•½**: ë‚´ìš©` í˜•íƒœë¡œ ë¸”ë¡ì¿¼íŠ¸ ì‚¬ìš©
    - **ë¹„êµ ì •ë³´ê°€ 3ê°œ ì´ìƒ**: ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” ì‚¬ìš©
    - **ëª©ë¡ í˜•íƒœ ì •ë³´**: ë°˜ë“œì‹œ `-` ë¶ˆë¦¿ í¬ì¸íŠ¸ ì‚¬ìš©
    - **ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ê°€ ìˆìœ¼ë©´**: ë°˜ë“œì‹œ `### ì†Œì œëª©` ì‚¬ìš©

    **í•„ìˆ˜ ì˜ˆì‹œ íŒ¨í„´**:
    ```
    **ë°°(Pear)**ì˜ ì „ì²´ ì¬ë°°ë©´ì ì€ **9,361ha**ë¡œ ì „ë…„ ëŒ€ë¹„ **0.6% ê°ì†Œ**í–ˆìœ¼ë©°, *ì§‘ì¤‘í˜¸ìš° í”¼í•´ ì§€ì—­*ìœ¼ë¡œ ë¶„ë¥˜ë˜ëŠ” ê°•ì› ë° í˜¸ë‚¨ ì§€ì—­ì—ì„œì˜ ë³€ë™ì´ ë‘ë“œëŸ¬ì¡ŒìŠµë‹ˆë‹¤. [SOURCE:2, 3]

    | ì‹ì¬ë£Œ | ì£¼ìš” ìƒì‚°ì§€ | ì¬ë°°ë©´ì  ë³€í™” | ì£¼ìš” ì›ì¸ |
    | :--- | :--- | :--- | :--- |
    | **ë°°** | ì „êµ­ | **-0.6%** (ì „ë…„ ëŒ€ë¹„) | *ì „êµ­ì  ì¬ë°°ë©´ì  ê°ì†Œ ì¶”ì„¸* [SOURCE:2] |
    | **í¬ë„** | ì „êµ­ | **-2.3% ~ -6.7%** | *ì‘í˜•ë³„ ë©´ì  ê°ì†Œ* [SOURCE:4] |

    > **í•µì‹¬ ê²°ë¡ **: ì§‘ì¤‘í˜¸ìš° ì§ì ‘ í”¼í•´ëŠ” *ë¯¸ë¯¸í•œ ìˆ˜ì¤€*ì´ì—ˆìœ¼ë‚˜, ê³ ì˜¨ ë° ê°€ë­„ì´ **ê³¼ë¹„ëŒ€ ì§€ì—°** ë“± í’ˆì§ˆì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ ë” ì»¸ìŠµë‹ˆë‹¤.
    ```

    **âš ï¸ ê°•ì œ ìš”êµ¬ì‚¬í•­**: ëª¨ë“  ë¬¸ë‹¨ì—ì„œ **êµµì€ ê¸€ì”¨** 2ê°œ ì´ìƒ, *ê¸°ìš¸ì„ì²´* 1ê°œ ì´ìƒ ë°˜ë“œì‹œ ì‚¬ìš©

    **í‘œ ì‘ì„± ì§€ì¹¨**:
    - 3ê°œ ì´ìƒì˜ í•­ëª©ì„ ë¹„êµí•˜ê±°ë‚˜ ë¶„ë¥˜í•  ë•Œ í…Œì´ë¸” ì‚¬ìš©
    - í…Œì´ë¸” í˜•ì‹: `| í•­ëª© | ë‚´ìš© | ë¹„ê³  |` ë° `| :--- | :--- | :--- |` êµ¬ë¶„ì„ 
    - í…Œì´ë¸” ì…€ ì•ˆì—ì„œë„ **êµµì€ ê¸€ì”¨**, *ê¸°ìš¸ì„ì²´*, [SOURCE:ìˆ«ì] ì¶œì²˜ í‘œê¸° ê°€ëŠ¥
    - ì˜ˆì‹œ:
    ```
    | ì£¼ìš” ì‹ì¬ë£Œ | ì£¼ìš” ìƒì‚°ì§€ | í˜„í™© ìš”ì•½ |
    | :--- | :--- | :--- |
    | **í† ë§ˆí† ** | í˜¸ë‚¨ ì§€ì—­ | ì§‘ì¤‘í˜¸ìš°ë¡œ **ìˆ˜í•´ ë°œìƒ** [SOURCE:2] |
    | **ë°°** | ì¶©ë‚¨, ì „ë‚¨ | í”¼í•´ëŠ” **ë¯¸ë¯¸í•œ ìˆ˜ì¤€** [SOURCE:1] |
    ```
    8. **â­ ì¶œì²˜ í‘œê¸° (ë°ì´í„° ì¸ë±ìŠ¤ ë²ˆí˜¸ ì‚¬ìš©)**: íŠ¹ì • ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•œ ë¬¸ì¥ ë°”ë¡œ ë’¤ì— [SOURCE:ìˆ«ì] í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ë¥¼ í‘œê¸°í•˜ì„¸ìš”.

    **ğŸ”´ ë§¤ìš° ì¤‘ìš” - SOURCE ë²ˆí˜¸ ì‚¬ìš© ê·œì¹™**:
    - **ë°˜ë“œì‹œ ìœ„ "ì°¸ê³  ë°ì´í„°"ì— ëª…ì‹œëœ "[ë°ì´í„° ì¸ë±ìŠ¤ X]" ì˜ X ë²ˆí˜¸ë§Œ ì‚¬ìš©í•˜ì„¸ìš”**
    - ì˜ˆ: "ë°ì´í„° 0", "ë°ì´í„° 3", "ë°ì´í„° 7"ì´ ì£¼ì–´ì¡Œë‹¤ë©´ â†’ [SOURCE:0], [SOURCE:3], [SOURCE:7]ë§Œ ì‚¬ìš© ê°€ëŠ¥
    - **ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë²ˆí˜¸ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”** (ì˜ˆ: ë°ì´í„° 0~7ë§Œ ìˆëŠ”ë° [SOURCE:15] ì‚¬ìš© ê¸ˆì§€)
    - ë°˜ë“œì‹œ ìˆ«ìë§Œ ì‚¬ìš©í•˜ê³  "ë°ì´í„°", "ë¬¸ì„œ" ë“±ì˜ ë‹¨ì–´ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
    - ì—¬ëŸ¬ ì¶œì²˜ëŠ” ì‰¼í‘œì™€ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„: [SOURCE:1, 4, 8]
    - ë‹¨ì¼ ì¶œì²˜: [SOURCE:8]
    - SOURCE íƒœê·¸ëŠ” ì™„ì „í•œ ë¬¸ì¥ì´ ëë‚œ í›„ ë°”ë¡œ ë¶™ì—¬ì„œ ì‘ì„±í•˜ì„¸ìš”
    - SOURCE íƒœê·¸ ì•ë’¤ë¡œ ì¤„ë°”ê¿ˆí•˜ì§€ ë§ˆì„¸ìš” (ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ë¶„í•  ë°©ì§€)

    **ì˜¬ë°”ë¥¸ ì˜ˆì‹œ** (ë°ì´í„° 0, 3, 8ì´ ì£¼ì–´ì§„ ê²½ìš°):
    - "**ë§¤ì¶œì´ ì¦ê°€í–ˆìŠµë‹ˆë‹¤** [SOURCE:8]"
    - "ì‹œì¥ ì ìœ ìœ¨ì´ ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤ [SOURCE:3]"
    - "**ë§¤ì¶œì´ 5% ê°ì†Œí–ˆìŠµë‹ˆë‹¤** [SOURCE:0, 3, 8]"

    **ğŸš« ì ˆëŒ€ ê¸ˆì§€ë˜ëŠ” ì˜ëª»ëœ ì˜ˆì‹œ**:
    - [SOURCE:ë°ì´í„° 1], [SOURCE:ë¬¸ì„œ 1] (ë‹¨ì–´ í¬í•¨ ê¸ˆì§€)
    - [SOURCE:1,\n4, 8] (ì¤„ë°”ê¿ˆ ê¸ˆì§€)
    - [SOURCE: 1 , 4 , 8] (ë¶ˆí•„ìš”í•œ ê³µë°± ê¸ˆì§€)
    - [SOURCE:15] (ìœ„ ì°¸ê³  ë°ì´í„°ì— ì—†ëŠ” ë²ˆí˜¸ ì‚¬ìš© ê¸ˆì§€)

    **âš ï¸ ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸ (ë°˜ë“œì‹œ í™•ì¸)**:
    â–¡ **êµµì€ ê¸€ì”¨**ê°€ ë¬¸ë‹¨ë‹¹ 2ê°œ ì´ìƒ ì‚¬ìš©ë˜ì—ˆëŠ”ê°€?
    â–¡ *ê¸°ìš¸ì„ì²´*ê°€ ì ì ˆíˆ ì‚¬ìš©ë˜ì—ˆëŠ”ê°€?
    â–¡ 3ê°œ ì´ìƒ ë¹„êµ ì‹œ í…Œì´ë¸”ì„ ì‚¬ìš©í–ˆëŠ”ê°€?
    â–¡ ì¤‘ìš”í•œ ê²°ë¡ ì— `> **í•µì‹¬ ìš”ì•½**:` ë¸”ë¡ì¿¼íŠ¸ë¥¼ ì‚¬ìš©í–ˆëŠ”ê°€?
    â–¡ [SOURCE:ìˆ«ì] í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ë¥¼ ì •í™•íˆ í‘œê¸°í–ˆëŠ”ê°€?
    â–¡ ë‹¨ë½ ê°„ ê³µë°± ë¼ì¸ì´ ìˆëŠ”ê°€?

    **ğŸš« ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­**:
    âŒ "ì¶”ê°€ ì •ë³´ ìš”ì²­", "ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤", "êµ¬ì²´ì ì¸ ë°ì´í„° ë¶€ì¡±" ë“±ì˜ í‘œí˜„ ì‚¬ìš© ê¸ˆì§€
    âŒ "...ì— ëŒ€í•œ ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤" ê°™ì€ ë¯¸ì™„ì„± ê²°ë¡  ì œì‹œ ê¸ˆì§€
    âœ… **í˜„ì¬ í™•ë³´ëœ ë°ì´í„°ë¡œ ìµœëŒ€í•œ êµ¬ì²´ì ì´ê³  ì™„ì „í•œ ë¶„ì„ ë° ê²°ë¡  ì œì‹œ í•„ìˆ˜**
    âœ… ë¶€ì¡±í•œ ì •ë³´ê°€ ìˆì–´ë„ í˜„ì¬ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ìµœì„ ì˜ ì¸ì‚¬ì´íŠ¸ì™€ í‘œ ì œê³µ

    **â­ ì§€ê¸ˆ ë°”ë¡œ ìœ„ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì„¹ì…˜ì„ ì‘ì„±í•˜ì„¸ìš”:**

    **ë³´ê³ ì„œ ì„¹ì…˜ ë‚´ìš©**:
    """

            prompt = prompt_template.format(
                persona_instruction=persona_instruction,
                original_query=original_query,
                section_title=section_title,
                awareness_context=awareness_context,
                description=description,
                section_data_content=section_data_content
            )

        else:  # "full_data_for_chart"
            section_data_content = ""
            for actual_index in use_indexes:
                if actual_index in full_data_dict:
                    data_info = full_data_dict[actual_index]
                    section_data_content += f"**ë°ì´í„° {actual_index}: {data_info['source']}**\n"
                    section_data_content += f"- **ì œëª©**: {data_info['title']}\n"
                    section_data_content += f"- **ë‚´ìš©**: {data_info['content']}\n"
                    section_data_content += f"- **ì¶œì²˜_ë§í¬**: {data_info.get('url') or data_info.get('source_url', '')}\n\n"

            prompt_template = """
            {persona_instruction}

    ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ê°€ì´ì ë³´ê³ ì„œ ì‘ì„±ê°€ì…ë‹ˆë‹¤. ìœ„ì˜ í˜ë¥´ì†Œë‚˜ ì§€ì¹¨ì„ ë”°ë¼ì„œ, ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í…ìŠ¤íŠ¸ ì„¤ëª…ê³¼ ì‹œê°ì  ì°¨íŠ¸ë¥¼ ê²°í•©í•œ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ë³´ê³ ì„œ ì„¹ì…˜ì„ ì‘ì„±í•©ë‹ˆë‹¤.

    **ì‚¬ìš©ìì˜ ì „ì²´ ì§ˆë¬¸**: "{original_query}"

    ---
    **[ë§¤ìš° ì¤‘ìš”] ì „ì²´ ë³´ê³ ì„œ êµ¬ì¡° ë° ë‹¹ì‹ ì˜ ì—­í• **:
    ë‹¹ì‹ ì€ ì•„ë˜ êµ¬ì¡°ë¡œ êµ¬ì„±ëœ ì „ì²´ ë³´ê³ ì„œì—ì„œ **ì˜¤ì§ '{section_title}' ì„¹ì…˜ë§Œ**ì„ ì±…ì„ì§€ê³  ìˆìŠµë‹ˆë‹¤.
    ë‹¤ë¥¸ ì „ë¬¸ê°€ë“¤ì´ ë‚˜ë¨¸ì§€ ì„¹ì…˜ë“¤ì„ ë™ì‹œì— ì‘ì„±í•˜ê³  ìˆìœ¼ë¯€ë¡œ, **ë‹¤ë¥¸ ì„¹ì…˜ì˜ ì£¼ì œë¥¼ ì ˆëŒ€ ì¹¨ë²”í•˜ì§€ ë§ê³  ë‹¹ì‹ ì˜ ì—­í• ì—ë§Œ ì§‘ì¤‘í•˜ì„¸ìš”.**

    {awareness_context}
    ---

    **í˜„ì¬ ì‘ì„±í•  ì„¹ì…˜ ì œëª©**: "{section_title}"
    **ì„¹ì…˜ ëª©í‘œ**: "{description}"

    **ì°¸ê³  ë°ì´í„° (ì‹¤ì œ ì¸ë±ìŠ¤ ë²ˆí˜¸ í¬í•¨)**:
    {section_data_content}

    **ì‘ì„± ì§€ì¹¨ (ë§¤ìš° ì¤‘ìš”)**:
    1. **ì—­í•  ì¤€ìˆ˜**: ìœ„ 'ì „ì²´ ë³´ê³ ì„œ êµ¬ì¡°'ì™€ 'í•µì‹¬ ëª©í‘œ'ë¥¼ ë°˜ë“œì‹œ ì¸ì§€í•˜ê³ , '{section_title}'ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ë§Œ ê¹Šì´ ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”.
    2. **í˜ë¥´ì†Œë‚˜ ì—­í•  ìœ ì§€**: ë‹¹ì‹ ì˜ ì—­í• ê³¼ ë§íˆ¬, ë¶„ì„ ê´€ì ì„ ë°˜ë“œì‹œ ìœ ì§€í•˜ë©° ì‘ì„±í•˜ì„¸ìš”.
    3. **ê°„ê²°ì„± ìœ ì§€**: ë°˜ë“œì‹œ 1~2 ë¬¸ë‹¨ ì´ë‚´ë¡œ, ë°ì´í„°ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì¸ì‚¬ì´íŠ¸ì™€ ë¶„ì„ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”.
    4. **ì œëª© ë°˜ë³µ ê¸ˆì§€**: ì£¼ì–´ì§„ ì„¹ì…˜ ì œëª©ì„ ì ˆëŒ€ ë°˜ë³µí•´ì„œ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”. ë°”ë¡œ ë³¸ë¬¸ ë‚´ìš©ìœ¼ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.
    5. **ë°ì´í„° ê¸°ë°˜**: ì„¤ëª…ì— êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ì‚¬ì‹¤, í†µê³„ ìë£Œë¥¼ ì ê·¹ì ìœ¼ë¡œ ì¸ìš©í•˜ì—¬ ì‹ ë¢°ë„ë¥¼ ë†’ì´ì„¸ìš”.
    6. **ì°¨íŠ¸ ë§ˆì»¤ ì‚½ì…**: í…ìŠ¤íŠ¸ ì„¤ëª…ì˜ íë¦„ ìƒ, ì‹œê°ì  ë°ì´í„°ê°€ í•„ìš”í•œ ì ì ˆí•œ ìœ„ì¹˜ì— [GENERATE_CHART] ë§ˆì»¤ë¥¼ í•œ ì¤„ì— ë‹¨ë…ìœ¼ë¡œ ì‚½ì…í•˜ì„¸ìš”.
    7. **ì„œìˆ  ê³„ì†**: ë§ˆì»¤ë¥¼ ì‚½ì…í•œ í›„, ì´ì–´ì„œ ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ìì—°ìŠ¤ëŸ½ê²Œ ê³„ì† ì‘ì„±í•˜ì„¸ìš”.
    8. **ë…¸ì…˜ ìŠ¤íƒ€ì¼ ë§ˆí¬ë‹¤ìš´ ì ê·¹ í™œìš©**: êµµì€ ê¸€ì”¨, ê¸°ìš¸ì„ì²´, ì¸ìš©ë¬¸, ëª©ë¡, í…Œì´ë¸” ë“±ì„ ì ì ˆíˆ ì‚¬ìš©í•˜ì„¸ìš”.
    - **ì¸ìš©ë¬¸(>) ì‚¬ìš© ì‹œì **:
     - **í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë‚˜ ê²°ë¡ **: ì„¹ì…˜ì˜ ê°€ì¥ ì¤‘ìš”í•œ ë°œê²¬ì‚¬í•­ì´ë‚˜ ê²°ë¡ 
     - **ì£¼ìš” í†µê³„ë‚˜ ìˆ˜ì¹˜**: íŠ¹ë³„íˆ ê°•ì¡°í•´ì•¼ í•  ì¤‘ìš”í•œ ë°ì´í„°
     - **ì „ë¬¸ê°€ ì˜ê²¬ì´ë‚˜ ë¶„ì„**: í˜ë¥´ì†Œë‚˜ ê´€ì ì—ì„œì˜ í•µì‹¬ íŒë‹¨ì´ë‚˜ ê²¬í•´
     - **ê²½ê³ ë‚˜ ì£¼ì˜ì‚¬í•­**: ë…ìê°€ ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  ì¤‘ìš”í•œ ì •ë³´
     - **í•´ë‹¹ ì„¹ì…˜ì— ëŒ€í•œ ìš”ì•½** : í•´ë‹¹ ì„¹ì…˜ì— ëŒ€í•œ ìš”ì•½ì´ í•„ìš”í•  ë•Œ ì‚¬ìš©
     - **ì¸ìš©ë¬¸ ì‚¬ìš© ì˜ˆì‹œ**:
      - > 2024ë…„ 4ë¶„ê¸° ì‹œì¥ ê·œëª¨ê°€ ì „ë…„ ë™ê¸° ëŒ€ë¹„ 15% ê¸‰ê°í•˜ì—¬ ì¦‰ê°ì ì¸ ëŒ€ì‘ì´ í•„ìš”í•©ë‹ˆë‹¤.
      - > ë°ì´í„° ë¶„ì„ ê²°ê³¼, A ì „ëµì´ B ì „ëµ ëŒ€ë¹„ ROIê°€ 3ë°° ë†’ì€ ê²ƒìœ¼ë¡œ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.
    - **í‘œ í˜•íƒœ ë°ì´í„°**: ë¹„êµë‚˜ ë¶„ë¥˜ê°€ í•„ìš”í•œ ì •ë³´ëŠ” ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”ë¡œ êµ¬ì„±
    - 3ê°œ ì´ìƒì˜ í•­ëª©ì„ ë¹„êµí•˜ê±°ë‚˜ ë¶„ë¥˜í•  ë•Œ í…Œì´ë¸” ì‚¬ìš© ê¶Œì¥
    - í…Œì´ë¸” ì…€ ì•ˆì—ì„œë„ **êµµì€ ê¸€ì”¨**, *ê¸°ìš¸ì„ì²´*, [SOURCE:ìˆ«ì] ì¶œì²˜ í‘œê¸° ê°€ëŠ¥
    9. **ì¶œì²˜ í‘œê¸° (ë°ì´í„° ì¸ë±ìŠ¤ ë²ˆí˜¸ ì‚¬ìš©)**: íŠ¹ì • ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•œ ë¬¸ì¥ ë°”ë¡œ ë’¤ì— [SOURCE:ìˆ«ì1, ìˆ«ì2, ìˆ«ì3] í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ë¥¼ í‘œê¸°í•˜ì„¸ìš”.


    **ë§¤ìš° ì¤‘ìš” - SOURCE ë²ˆí˜¸ ì‚¬ìš© ê·œì¹™**:
    - **ë°˜ë“œì‹œ ìœ„ "ì°¸ê³  ë°ì´í„°"ì— ëª…ì‹œëœ "[ë°ì´í„° ì¸ë±ìŠ¤ X]" ì˜ X ë²ˆí˜¸ë§Œ ì‚¬ìš©í•˜ì„¸ìš”**
    - ì˜ˆ: "ë°ì´í„° 0", "ë°ì´í„° 3", "ë°ì´í„° 7"ì´ ì£¼ì–´ì¡Œë‹¤ë©´ â†’ [SOURCE:0], [SOURCE:3], [SOURCE:7]ë§Œ ì‚¬ìš© ê°€ëŠ¥
    - **ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë²ˆí˜¸ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”** (ì˜ˆ: ë°ì´í„° 0~7ë§Œ ìˆëŠ”ë° [SOURCE:15] ì‚¬ìš© ê¸ˆì§€)
    - ë°˜ë“œì‹œ ìˆ«ìë§Œ ì‚¬ìš©í•˜ê³  "ë°ì´í„°", "ë¬¸ì„œ" ë“±ì˜ ë‹¨ì–´ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”

    **ì˜¬ë°”ë¥¸ ì˜ˆì‹œ** (ë°ì´í„° 0, 3, 8ì´ ì£¼ì–´ì§„ ê²½ìš°):
    - "**ì‹œì¥ ê·œëª¨ê°€ 10% ì¦ê°€**í–ˆìŠµë‹ˆë‹¤. [SOURCE:8]"
    - "**ë§¤ì¶œì´ 5% ê°ì†Œ**í–ˆìŠµë‹ˆë‹¤. [SOURCE:0, 3, 8]"

    **ì ˆëŒ€ ê¸ˆì§€ë˜ëŠ” ì˜ëª»ëœ ì˜ˆì‹œ**:
    - [SOURCE:ë°ì´í„° 8], [SOURCE:ë¬¸ì„œ 8] (ë‹¨ì–´ í¬í•¨ ê¸ˆì§€)
    - [SOURCE:15] (ìœ„ ì°¸ê³  ë°ì´í„°ì— ì—†ëŠ” ë²ˆí˜¸ ì‚¬ìš© ê¸ˆì§€)

    **ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­**:
    "ì¶”ê°€ ì •ë³´ ìš”ì²­", "ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤", "êµ¬ì²´ì ì¸ ë°ì´í„° ë¶€ì¡±" ë“±ì˜ í‘œí˜„ ì‚¬ìš© ê¸ˆì§€
    "...ì— ëŒ€í•œ ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤" ê°™ì€ ë¯¸ì™„ì„± ê²°ë¡  ì œì‹œ ê¸ˆì§€
    "ë°ì´í„°ê°€ ì œí•œì ì…ë‹ˆë‹¤", "ì •í™•í•œ ëª©ë¡ ì‘ì„±ì„ ìœ„í•´ì„œëŠ”..." ë“±ì˜ í•œê³„ ì–¸ê¸‰ ê¸ˆì§€
    **í˜„ì¬ í™•ë³´ëœ ë°ì´í„°ë¡œ ìµœëŒ€í•œ êµ¬ì²´ì ì´ê³  ì™„ì „í•œ ë¶„ì„ ë° ê²°ë¡  ì œì‹œ í•„ìˆ˜**
    ë¶€ì¡±í•œ ì •ë³´ê°€ ìˆì–´ë„ í˜„ì¬ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ìµœì„ ì˜ ì¸ì‚¬ì´íŠ¸ì™€ í‘œ ì œê³µ
    í‘œ ìš”ì²­ì´ ìˆìœ¼ë©´ í˜„ì¬ ë°ì´í„°ë¡œ ê°€ëŠ¥í•œ í•œ ì™„ì „í•œ í‘œ ì‘ì„±

    **ë³´ê³ ì„œ ì„¹ì…˜ ë³¸ë¬¸**:
    """

            prompt = prompt_template.format(
                persona_instruction=persona_instruction,
                original_query=original_query,
                section_title=section_title,
                awareness_context=awareness_context,
                description=description,
                section_data_content=section_data_content
            )

        try:
            print(f"\n>> ì„¹ì…˜ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {section_title} (ì‚¬ìš© ì¸ë±ìŠ¤: {use_indexes})")
            total_content = ""
            chunk_count = 0
            valid_content_count = 0

            async for chunk in self._astream_with_fallback(
                prompt,
                self.llm_pro,
                self.llm_pro_backup,
                self.llm_openai_4o
            ):
                chunk_count += 1
                if hasattr(chunk, 'content') and chunk.content:
                    total_content += chunk.content
                    chunk_text = chunk.content
                    valid_content_count += 1

                    print(f"- ì›ë³¸ ì²­í¬ {chunk_count}: {len(chunk_text)} ë¬¸ì")

                    # 5ì ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ ì „ì†¡
                    for i in range(0, len(chunk_text), 5):
                        mini_chunk = chunk_text[i:i+5]
                        yield mini_chunk

            print(f"\n>> ì„¹ì…˜ ì™„ë£Œ: {section_title}, ì´ {chunk_count}ê°œ ì›ë³¸ ì²­í¬, {valid_content_count}ê°œ ìœ íš¨ ì²­í¬, {len(total_content)} ë¬¸ì")

            if not total_content.strip() or valid_content_count == 0:
                print(f"- ì„¹ì…˜ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜ ({section_title}): No generation chunks were returned")
                raise Exception("No generation chunks were returned")

        except Exception as e:
            print(f"- ì„¹ì…˜ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜ ({section_title}): {e}")
            if "No generation chunks" in str(e) or "no valid content" in str(e).lower():
                try:
                    print(f"- OpenAIë¡œ ì§ì ‘ ì¬ì‹œë„: {section_title}")
                    total_content = ""
                    chunk_count = 0

                    async for chunk in self.llm_openai_4o.astream(prompt):
                        chunk_count += 1
                        if hasattr(chunk, 'content') and chunk.content:
                            total_content += chunk.content
                            chunk_text = chunk.content
                            print(f"- OpenAI ì¬ì‹œë„ ì²­í¬ {chunk_count}: {len(chunk_text)} ë¬¸ì")

                            for i in range(0, len(chunk_text), 5):
                                mini_chunk = chunk_text[i:i+5]
                                yield mini_chunk

                    print(f"- OpenAI ì¬ì‹œë„ ì™„ë£Œ: {section_title}, {chunk_count}ê°œ ì²­í¬, {len(total_content)} ë¬¸ì")

                    if not total_content.strip():
                        print(f"- OpenAI ì¬ì‹œë„ë„ ì‹¤íŒ¨, fallback ë‚´ìš© ìƒì„±")
                        raise Exception("OpenAI retry also failed")

                except Exception as retry_error:
                    print(f"- OpenAI ì¬ì‹œë„ ì‹¤íŒ¨: {retry_error}")
                    fallback_content = f"*'{section_title}' ì„¹ì…˜ì— ëŒ€í•œ ìƒì„¸í•œ ë¶„ì„ì„ ìƒì„±í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.*\n\n"
                    yield fallback_content
            else:
                error_content = f"*'{section_title}' ì„¹ì…˜ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}*\n\n"
                yield error_content

    async def _create_charts(self, section_data: List[SearchResult], section_title: str, generated_content: str = "", description: str = "", yield_callback=None, state: Dict[str, Any] = None):
        """â­ ìˆ˜ì •: í˜ë¥´ì†Œë‚˜ ê´€ì ì„ ë°˜ì˜í•˜ì—¬ ì„¹ì…˜ë³„ ì„ íƒëœ ë°ì´í„°ì™€ ìƒì„±ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ ì°¨íŠ¸ ìƒì„±"""
        print(f"  - ì°¨íŠ¸ ë°ì´í„° ìƒì„±: '{section_title}' (ë°ì´í„° {len(section_data)}ê°œ)")
        if description:
            print(f"  - ì„¹ì…˜ ëª©í‘œ: '{description}'")

        # í˜„ì¬ ë‚ ì§œ ì •ë³´ ì¶”ê°€
        import pytz
        kst = pytz.timezone('Asia/Seoul')
        current_date_str = datetime.now(kst).strftime("%Yë…„ %mì›” %dì¼")

        # ì£¼ì–´ì§„ ë°ì´í„°ë¡œë§Œ ì°¨íŠ¸ ìƒì„± (ì¶”ê°€ ê²€ìƒ‰ ì—†ìŒ)

        # í˜ë¥´ì†Œë‚˜ ì •ë³´ ì¶”ì¶œ
        persona_name = state.get("persona", "ê¸°ë³¸") if state else "ê¸°ë³¸"
        default_chart_prompt = "ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¥ ëª…í™•í•˜ê³  ìœ ìš©í•œ Chart.js ì°¨íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."
        persona_chart_instruction = self.personas.get(persona_name, {}).get("chart_prompt", default_chart_prompt)

        print(f"  - ì°¨íŠ¸ ìƒì„±ì— '{persona_name}' í˜ë¥´ì†Œë‚˜ ê´€ì  ì ìš© (ì°¨íŠ¸ìš© í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)")


        # ì°¨íŠ¸ ìƒì„± context ì¶”ì¶œ
        chart_context = state.get("chart_context", {}) if state else {}
        previous_charts = chart_context.get("previous_charts", [])
        previous_sections = chart_context.get("previous_sections", [])

        async def _generate_chart_with_data(current_data: List[SearchResult], attempt: int = 1):
            """ì‹¤ì œ ì°¨íŠ¸ ìƒì„± ë¡œì§ (ì¬ì‹œë„ ê°€ëŠ¥)"""
            try:
                # ë°ì´í„° ìš”ì•½ ìƒì„±
                data_summary = ""
                for i, item in enumerate(current_data):
                    source = getattr(item, 'source', 'Unknown')
                    title = getattr(item, 'title', 'No Title')
                    content = getattr(item, 'content', '')[:]
                    data_summary += f"[{i}] [{source}] {title}\në‚´ìš©: {content}...\n\n"

                # ì§ì „ì— ìƒì„±ëœ ë³´ê³ ì„œ ë‚´ìš© ì¶”ê°€
                context_info = ""
                if generated_content:
                    content_preview = generated_content[:] if generated_content else ""
                    context_info = f"\n**ì§ì „ì— ìƒì„±ëœ ë³´ê³ ì„œ ë‚´ìš© (ì°¨íŠ¸ì™€ ì¼ë§¥ìƒí†µí•´ì•¼ í•¨)**:\n{content_preview}\n"

                # ì´ì „ ì°¨íŠ¸ ì •ë³´ ì¶”ê°€
                if previous_charts:
                    context_info += "\n**ì´ì „ ì„¹ì…˜ì—ì„œ ìƒì„±ëœ ì°¨íŠ¸ë“¤ (ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì°¸ê³ )**:\n"
                    for prev_chart in previous_charts[-2:]:  # ìµœê·¼ 2ê°œë§Œ
                        chart_section = prev_chart.get("section", "")
                        chart_type = prev_chart.get("chart", {}).get("type", "")
                        chart_labels = prev_chart.get("chart", {}).get("data", {}).get("labels", [])[:]
                        context_info += f"- {chart_section}: {chart_type} ì°¨íŠ¸ (í•­ëª©: {', '.join(map(str, chart_labels))}...)\n"
                    context_info += "\n"

                chart_prompt = f"""
        ë‹¹ì‹ ì€ ë°ì´í„°ì˜ **ê´€ë ¨ì„±**ì„ íŒë‹¨í•˜ê³  **ì˜ë¯¸ ìˆëŠ” ì‹œê°í™”**ë¥¼ ë§Œë“œëŠ” ë°ì´í„° ì‹œê°í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

        ---

        **[PART 1: ì…ë ¥ ì •ë³´]**
        ë‹¹ì‹ ì´ ë¶„ì„í•´ì•¼ í•  ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

        1.  **ìƒì„± ëª©í‘œ:** '{section_title}'ì— ëŒ€í•œ ì°¨íŠ¸ ìƒì„±
        1.1. **ì„¹ì…˜ ëª©í‘œ:** '{description}'
        2.  **í˜„ì¬ ë‚ ì§œ:** {current_date_str}
        3.  **í˜ë¥´ì†Œë‚˜:** '{persona_name}' (ì‹œë„: {attempt}íšŒì°¨)
        4.  **ì°¸ê³  ì»¨í…ìŠ¤íŠ¸:**
            - ì´ì „ì— ìƒì„±ëœ í…ìŠ¤íŠ¸: {context_info}
            - ì´ì „ì— ìƒì„±ëœ ì°¨íŠ¸: (ì¤‘ë³µ ë°©ì§€ìš©)
        5.  **ë¶„ì„í•  ì›ë³¸ ë°ì´í„°:**
        {data_summary}

        ---

        **[PART 2: ìˆ˜í–‰í•  ëª…ë ¹]**
        ì´ì œ, ë‹¤ìŒ 3ë‹¨ê³„ ì‚¬ê³  í”„ë¡œì„¸ìŠ¤ì— ë”°ë¼ ëª…ë ¹ì„ **ë°˜ë“œì‹œ ìˆœì„œëŒ€ë¡œ** ìˆ˜í–‰í•˜ì„¸ìš”.

        **STEP 1: ìƒì„¸ ë°ì´í„° ë¶„ì„ ë° ì°¨íŠ¸ ì•„ì´ë””ì–´ êµ¬ìƒ**
        - `[PART 1]`ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹œê°í™”í•  ë°ì´í„°(í•­ëª©, ìˆ˜ì¹˜, ë¼ë²¨)ë¥¼ ìµœëŒ€í•œ ì¶”ì¶œí•˜ê³ , ë§Œë“¤ ìˆ˜ ìˆëŠ” ì°¨íŠ¸ì˜ ì£¼ì œë¥¼ êµ¬ì²´í™”í•©ë‹ˆë‹¤.
        - ì´ ê³¼ì •ì€ ì•„ë˜ **`ë¶„ì„ í…œí”Œë¦¿`**ì„ ì±„ìš°ëŠ” ë°©ì‹ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.

        **STEP 2: ì£¼ì œ ì¼ê´€ì„± ê²€ì¦**
        - **ì§ˆë¬¸:** "STEP 1ì—ì„œ êµ¬ìƒí•œ ì°¨íŠ¸ ì£¼ì œê°€ `[PART 1]`ì˜ **ìƒì„± ëª©í‘œ**ì¸ '{section_title}'ë° **ì„¹ì…˜ ëª©í‘œ**ì¸ '{description}'ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆìŠµë‹ˆê¹Œ?"
        - ì´ ì§ˆë¬¸ì— ëŒ€í•´ **"ë‹µë³€ (Yes/No)"**ê³¼ **"ê·¼ê±°"**ë¥¼ ëª…í™•íˆ ê²°ì •í•©ë‹ˆë‹¤.

        **STEP 3: ì¡°ê±´ë¶€ JSON ì¶œë ¥**
        - **ë§Œì•½ STEP 2ì˜ ë‹µë³€ì´ 'No'ë¼ë©´:** **`PLACEHOLDER_JSON`**ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
        - **ë§Œì•½ STEP 2ì˜ ë‹µë³€ì´ 'Yes'ë¼ë©´:** **`ì‹¤ì œ ì°¨íŠ¸ JSON`**ì„ ìƒì„±í•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤.

        ---

        **[PART 3: ì¶œë ¥ í˜•ì‹ ë° ê°€ì´ë“œ]**
        ìµœì¢… ê²°ê³¼ë¬¼ì€ ì•„ë˜ ê°€ì´ë“œì— ë”°ë¼ **'ë¶„ì„'**ê³¼ **'JSON'** ë‘ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

        **1. ë¶„ì„ (Analysis Block):**
        - STEP 1ê³¼ STEP 2ì—ì„œ ë‹¹ì‹ ì´ ìƒê°í•œ ê³¼ì •ì„ ì•„ë˜ `ë¶„ì„ í…œí”Œë¦¿`ì— ë§ì¶° ì‘ì„±í•©ë‹ˆë‹¤.
        - Vector DB ë°ì´í„°ê°€ ë¶ˆë¶„ëª…í•  ìˆ˜ ìˆìœ¼ë‹ˆ, ì„¹ì…˜ ì œëª©ê³¼ ë°ì´í„° ë‚´ìš©ì„ ì¢…í•©ì ìœ¼ë¡œ ì¶”ë¡ í•´ì•¼ í•©ë‹ˆë‹¤.

        ë¶„ì„:
        1. ìƒì„¸ ë¶„ì„ ë° ì•„ì´ë””ì–´ (STEP 1):
            (1) ì»¨í…ìŠ¤íŠ¸ ì¶”ë¡ :
                - ì„¹ì…˜ ì œëª© "{section_title}"ê³¼ **ì„¹ì…˜ ëª©í‘œ "{description}"**ì„ ë³´ê³  ì´ ë°ì´í„°ê°€ ë¬´ì—‡ì— ê´€í•œ í†µê³„ì¸ì§€ ì¶”ë¡ 
                - ë‹¨ìœ„ ì •ë³´ (ì›/20ê°œ, ì›/50ê°œ ë“±)ë¥¼ ë³´ê³  ì–´ë–¤ ìƒí’ˆ/í’ˆëª©ì¸ì§€ ì¶”ë¡ 
                - ì¶œì²˜ URLì´ë‚˜ ë¬¸ì„œëª…ì—ì„œ ì¶”ê°€ íŒíŠ¸ ì°¾ê¸°
                - ì¶”ë¡ ëœ í’ˆëª©/ìƒí’ˆ: [ì˜ˆ: ê³„ë€, ë°°ì¶”, ìŒ€ ë“±]

            (2) ì„¹ì…˜ ëª©ì : [ì´ ì„¹ì…˜ì´ ë³´ì—¬ì£¼ë ¤ëŠ” í•µì‹¬ ë‚´ìš©, **ì„¹ì…˜ ëª©í‘œ**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±]

            (3) ì¶”ì¶œ ê°€ëŠ¥í•œ ìˆ˜ì¹˜ ë°ì´í„°:
                - í•­ëª©A: [ì •í™•í•œ ìˆ˜ì¹˜] + [ì¶”ë¡ ëœ ì˜ë¯¸] (ì¶œì²˜: ë°ì´í„° ì¸ë±ìŠ¤ X)
                - í•­ëª©B: [ì •í™•í•œ ìˆ˜ì¹˜] + [ì¶”ë¡ ëœ ì˜ë¯¸] (ì¶œì²˜: ë°ì´í„° ì¸ë±ìŠ¤ Y)
                - í•­ëª©C: [ì •í™•í•œ ìˆ˜ì¹˜] + [ì¶”ë¡ ëœ ì˜ë¯¸] (ì¶œì²˜: ë°ì´í„° ì¸ë±ìŠ¤ Z)

            (4) ì¶”ì¶œëœ ë¼ë²¨/ì¹´í…Œê³ ë¦¬: [ì‹¤ì œ ì—°ë„, ì›”, ì§€ì—­ëª… ë“±]

            (5) ìµœì  ì°¨íŠ¸ íƒ€ì…: [ì•„ë˜ ì§€ì› ì°¨íŠ¸ íƒ€ì…ì—ì„œë§Œ ì„ íƒ]

            (6) ì°¨íŠ¸ êµ¬ì„±:
                - Xì¶•: [ì¶”ë¡ ëœ í’ˆëª©ëª…ì˜ ì‹œê°„/ì§€ì—­/ì¹´í…Œê³ ë¦¬]
                - Yì¶•: [ì¶”ë¡ ëœ í’ˆëª©ëª…ì˜ ê°€ê²©/ìˆ˜ëŸ‰ + ë‹¨ìœ„]
                - ë°ì´í„°ì…‹: [ì˜ë¯¸ìˆëŠ” ë¼ë²¨ë“¤ê³¼ í•´ë‹¹ ìˆ˜ì¹˜ë“¤]
                - **ë²”ë¡€ ì „ëµ**: ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ ë¹„êµì‹œ ê°ê°ì„ ë³„ë„ ë°ì´í„°ì…‹ìœ¼ë¡œ êµ¬ì„±!

        2. ì£¼ì œ ì¼ê´€ì„± ê²€ì¦ (STEP 2):
            - ë‹µë³€: [Yes ë˜ëŠ” No]
            - ê·¼ê±°: [ì˜ˆ: 'ë¯¸ìƒë¬¼ ë°œíš¨ R&D'ì™€ 'í˜¸ì£¼ ê¸°ì—… ì„¤ë¦½ ì—°ë„'ëŠ” ì „í˜€ ë‹¤ë¥¸ ì£¼ì œì´ë¯€ë¡œ No. ì„¹ì…˜ ëª©í‘œëŠ” R&D ë™í–¥ ë¶„ì„ì¸ë°, ê¸°ì—… ì„¤ë¦½ ì—°ë„ëŠ” ê´€ë ¨ì„±ì´ ë‚®ìŒ.]

            **ì¤‘ìš”**: ê´€ë ¨ì„±ì´ ë‚®ì•„ë„ ë°ì´í„°ì—ì„œ ìˆ«ìë‚˜ í•­ëª©ì„ ì¶”ì¶œí•  ìˆ˜ ìˆë‹¤ë©´ "ë¶€ë¶„ì  Yes"ë¡œ ì²˜ë¦¬í•˜ì—¬ ì°¨íŠ¸ ìƒì„± ì‹œë„!


        **2. JSON (JSON Block):**
        - ìœ„ 'ë¶„ì„' ë¸”ë¡ ë°”ë¡œ ë‹¤ìŒì—, STEP 3ì˜ ê·œì¹™ì— ë”°ë¼ ì•„ë˜ ë‘ JSON ì¤‘ í•˜ë‚˜ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

        **[PLACEHOLDER_JSON]**
        ```json
        {{
            "type": "placeholder",
            "labels": ["'{section_title}' ê´€ë ¨ ë°ì´í„° ë¶€ì¡±"],
            "datasets": [{{"label": "ë°ì´í„° ë¶„ì„ ë¶ˆê°€", "data": [0], "backgroundColor": ["#FFB1C1"]}}],
            "title": "ê´€ë ¨ ë°ì´í„° ë¶€ì¡±",
            "palette": "modern",
            "options": {{"responsive": true, "plugins": {{"title": {{"display": true, "text": "í˜„ì¬ ì„¹ì…˜ ì£¼ì œì™€ ì¼ì¹˜í•˜ëŠ” ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ì°¨íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}}}}}}
        }}
        ```

        **[ì‹¤ì œ ì°¨íŠ¸ JSON ìƒì„± ê°€ì´ë“œ]**

        **ì¤‘ìš”: ì§€ì›ë˜ëŠ” ì°¨íŠ¸ íƒ€ì…ë§Œ ì‚¬ìš©í•˜ì„¸ìš”!**
        **ê¸°ë³¸ ì§€ì› ì°¨íŠ¸ íƒ€ì…**:
        - line, bar, pie, doughnut, radar, polararea, scatter, bubble

        **í™•ì¥ ì§€ì› ì°¨íŠ¸ íƒ€ì…**:
        - area (Line ì»´í¬ë„ŒíŠ¸ + fill ì˜µì…˜)
        - column (Bar ì»´í¬ë„ŒíŠ¸)
        - donut (Doughnut ì»´í¬ë„ŒíŠ¸)
        - polar (PolarArea ì»´í¬ë„ŒíŠ¸)
        - horizontalbar (Bar ì»´í¬ë„ŒíŠ¸)
        - stacked (Bar ì»´í¬ë„ŒíŠ¸ + stack ì˜µì…˜)
        - mixed (Line ì»´í¬ë„ŒíŠ¸)
        - funnel (Bar ì»´í¬ë„ŒíŠ¸)
        - waterfall (Bar ì»´í¬ë„ŒíŠ¸)
        - gauge (Doughnut ì»´í¬ë„ŒíŠ¸)
        - timeseries (Line ì»´í¬ë„ŒíŠ¸)
        - timeline (Line ì»´í¬ë„ŒíŠ¸)
        - gantt (Bar ì»´í¬ë„ŒíŠ¸)
        - multiline (Line ì»´í¬ë„ŒíŠ¸)
        - groupedbar (Bar ì»´í¬ë„ŒíŠ¸)
        - stackedarea (Line ì»´í¬ë„ŒíŠ¸)
        - combo (Line ì»´í¬ë„ŒíŠ¸)
        - heatmap (Bar ì»´í¬ë„ŒíŠ¸)
        - treemap (Bar ì»´í¬ë„ŒíŠ¸)
        - sankey (Bar ì»´í¬ë„ŒíŠ¸)
        - candlestick (Line ì»´í¬ë„ŒíŠ¸)
        - violin (Bar ì»´í¬ë„ŒíŠ¸)
        - boxplot (Bar ì»´í¬ë„ŒíŠ¸)

        **ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì…ë“¤ (ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€)**:
        - groupedbarchart (ì˜¬ë°”ë¥¸ íƒ€ì…: groupedbar)
        - ê¸°íƒ€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” íƒ€ì…ëª…ë“¤

        **ì„ íƒ ê¸°ì¤€**:
        - ì¹´í…Œê³ ë¦¬ë³„ ë¹„êµ â†’ **bar**, **column**, **horizontalbar**
        - ê·¸ë£¹í™”ëœ ë¹„êµ â†’ **groupedbar**, **stacked**
        - ì‹œê°„ ë³€í™” â†’ **line**, **timeseries**, **timeline**, **area**
        - ë¹„ìœ¨/êµ¬ì„± â†’ **pie**, **doughnut**, **donut**
        - ë‹¤ì°¨ì› ë¹„êµ â†’ **radar**, **polar**
        - ê´€ê³„ ë¶„ì„ â†’ **scatter**, **bubble**
        - ë³µí•© ì°¨íŠ¸ â†’ **mixed**, **combo**
        - íŠ¹ìˆ˜ ëª©ì  â†’ **funnel**, **waterfall**, **gauge**, **heatmap**

        **ì˜¬ë°”ë¥¸ ìƒ‰ìƒ ë° ë¼ë²¨ë§ ê·œì¹™ - ë²”ë¡€ ë¬¸ì œ í•´ê²°**:

        **ì¤‘ìš”: ì¹´í…Œê³ ë¦¬ë³„ ë¹„êµ ì‹œ ë°˜ë“œì‹œ ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì‚¬ìš©!**

        **ì˜ëª»ëœ ë°©ë²• (ë²”ë¡€ 1ê°œë§Œ ë‚˜ì˜´)**:
        ```json
        {{
            "type": "bar",
            "data": {{
                "labels": ["ë¯¸êµ­", "ì¼ë³¸"],
                "datasets": [{{
                    "label": "2023ë…„ ìŠ¤ë‚µ ì‹œì¥ ê·œëª¨",
                    "data": [71000, 11330],
                    "backgroundColor": ["#4F46E5", "#7C3AED"]
                }}]
            }}
        }}
        ```

        **ì˜¬ë°”ë¥¸ ë°©ë²• (ê° ì¹´í…Œê³ ë¦¬ë³„ ë²”ë¡€ í‘œì‹œ)**:
        ```json
        {{
            "type": "bar",
            "data": {{
                "labels": ["ì‹œì¥ ê·œëª¨"],
                "datasets": [
                    {{
                        "label": "ë¯¸êµ­",
                        "data": [71000],
                        "backgroundColor": "#4F46E5"
                    }},
                    {{
                        "label": "ì¼ë³¸",
                        "data": [11330],
                        "backgroundColor": "#7C3AED"
                    }}
                ]
            }}
        }}
        ```

        **ë°ì´í„° êµ¬ì¡° ë³€í™˜ ê·œì¹™**:

        **ì§€ì—­/êµ­ê°€ë³„ ë¹„êµ â†’ ê°ê°ì„ ë³„ë„ ë°ì´í„°ì…‹ìœ¼ë¡œ**
        **ì‹œê³„ì—´ ë¹„êµ â†’ ê° ì‹œì ì„ ë³„ë„ ë°ì´í„°ì…‹ìœ¼ë¡œ**
        **ì œí’ˆ/ì¹´í…Œê³ ë¦¬ ë¹„êµ â†’ ê°ê°ì„ ë³„ë„ ë°ì´í„°ì…‹ìœ¼ë¡œ**

        **ë‹¤ì–‘í•œ ë¹„êµ ì¼€ì´ìŠ¤ë³„ ì˜¬ë°”ë¥¸ êµ¬ì¡°**:

        **1. ì§€ì—­ë³„ ë¹„êµ**:
        ```json
        {{
            "type": "bar",
            "data": {{
                "labels": ["ì‹œì¥ ê·œëª¨"],
                "datasets": [
                    {{"label": "ë¯¸êµ­", "data": [ê°’1], "backgroundColor": "#4F46E5"}},
                    {{"label": "ì¤‘êµ­", "data": [ê°’2], "backgroundColor": "#7C3AED"}},
                    {{"label": "ì¼ë³¸", "data": [ê°’3], "backgroundColor": "#EC4899"}}
                ]
            }}
        }}
        ```

        **2. ì—°ë„ë³„ ë¹„êµ**:
        ```json
        {{
            "type": "bar",
            "data": {{
                "labels": ["ì„±ì¥ë¥ "],
                "datasets": [
                    {{"label": "2022ë…„", "data": [ê°’1], "backgroundColor": "#4F46E5"}},
                    {{"label": "2023ë…„", "data": [ê°’2], "backgroundColor": "#7C3AED"}},
                    {{"label": "2024ë…„", "data": [ê°’3], "backgroundColor": "#EC4899"}}
                ]
            }}
        }}
        ```

        **3. ì œí’ˆë³„ ë¹„êµ**:
        ```json
        {{
            "type": "bar",
            "data": {{
                "labels": ["ë§¤ì¶œ"],
                "datasets": [
                    {{"label": "ì œí’ˆA", "data": [ê°’1], "backgroundColor": "#4F46E5"}},
                    {{"label": "ì œí’ˆB", "data": [ê°’2], "backgroundColor": "#7C3AED"}},
                    {{"label": "ì œí’ˆC", "data": [ê°’3], "backgroundColor": "#EC4899"}}
                ]
            }}
        }}
        ```

        **4. ë‹¨ì¼ í•­ëª©ì˜ ì‹œê°„ ë³€í™” (ì´ ê²½ìš°ë§Œ ë‹¨ì¼ ë°ì´í„°ì…‹)**:
        ```json
        {{
            "type": "line",
            "data": {{
                "labels": ["2021", "2022", "2023", "2024"],
                "datasets": [{{
                    "label": "ì„±ì¥ ì¶”ì„¸",
                    "data": [100, 120, 150, 180],
                    "borderColor": "#4F46E5",
                    "backgroundColor": "#4F46E520"
                }}]
            }}
        }}
        ```

        **ì°¨íŠ¸ íƒ€ì…ë³„ ë°ì´í„° êµ¬ì¡° ê°€ì´ë“œ**:

        **stacked ì°¨íŠ¸**:
        ```json
        {{
            "type": "stacked",
            "data": {{
                "labels": ["Q1", "Q2", "Q3"],
                "datasets": [
                    {{"label": "ì œí’ˆA", "data": [10, 20, 30], "backgroundColor": "#4F46E5"}},
                    {{"label": "ì œí’ˆB", "data": [15, 25, 35], "backgroundColor": "#EC4899"}}
                ]
            }}
        }}
        ```

        **timeseries/timeline ì°¨íŠ¸**:
        ```json
        {{
            "type": "timeseries",
            "data": {{
                "labels": ["2023-01", "2023-02", "2023-03"],
                "datasets": [{{
                    "label": "ì›”ë³„ ë°ì´í„°",
                    "data": [100, 150, 200],
                    "borderColor": "#4F46E5",
                    "backgroundColor": "#4F46E520"
                }}]
            }}
        }}
        ```

        **ì¶”ì²œ ìƒ‰ìƒ íŒ”ë ˆíŠ¸**:
        - ë©”ì¸: ["#4F46E5", "#7C3AED", "#EC4899", "#EF4444", "#F59E0B"]
        - ë³´ì¡°: ["#06B6D4", "#10B981", "#84CC16", "#F97316", "#8B5CF6"]

        **ì°¨íŠ¸ ìƒì„± ìš°ì„  ì›ì¹™ - PLACEHOLDER ê¸ˆì§€!**:

        **1ë‹¨ê³„: ì§ì ‘ ìˆ˜ì¹˜ ë°ì´í„° í™œìš©**
        - ë¬¸ì„œì—ì„œ ê¸ˆì•¡, ê°œìˆ˜, ë¹„ìœ¨, ì ìˆ˜ ë“± êµ¬ì²´ì  ìˆ«ìê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì°¨íŠ¸í™”
        - ì˜ˆ: "2023ë…„ 113ì–µ 3,070ë§Œ ë‹¬ëŸ¬" â†’ bar ì°¨íŠ¸ë¡œ ì—°ë„ë³„ ìˆ˜ì¹˜

        **2ë‹¨ê³„: ì¹´í…Œê³ ë¦¬/í•­ëª© ê°œìˆ˜ ì°¨íŠ¸í™”**
        - ì—¬ëŸ¬ í•­ëª©ì´ ë‚˜ì—´ë˜ì–´ ìˆìœ¼ë©´ í•­ëª©ë³„ ê°œìˆ˜/ë¹ˆë„ë¡œ ì°¨íŠ¸ ìƒì„±
        - ì˜ˆ: "ì¼ë³¸ ìŠ¤ë‚µ, ì €ì—¼ ìŠ¤ë‚µ, ì„¸ì´ë²„ë¦¬ ë¹„ìŠ¤í‚·, íŒì½˜, í”„ë ˆì²¼" â†’ 5ê°œ í•­ëª© bar ì°¨íŠ¸

        **3ë‹¨ê³„: ì§€ì—­/ì‹œê°„ ì •ë³´ í™œìš©**
        - ì§€ì—­ëª… ì–¸ê¸‰ â†’ ì§€ì—­ë³„ ë¶„í¬ ì°¨íŠ¸
        - ì—°ë„/ë‚ ì§œ ì–¸ê¸‰ â†’ ì‹œê³„ì—´ ì°¨íŠ¸
        - ì˜ˆ: "ë¯¸êµ­, ì¤‘êµ­, ë™ë‚¨ì•„" ì–¸ê¸‰ â†’ 3ê°œ ì§€ì—­ pie ì°¨íŠ¸

        **4ë‹¨ê³„: ì¶”ë¡  ê°€ëŠ¥í•œ ë°ì´í„° ìƒì„±**
        - ì‹œì¥ ê·œëª¨ ì–¸ê¸‰ â†’ ìƒëŒ€ì  í¬ê¸° ë¹„êµ ì°¨íŠ¸
        - ì„±ì¥ë¥  ì–¸ê¸‰ â†’ ì¦ê°€ ì¶”ì„¸ ë¼ì¸ ì°¨íŠ¸
        - ì ìœ ìœ¨ ì–¸ê¸‰ â†’ íŒŒì´ ì°¨íŠ¸

        **5ë‹¨ê³„: ë©”íƒ€ ì •ë³´ ì°¨íŠ¸í™”**
        - ë¬¸ì„œ ê°œìˆ˜, ì–¸ê¸‰ íšŸìˆ˜, í‚¤ì›Œë“œ ë¹ˆë„
        - ì˜ˆ: "3ê°œ ë¬¸ì„œì—ì„œ ì–¸ê¸‰" â†’ ë¬¸ì„œë³„ ì–¸ê¸‰ íšŸìˆ˜ ì°¨íŠ¸

        **ì°½ì¡°ì  ì°¨íŠ¸ ìƒì„± ì˜ˆì‹œ**:
        ```
        - "ì¼ë³¸, ë¯¸êµ­, ë™ë‚¨ì•„ì‹œì•„ ì‹œì¥" â†’ {{"labels": ["ì¼ë³¸", "ë¯¸êµ­", "ë™ë‚¨ì•„"], "data": [1, 1, 1]}}
        - "2019ë…„ ì´í›„ ì—°í‰ê·  3.5% ê°ì†Œ" â†’ {{"labels": ["2019", "2020", "2021", "2022"], "data": [100, 96.5, 93.2, 90.0]}}
        - "5ê°€ì§€ íŠ¸ë Œë“œ ì–¸ê¸‰" â†’ {{"labels": ["íŠ¸ë Œë“œ1", "íŠ¸ë Œë“œ2", "íŠ¸ë Œë“œ3", "íŠ¸ë Œë“œ4", "íŠ¸ë Œë“œ5"], "data": [1, 1, 1, 1, 1]}}
        ```

        **PLACEHOLDERëŠ” ë‹¤ìŒ ê²½ìš°ì—ë§Œ (ë§¤ìš° ì˜ˆì™¸ì )**:
        - ì„¹ì…˜ ì œëª©ê³¼ ë°ì´í„°ê°€ ì™„ì „íˆ ë¬´ê´€í•œ ê²½ìš°ë§Œ (ì˜ˆ: "ìë™ì°¨ ì‚°ì—…" ì„¹ì…˜ì¸ë° ë°ì´í„°ê°€ "ìš”ë¦¬ ë ˆì‹œí”¼"ì¸ ê²½ìš°)
        - ë°ì´í„°ê°€ ì™„ì „íˆ ë¹„ì–´ìˆê±°ë‚˜ ì˜ë¯¸ì—†ëŠ” í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš°

        **ë°ì´í„° ì¶”ì¶œ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
        ìˆ«ìê°€ í•˜ë‚˜ë¼ë„ ìˆëŠ”ê°€? â†’ ì°¨íŠ¸ ìƒì„±!
        í•­ëª©ì´ 2ê°œ ì´ìƒ ë‚˜ì—´ë˜ì–´ ìˆëŠ”ê°€? â†’ ì°¨íŠ¸ ìƒì„±!
        ì§€ì—­/êµ­ê°€ëª…ì´ ì–¸ê¸‰ë˜ëŠ”ê°€? â†’ ì°¨íŠ¸ ìƒì„±!
        ì—°ë„/ì‹œê¸°ê°€ ì–¸ê¸‰ë˜ëŠ”ê°€? â†’ ì°¨íŠ¸ ìƒì„±!
        ë¹„êµ í‘œí˜„ì´ ìˆëŠ”ê°€? (ë” í¬ë‹¤, ì¦ê°€, ê°ì†Œ ë“±) â†’ ì°¨íŠ¸ ìƒì„±!

        **í‘œì¤€ JSON í˜•ì‹**:
        {{
            "type": "STEP1ë¶„ì„_ê¸°ë°˜_ì°¨íŠ¸íƒ€ì…",
            "data": {{
                "labels": ["STEP1ì¶”ì¶œ_ì‹¤ì œë¼ë²¨1", "ì‹¤ì œë¼ë²¨2", "ì‹¤ì œë¼ë²¨3"],
                "datasets": [{{
                    "label": "STEP1ì •ì˜_ë°ì´í„°ì…‹ëª…",
                    "data": [STEP1ì¶”ì¶œ_ì‹¤ì œìˆ˜ì¹˜1, ì‹¤ì œìˆ˜ì¹˜2, ì‹¤ì œìˆ˜ì¹˜3],
                    "backgroundColor": ["#4F46E5", "#7C3AED", "#EC4899", "#EF4444", "#F59E0B"],
                    "borderColor": ["#4F46E5", "#7C3AED", "#EC4899", "#EF4444", "#F59E0B"],
                    "borderWidth": 1
                }}]
            }},
            "options": {{
                "responsive": true,
                "plugins": {{
                    "title": {{
                        "display": true,
                        "text": "{section_title}"
                    }},
                    "legend": {{
                        "display": true,
                        "position": "top"
                    }}
                }},
                "scales": {{
                    "y": {{
                        "beginAtZero": true,
                        "title": {{
                            "display": true,
                            "text": "ê°’ (ë‹¨ìœ„)"
                        }}
                    }},
                    "x": {{
                        "title": {{
                            "display": true,
                            "text": "ì¹´í…Œê³ ë¦¬"
                        }}
                    }}
                }}
            }}
        }}
        """

                response = await self._invoke_with_fallback(
                    chart_prompt,
                    self.llm_flash,
                    self.llm_flash_backup,
                    self.llm_openai_mini
                )
                response_text = response.content.strip()

                # JavaScript í•¨ìˆ˜ ì œê±° í—¬í¼ í•¨ìˆ˜ ì •ì˜
                def clean_js_functions(json_str):
                    """JSONì—ì„œ JavaScript í•¨ìˆ˜ ì½”ë“œë¥¼ ë” ê²¬ê³ í•˜ê²Œ ì œê±°"""
                    import re

                    # 1. ê°€ì¥ ì•ˆì „í•œ ë°©ë²•: callbacks ê°ì²´ ì „ì²´ë¥¼ ë¹ˆ ê°ì²´ë¡œ êµì²´
                    # ì¤‘ì²©ëœ ì¤‘ê´„í˜¸ê¹Œì§€ ëª¨ë‘ í¬í•¨í•˜ì—¬ ì œê±°
                    def find_and_replace_callbacks(text):
                        """callbacks ê°ì²´ë¥¼ ì°¾ì•„ì„œ ë¹ˆ ê°ì²´ë¡œ êµì²´"""
                        pattern = r'"callbacks"\s*:\s*\{'
                        match = re.search(pattern, text)
                        if not match:
                            return text

                        start_pos = match.start()
                        brace_pos = text.find('{', match.end() - 1)

                        if brace_pos == -1:
                            return text

                        # ì¤‘ê´„í˜¸ ê· í˜• ë§ì¶”ê¸°
                        brace_count = 1
                        i = brace_pos + 1

                        while i < len(text) and brace_count > 0:
                            if text[i] == '{':
                                brace_count += 1
                            elif text[i] == '}':
                                brace_count -= 1
                            i += 1

                        if brace_count == 0:
                            # callbacks ê°ì²´ ì „ì²´ë¥¼ ë¹ˆ ê°ì²´ë¡œ êµì²´
                            before = text[:start_pos]
                            after = text[i:]
                            return before + '"callbacks": {}' + after

                        return text

                    # 2. callbacks ê°ì²´ êµì²´
                    json_str = find_and_replace_callbacks(json_str)

                    # 3. ë‚¨ì€ function íŒ¨í„´ë“¤ ì œê±°
                    # function(...) { ... } íŒ¨í„´ì„ nullë¡œ êµì²´
                    json_str = re.sub(r'function\s*\([^)]*\)\s*\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', 'null', json_str, flags=re.DOTALL)

                    # 4. í‚¤-ê°’ì—ì„œ functionìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê°’ë“¤ ì œê±°
                    json_str = re.sub(r':\s*function\s*\([^)]*\)\s*\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', ': null', json_str, flags=re.DOTALL)

                    # 5. ì˜ëª»ëœ JSON êµ¬ì¡° ì •ë¦¬
                    # null ë’¤ì— ì˜¤ëŠ” ì˜ëª»ëœ ì½”ë“œ íŒ¨í„´ ì œê±°
                    json_str = re.sub(r'null\s+[^,}\]]*(?:if|let|var|return|context)[^,}\]]*', 'null', json_str, flags=re.DOTALL)

                    # 6. ì—¬ëŸ¬ ì¤„ì— ê±¸ì¹œ í•¨ìˆ˜ ë³¸ë¬¸ ì •ë¦¬
                    json_str = re.sub(r'null\s*\n\s*if\s*\([^)]*\)\s*\{[^}]*\}\s*return[^;}]*;?\s*\}?', 'null', json_str, flags=re.DOTALL)

                    # 7. ë‚¨ì€ JavaScript ì½”ë“œ ì¡°ê°ë“¤ ì œê±°
                    json_str = re.sub(r'if\s*\([^)]*\)\s*\{[^}]*\}', '', json_str, flags=re.DOTALL)
                    json_str = re.sub(r'return\s+[^;}]*;?', '', json_str, flags=re.DOTALL)
                    json_str = re.sub(r'let\s+\w+\s*=\s*[^;]*;', '', json_str, flags=re.DOTALL)
                    json_str = re.sub(r'var\s+\w+\s*=\s*[^;]*;', '', json_str, flags=re.DOTALL)

                    # 8. ë¹ˆ ë¬¸ìì—´ì´ë‚˜ null ê°’ë“¤ ì •ë¦¬
                    json_str = re.sub(r',\s*null\s*,', ',', json_str)
                    json_str = re.sub(r',\s*null\s*}', '}', json_str)
                    json_str = re.sub(r'{\s*null\s*,', '{', json_str)

                    # 9. ë¶ˆì™„ì „í•œ êµ¬ì¡° ì •ë¦¬
                    json_str = re.sub(r'\s*\n\s*\n\s*', '\n', json_str)  # ë¹ˆ ì¤„ ì •ë¦¬
                    json_str = re.sub(r',\s*}', '}', json_str)  # trailing comma ì œê±°
                    json_str = re.sub(r',\s*]', ']', json_str)  # trailing comma ì œê±°

                    return json_str

                # ê°„ë‹¨í•˜ê³  ì •í™•í•œ JSON ì¶”ì¶œ í•¨ìˆ˜
                def extract_json_simple(text):
                    """ê°„ë‹¨í•˜ê³  ì •í™•í•œ JSON ì¶”ì¶œ"""
                    # 1. ```json ë¸”ë¡ì—ì„œ ì¶”ì¶œ
                    if "```json" in text:
                        start = text.find("```json") + 7
                        end = text.find("```", start)
                        if end != -1:
                            return text[start:end].strip()

                    # 2. JSON: ë‹¤ìŒì—ì„œ ì¶”ì¶œ
                    if "JSON:" in text:
                        start = text.find("JSON:") + 5
                        # ì²« ë²ˆì§¸ { ì°¾ê¸°
                        json_start = text.find("{", start)
                        if json_start == -1:
                            return None

                        # ê· í˜•ì¡íŒ } ì°¾ê¸°
                        brace_count = 0
                        for i in range(json_start, len(text)):
                            if text[i] == '{':
                                brace_count += 1
                            elif text[i] == '}':
                                brace_count -= 1
                                if brace_count == 0:
                                    return text[json_start:i+1]

                    # 3. ì²« ë²ˆì§¸ { }ë¸”ë¡ ì¶”ì¶œ
                    json_start = text.find("{")
                    if json_start == -1:
                        return None

                    brace_count = 0
                    for i in range(json_start, len(text)):
                        if text[i] == '{':
                            brace_count += 1
                        elif text[i] == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                return text[json_start:i+1]

                    return None

                # COT ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ
                try:
                    print(f"  - ì›ë³¸ LLM ì‘ë‹µ (ì²˜ìŒ 500ì): {response_text[:500]}...")
                    print(f"  - ì›ë³¸ ì‘ë‹µ ê¸¸ì´: {len(response_text)}ì")

                    # ê°„ë‹¨í•œ JSON ì¶”ì¶œ ì ìš©
                    json_part = extract_json_simple(response_text)
                    if not json_part:
                        print(f"  - JSON ì¶”ì¶œ ì‹¤íŒ¨: JSON ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        raise ValueError("JSON ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

                    print(f"  - JSON ì¶”ì¶œ ì„±ê³µ: {len(json_part)}ì")

                    print(f"  - ì¶”ì¶œëœ JSON íŒŒíŠ¸: {json_part[:300]}...")

                    # JavaScript í•¨ìˆ˜ ì œê±° (JSON íŒŒì‹± ì „ì— ì‹¤í–‰)
                    cleaned_json = clean_js_functions(json_part)
                    print(f"  - JavaScript í•¨ìˆ˜ ì œê±° í›„: {cleaned_json[:300]}...")

                    # JSON íŒŒì‹±
                    chart_response = json.loads(cleaned_json)

                    # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ JSON ì‘ë‹µ ì²˜ë¦¬
                    if isinstance(chart_response, list) and len(chart_response) > 0:
                        print(f"  - JSON íŒŒì‹± ì„±ê³µ: ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ({len(chart_response)}ê°œ í•­ëª©), ì²« ë²ˆì§¸ í•­ëª© ì‚¬ìš©")
                        chart_response = chart_response[0]  # ì²« ë²ˆì§¸ í•­ëª© ì‚¬ìš©
                    elif isinstance(chart_response, dict):
                        print(f"  - JSON íŒŒì‹± ì„±ê³µ: ë”•ì…”ë„ˆë¦¬ í˜•íƒœ")
                    else:
                        raise ValueError(f"ì˜¬ë°”ë¥´ì§€ ì•Šì€ JSON í˜•ì‹: {type(chart_response)}")

                    print(f"  - ì°¨íŠ¸ íƒ€ì…: {chart_response.get('type', 'unknown')}")
                    print(f"  - insufficient_data ì—¬ë¶€: {chart_response.get('insufficient_data', False)}")

                    if chart_response.get('insufficient_data', False):
                        print(f"  - ë¶€ì¡±í•œ ë°ì´í„° ì •ë³´: {chart_response.get('missing_info', 'N/A')}")
                        print(f"  - ì œì•ˆëœ ê²€ìƒ‰ì–´: {chart_response.get('suggested_search_query', 'N/A')}")

                    # ì´ì œ ì¶”ê°€ ê²€ìƒ‰ì€ ë³´ê³ ì„œ êµ¬ì¡° ë‹¨ê³„ì—ì„œ ì²˜ë¦¬ë¨

                    # >> ì •ìƒì ì¸ ì°¨íŠ¸ ë°ì´í„°ì¸ ê²½ìš°
                    elif "type" in chart_response and "data" in chart_response:
                        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                        datasets = chart_response.get("data", {}).get("datasets", [])
                        if datasets and len(datasets) > 0:
                            data_points = datasets[0].get("data", [])
                            if len(data_points) < 2:
                                print(f"  - ê²½ê³ : ì°¨íŠ¸ ë°ì´í„° í¬ì¸íŠ¸ê°€ ë¶€ì¡±í•¨ ({len(data_points)}ê°œ)")

                        # ì½œë°± í•¨ìˆ˜ ì œê±° (í”„ë¡ íŠ¸ì—”ë“œ ì˜¤ë¥˜ ë°©ì§€)
                        def remove_callbacks(obj):
                            if isinstance(obj, dict):
                                # ì½œë°± í•¨ìˆ˜ ê´€ë ¨ í‚¤ë“¤ ì œê±°
                                callback_keys = ['callback', 'callbacks', 'generateLabels']
                                for key in list(obj.keys()):
                                    if key in callback_keys:
                                        del obj[key]
                                    elif isinstance(obj[key], str) and 'function' in obj[key]:
                                        del obj[key]
                                    else:
                                        remove_callbacks(obj[key])
                            elif isinstance(obj, list):
                                for item in obj:
                                    remove_callbacks(item)

                        remove_callbacks(chart_response)

                        print(f"  - ì°¨íŠ¸ ìƒì„± ì„±ê³µ: {chart_response['type']} íƒ€ì…, {len(datasets)}ê°œ ë°ì´í„°ì…‹ (ì‹œë„ {attempt})")
                        yield {
                            "type": "chart",
                            "data": chart_response
                        }
                        return
                    else:
                        raise ValueError("ì˜¬ë°”ë¥´ì§€ ì•Šì€ JSON í˜•ì‹")

                except (json.JSONDecodeError, ValueError) as e:
                    print(f"  - ì°¨íŠ¸ JSON íŒŒì‹± ì‹¤íŒ¨ (ì‹œë„ {attempt}): {e}")
                    print(f"  - ì¶”ì¶œëœ JSON ê¸¸ì´: {len(json_part)}ì")
                    print(f"  - JSON ì‹œì‘: {json_part[:200]}...")
                    print(f"  - JSON ë: ...{json_part[-200:]}")

                    # ê°„ë‹¨í•œ ì¬ì‹œë„: ì „ì²´ ì‘ë‹µì—ì„œ ë‹¤ì‹œ JSON ì¶”ì¶œ ì‹œë„
                    retry_success = False
                    try:
                        print(f"  - JSON íŒŒì‹± ì¬ì‹œë„ ì¤‘...")
                        # ì „ì²´ ì‘ë‹µì—ì„œ ì™„ì „í•œ JSON ë¸”ë¡ ì°¾ê¸°
                        json_start = response_text.find("{")
                        if json_start != -1:
                            brace_count = 0
                            for i in range(json_start, len(response_text)):
                                if response_text[i] == '{':
                                    brace_count += 1
                                elif response_text[i] == '}':
                                    brace_count -= 1
                                    if brace_count == 0:
                                        retry_json = response_text[json_start:i+1]
                                        cleaned_retry = clean_js_functions(retry_json)
                                        chart_response = json.loads(cleaned_retry)

                                        # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ JSON ì‘ë‹µ ì²˜ë¦¬
                                        if isinstance(chart_response, list) and len(chart_response) > 0:
                                            print(f"  - ì¬ì‹œë„: ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ({len(chart_response)}ê°œ í•­ëª©), ì²« ë²ˆì§¸ í•­ëª© ì‚¬ìš©")
                                            chart_response = chart_response[0]

                                        print(f"  - ì¬ì‹œë„ JSON íŒŒì‹± ì„±ê³µ! ({len(retry_json)}ì)")
                                        retry_success = True
                                        break
                    except Exception as retry_e:
                        print(f"  - JSON íŒŒì‹± ì¬ì‹œë„ë„ ì‹¤íŒ¨: {retry_e}")

                    # ì¬ì‹œë„ ì„±ê³µí•œ ê²½ìš° ì²˜ë¦¬
                    if retry_success:
                        print(f"  - JSON íŒŒì‹± ì¬ì‹œë„ ì„±ê³µ! ì°¨íŠ¸ ìƒì„± ì§„í–‰")

                        # insufficient_data ì²´í¬
                        if chart_response.get('insufficient_data', False):
                            print(f"  - ë¶€ì¡±í•œ ë°ì´í„° ì •ë³´: {chart_response.get('missing_info', 'N/A')}")
                            print(f"  - ì œì•ˆëœ ê²€ìƒ‰ì–´: {chart_response.get('suggested_search_query', 'N/A')}")
                            # ì´ì œ ì¶”ê°€ ê²€ìƒ‰ì€ ë³´ê³ ì„œ êµ¬ì¡° ë‹¨ê³„ì—ì„œ ì²˜ë¦¬ë¨

                        # ì •ìƒì ì¸ ì°¨íŠ¸ ë°ì´í„°ì¸ ê²½ìš°
                        elif "type" in chart_response and "data" in chart_response:
                            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                            datasets = chart_response.get("data", {}).get("datasets", [])
                            if datasets and len(datasets) > 0:
                                data_points = datasets[0].get("data", [])
                                if len(data_points) < 2:
                                    print(f"  - ê²½ê³ : ì°¨íŠ¸ ë°ì´í„° í¬ì¸íŠ¸ê°€ ë¶€ì¡±í•¨ ({len(data_points)}ê°œ)")

                            # ì½œë°± í•¨ìˆ˜ ì œê±° (í”„ë¡ íŠ¸ì—”ë“œ ì˜¤ë¥˜ ë°©ì§€)
                            def remove_callbacks(obj):
                                if isinstance(obj, dict):
                                    # ì½œë°± í•¨ìˆ˜ ê´€ë ¨ í‚¤ë“¤ ì œê±°
                                    callback_keys = ['callback', 'callbacks', 'generateLabels']
                                    for key in list(obj.keys()):
                                        if key in callback_keys:
                                            del obj[key]
                                        elif isinstance(obj[key], str) and 'function' in obj[key]:
                                            del obj[key]
                                        else:
                                            remove_callbacks(obj[key])
                                elif isinstance(obj, list):
                                    for item in obj:
                                        remove_callbacks(item)

                            remove_callbacks(chart_response)

                            print(f"  - ì°¨íŠ¸ ìƒì„± ì„±ê³µ: {chart_response['type']} íƒ€ì…, {len(datasets)}ê°œ ë°ì´í„°ì…‹ (ì¬ì‹œë„ ì„±ê³µ)")
                            yield {
                                "type": "chart",
                                "data": chart_response
                            }
                            return
                        else:
                            print(f"  - ì¬ì‹œë„ ì„±ê³µí–ˆì§€ë§Œ ì˜¬ë°”ë¥´ì§€ ì•Šì€ JSON í˜•ì‹")

                    print(f"  - JSON íŒŒì‹± ì¬ì‹œë„ë„ ì‹¤íŒ¨, fallback ì°¨íŠ¸ë¡œ ì§„í–‰")

                    # ìµœì¢… fallback ì°¨íŠ¸
                    yield {
                        "type": "chart",
                        "data": {
                            "type": "bar",
                            "data": {
                                "labels": [f"{section_title} ê´€ë ¨ ë°ì´í„°"],
                                "datasets": [{
                                    "label": "ì •ë³´ ìˆ˜ì§‘ ìƒíƒœ",
                                    "data": [1],
                                    "backgroundColor": "rgba(255, 193, 7, 0.6)",
                                    "borderColor": "rgba(255, 193, 7, 1)",
                                    "borderWidth": 1
                                }]
                            },
                            "options": {
                                "responsive": True,
                                "plugins": {
                                    "title": {
                                        "display": True,
                                        "text": f"{section_title} - ë°ì´í„° ë¶„ì„ ì¤‘"
                                    }
                                },
                                "scales": {
                                    "y": {
                                        "beginAtZero": True,
                                        "max": 2,
                                        "ticks": {
                                            "stepSize": 1
                                        }
                                    }
                                }
                            }
                        }
                    }

            except Exception as e:
                print(f"  - ì°¨íŠ¸ ìƒì„± ì „ì²´ ì˜¤ë¥˜ (ì‹œë„ {attempt}): {e}")
                yield {
                    "type": "chart",
                    "data": {
                        "type": "bar",
                        "data": {
                            "labels": ["ì‹œìŠ¤í…œ ì˜¤ë¥˜"],
                            "datasets": [{
                                "label": "ì²˜ë¦¬ ìƒíƒœ",
                                "data": [0],
                                "backgroundColor": "rgba(220, 53, 69, 0.6)",
                                "borderColor": "rgba(220, 53, 69, 1)",
                                "borderWidth": 1
                            }]
                        },
                        "options": {
                            "responsive": True,
                            "plugins": {
                                "title": {
                                    "display": True,
                                    "text": "ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
                                }
                            }
                        }
                    }
                }

        # >> ë©”ì¸ ë¡œì§ ì‹¤í–‰
        async for result in _generate_chart_with_data(section_data, attempt=1):
            yield result
